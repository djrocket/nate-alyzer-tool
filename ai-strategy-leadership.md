<!-- VIDEO_ID: -oI7mrudRn8 -->

Date: 2025-10-05

## Core Thesis
The speaker's core, non-obvious argument is that the strategic advantage in AI agents lies not in selecting the "best" model, but in immediately building robust, model-agnostic orchestration architectures that embed organizational context, foster compounding institutional learning, and proactively integrate compliance, thereby creating defensible, high-ROI systems that accelerate business velocity.

## Key Concepts
*   **Architecture-First, Model-Agnostic Orchestration:** The primary competitive advantage stems from a flexible, robust orchestration layer capable of swapping out underlying models, treating models as commoditized components. This challenges the common focus on "which model is best" and aligns with principles of modular system design and future-proofing.
*   **Compounding Institutional Learning (Memory Systems):** Early deployment of memory-augmented agents creates an "unclosable gap" in organizational knowledge and data accumulation, providing an asymmetric strategic advantage that cannot be bought or replicated later. This leverages the first-principle of compounding returns applied to information and experience.
*   **"Boring Automation" for ROI:** True enterprise value is derived from aggressively automating high-frequency, high-cost, well-defined workflows to achieve high completion rates (e.g., 90%+), rather than pursuing flashy, ambiguous demos. This highlights a pragmatic engineering approach focused on measurable business impact.
*   **Vertical Defensibility through Contextualization:** Sustainable competitive advantage is built by embedding deep domain expertise, business rules, and organizational context within the agent's orchestration layer and tools, making the system resilient to better generic models and enhancing its value. This is a mental model of building a "moat" around specialized knowledge.
*   **Compliance as a Competitive Moat:** Proactively building auditability, traceability, and policy controls into the agent orchestration layer transforms regulatory requirements (like EU AI Act, HIPAA) from a burden into a unique, defensible competitive advantage. This is a counter-intuitive finding, reframing compliance as a strategic asset.
*   **Velocity Over Perfection (Blast Radius Effect):** Prioritizing rapid deployment (e.g., 6 weeks for 85% completion) over prolonged planning cycles generates compounding returns and accelerates overall business operations by reducing "context switching costs" and "team drag" across the organization. This emphasizes the value of iterative development and the systemic impact of automation.
*   **The "Decision Cascade" Framework:** The six principles (Architecture, Memory, Workflows, Verticality, Compliance, Velocity) are not isolated but form an interconnected, sequential framework for building robust, defensible, and high-ROI AI agent systems.

---

Fortune 100 AI Agent Secrets: The 6 Principles Your Competitors Don't Want You to Know - YouTube
https://www.youtube.com/watch?v=-oI7mrudRn8

leaders. If anyone in your org is saying, "We're not ready for AI agents," they're wrong. If you are asking, "Are we ready for AI agents? Are AI agents mature enough for us?" You're wrong. While you are figuring out how to perfect your AI readiness deck, Walmart has automated 95% of their bug fixes with 200 specialized agents.
AI agents are here in production at Fortune 100 companies right now. Here are the six principles that Walmart and others have used to build AI agents into production workflows. And you can absolutely go after that this quarter. You do not have to wait. This is not a 2026 conversation. One of the things that I worry about is that companies are going to take AI agents too slowly like traditional software.
Like, okay, maybe we'll be ready. We'll add it in 2026. No. If you see an AI agent use case, go after it and go after it aggressively. So, what are those six principles? We're going to dive right in. Principle number one is architecture first thinking. So, executives tend to ask me, Nate, which model should we bet on for AI agents? Wrong question, guys.
The right question is which architecture delivers the workflows we're looking for? Model advantage lasts maybe a quarter at best. Architectural advantage is something that persists for years. So the principle is this. You want to build model agnostic orchestration that isn't dependent on particular models to execute particular workflows.
In particular, you want to make sure that your agents are selected, evaluated, swapped, and combined individually. And you do that based on the architecture and what it demands, the workflows, right? This comes back to your orchestration layer being the competitive bet, not the models. The models are replaceable commodities.
you can build a model orchestration layer which is exactly what Walmart built and that is going to get you much much farther. So, you want to be in a place where you can deploy central orchestrators that manage specialized agents like Walmart's YB model is what they use. WIBY, you can look it up. They are designed to tackle a range of tough tasks with appropriate tooling, right? You can go after that by separating out your workflows into tasks, delegating those appropriately, aggregating them up, having appropriate guardrails and
governance. It's system design. You're designing a system in which models sit and do particular tasks with particular tools. That way you will have infrastructure that enables you to switch models regardless of what comes out tomorrow. Microsoft cares so much about this they retired their autogen framework entirely to unify around an agent framework.
80% of enterprises out there are starting to get into orchestration layers roughly speaking. If you were going directly to models, you're building AI agents on sand. It's not going to last. Principle number two, learning compounds. And with AI agents, you need to invest early in order to get ahead of the market on compounded learning in 18 to 24 months.
JP Morgan has had 18 months of institutional learning and you cannot buy, steal or or replicate that with better tech. They have spent 18 months on AI agents already in late 2025. Every day you wait, that gap widens. Right now, you may not be competing with JP Morgan, so you don't care. But that's what we're talking about when we talk about compounding learning.
Memory systems need to be turned on early to turn adoption into a permanent advantage for you. Organizational learning accumulates. competitors are going to have to face unclosable gaps because you are going to have agents trained on more data than they have always. Always. So you want to be in a place where you can deploy early and capture significant percentage gains in accuracy in completion etc.
You basically preserve an accuracy and quality gain as an edge over time because you are always getting the best models against a growing body of data that other people don't have. This gives you asymmetric strategic value. So you want to implement memory augmented agents inside an orchestration layer that learn your organizational context that track the information that matters when it matters and how it relates to decisions.
And yes, the examples I'm giving you, that's exactly what's happening in task appropriate ways. In this case, for Walmart, it was triaging bug tickets. That was really important. That's what they focused on. Focus on systems that are able to absorb and understand meaningful workflows. Make sure they have clear terminology they can learn from, like dictionaries.
Make sure they have clear workflow patterns. to make sure they have really really crisp precedents to work against how things have worked in the past, how you've done b business logic in the past, how you handle data ambiguities, and make sure they have exception paths. 200,000 JP Morgan employees have gotten 450 use cases into AI agent land.
Basically, they've they've uploaded those use cases. They've had AI agents start to get to get into those use cases. A system deployed today will understand your organization as well as JP Morgan does now for those 450 use cases in 2027. Right? Don't wait. That's the whole point of this video. Don't wait. AI agents are ready now.
That is one of the big things that has shifted in the past few months is that there is no reason to wait. It isn't even as expensive as it used to be. AI agent SDKs are out there for major model makers all over the place. They're out there for cloud providers. You can assemble AI agent frameworks. Your team can. Principle number three.
Let's talk about workflows. I've been referencing it a little bit. Demos are going to get you applause or get your engineers applaud, but but the workflows that I'm talking about are really aimed at real ROI. And that is the difference that most executives talk about when they get AI right. Enterprise AI spending is scaling by like triple digits a year in percentage point terms.
But it's scaling because boring automation prints return on investment. It really like the the return on investment is skyhigh if you can pick specific automations for defined workflows and stick agents against them. And so you should be looking not at feature counts, not at the number of agents you launch, not at login metrics, not at the number of tickets even.
You should be looking at the rate of correct completion for workflows that matter to the business. Can you get to 90% plus AI automated completion for workflows that matter to the business? Recurring automation will compound in value in a way that flashy demos will never do. If you can get to, you know, call it 30% automation in month one and you have a dedicated team working to drive that number, the value just compounds.
is that team bites off more edge cases and is able to knock out that workflow and they go on to the next workflow and eventually it gets easier and easier as you start to stamp these out. Identify the highfrequency high-cost workflows first. Of course, you want to focus on workflows that have enough defined inputs that you can have tools called against defined inputs and the agent can make correct decisions at a very high rate to fully complete the workflow.
If you can't get that, pick a different workflow initially because if you pick a high ambiguity workflow initially, it's going to be hard to get success. You'll grow discouraged and you probably won't go after workflows with the aggression that you need to. Principle number four, vertical defensibility. So, a better model can replace a generic tool overnight, right? Maybe chat GPT6 comes out, it becomes amazing and you worry about your customtuned special model because it's out of juice.
Or you worry about your generic tool that was just going to do one simple thing and then Chad GPT just beats it. But if you have vertical expertise backed by vertical specific data, backed by your vertical specific expertise encoded in business rules, encoded in logic, that takes years to build. that can't be commoditized.
That becomes an agent framework that a better model complements. You basically want to give the organizational context to the model as memory, as specialized tool calls, as special workflows. And that becomes something that a better model only enhances because you have vertical defensibility encoded in the context of the orchestration layer around the model.
You don't need to worry about a better model un undoing the value that you've built. It actually just enhances it because it's smarter, which enables you to do higher quality work, tackle more difficult problems, hit higher completion rates. You get the idea. The principle is to specialize and pick vertical specific workflows. And if you're building for the customer, if you're in a B2B space, watch out for those generic horizontal capabilities.
Those are very much at risk of disruption right now. Generic tools do get optimized out of existence because a general model comes along and they just optimize too fast and they were like we're going to be a generic tool for everybody for this specific thing and like people don't care. As an example, the chat with your PDF tools that's going away because you can just upload a PDF and chat with it now.
See generic tool. It solved a problem. It's no good now. But if you have a vertical specific workflow around health care or around finance, that is not going away when a better model comes out. And so this is really good news for businesses that have deep vertical expertise and comparative advantage in specific things.
If you have comparative advantage around a fintech thing or a healthc care expertise, if you deeply understand the regulatory environment around real estate acquisition in particular state, whatever your expertise is, build vertical specific orchestration frameworks, embed the domain expertise inside the workflow as much as you can, but keep it out of the model because then you can swap the model in and the model will learn your verticals, compliance, terminology, decision patterns, etc.
uh from the tools, from the context, from the orchestration layer and be able to operate. And so for example, a legal compliance framework might understand privilege. A healthcare memory system might understand consent in a certain way. Manufacturing agents can understand equipment history. You get the idea. But critically, these require the ability to call in that context, not to assume the model has it built in because you don't have to assume that.
you can get a better model swapped in tomorrow and the vertical specific piece can live inside your business as part of that agent orchestration layer. Principle number five, compliance is a competitive moat with agents. So we talk about the EU AI act a lot because it is hugely relevant for any business that that does any kind of volume in the EU.
Enforcement is going to begin in the next year. You have to have a compliance framework that actually works. But if you do, it becomes a moat. And I would assume that you want compliance frameworks that scale for you and anticipate emerging regulatory environments in the United States as well, which are happening on a state-by-state basis and are very complex.
You want to think about the regulatory infrastructure that enables you to show auditability, traceability, security first vendor integrations, policy controls. Those are all things, spoiler alert, that come from a solid orchestration layer. They are not things that the next model Chad GPT6 is going to give you for free.
They are things that you have to build for your vertical that essentially are like an extension of your expertise into the regulatory space touched by AI. You are building, you are forging an orchestration layer for agents that captures the regulatory nuance of your world. I talked about healthcare earlier. Well, HIPPA is relevant in the US, right? You have to think about privacy.
How do you show that your agentic framework is HIPPA compliant? How can you show that at the database level? How can you show that in the run traces you have for agents? How can you show that in the ability to generate audit frameworks that people who need to look at your systems and sign off on them can easily verify and understand.
We are pioneering here, right? We have not had a year of agent maturity yet. We do now. Like the clock started a couple of months ago and I am issuing this warning because I have heard too many times from leaders that they are still picking the model. They are waiting for agents to be fully ready. Please don't wait. Please don't wait.
Principle number six, velocity matters more than perfection. And I feel like that's a wonderful segue. Right? If you can get to something in 6 weeks that gives you an 85% completion on a particular workflow task, that is going to beat a six-month planning cycle. That is going to be thinking about this for the budgetary 2026 cycle.
Please think about how you are building speed into your systems. And I'm saying that on multiple dimensions, both in the first run of the agent and how you get it going in your hiring plans and how you're hiring for engineering talent to get you there. And also think about the fact that agents themselves are accelerators.
A good agent orchestration layer delivers an accelerated impact across not just the workflows it touches but everybody around that agent because they're no longer dealing with that. Let's go back to Walmart. Walmart is no longer dealing with the context switching costs, the team drag that comes from manually dealing with all of those bugs because the agents are doing it.
there is a speed up effect across the team. And so if you can invest in velocity now to get into an agent deployment, you are going to get velocity speed up dividends after it's deployed. And so you should think about where in your business you have the most blast radius effect if you deploy an early workflow for agents. What is something that is a huge pain point for teams where you can deploy an agent and you can see a tremendous speed up not just cuz that task gets done.
Sure, we all know that's going to happen, but because other teams aren't touching things that have been incredibly painful. So here here is the critical piece. I want you to go back to the beginning with me. What we have here is a decision cascade and I want to walk through all six and show you how they go together.
You are picking an architecture because a model agnostic orchestration survives churn. Inside that architecture, you are deploying memory because you are starting institutional learning to generate unclosable gaps with your competition. Then inside the architecture with the memory, you're automating workflows. You're closing loops that generate recurring compounding returns because you're actually finishing those workflows completely.
and you are picking those workflows around verticality. You are building expertise that you cannot commoditize. That entire system is going to be built inside a regulatory competitive moat. You are going to think about the regulatory environment you're in and you're going to recognize that if you can demonstrate compliance proactively with agents, you have a moat just as you have a moat with memory and you have a moat with verticality.
Compliance is a specialized skill set that agents can learn and you can teach it to them for your business. Last but not least, moving fast enables you to generate compounding returns not just from the initial agent deploy, but because this entire system generates an acceleration for the business. And that's what Walmart found and that's what you're going to find.
And so my challenge to you is do not wait. Your competitors who are doing this are not waiting to be perfectly ready. They are figuring out very scrappily how to apply these kinds of principles to build production agents. There is no technical gap here. You may have a talent gap you need to close and you can go and get the talent for it, but there is no technical gap. Agents are ready for production.
There is no reason to wait and people who are asking me whether it is time are already late. Don't be that person. You have the principles here. you understand how to think in agentic terms. If you if you listen to this video a couple times, you understand more about agentic orchestration than 90% of the seauitees I talked to.
This is how you need to think to build aic systems that last and are sturdy both this year and into next year and beyond. These are these become systems that have ROI and investment value, not just flashy demos. So build to last, build now, and don't ask me if we're ready for AI agents because it's already too late. We are ready.
We are done. We're deploying. They're at the Fortune 100 level. We are off to the races.

---

<!-- VIDEO_ID: 1FKxyPAJ2Ok -->

Date: 2025-09-13

## Core Thesis
Effective AI strategy demands a disciplined, cost-benefit driven approach to problem-solving, prioritizing the simplest viable solution from a hierarchy of data processing methods, and resisting the pervasive hype by focusing on problem structure, measurable ROI, and the often-underestimated exponential costs of complexity, maintenance, and specialized talent.

## Key Concepts
*   **The "Ladder of AI/Data Solutions" Framework:** A four-tiered hierarchy (Plain Old Data Processing, Classical Predictive ML, Generative AI, AI Agents) for classifying problem types and matching them to the appropriate technology, emphasizing the principle of "pick the lowest rung on the ladder you possibly can."
*   **Disproportionate Returns and Costs:** While higher rungs on the AI ladder offer power-law-like leverage (e.g., 100x-1000x returns), they also incur exponentially higher compute, maintenance, and talent costs (e.g., 1-2-4-8 costs, not 1-2-3-4), making advanced AI a "roller coaster to heaven" in both gain and expense.
*   **The "10x Rule" for Advanced AI Adoption:** For generative AI or agentic workflows to be worthwhile, they must deliver at least a 10x improvement over baseline solutions to justify the significant overhead in adoption, talent, and ongoing maintenance.
*   **Data Quality Over Model Complexity:** Superior data quality consistently outperforms more complex models; fixing data pipelines is a foundational priority that unlocks leverage and allows for the use of cheaper, simpler models.
*   **The Value of "Boring Solutions":** Simple, auditable, and well-understood solutions (e.g., a clear BI dashboard) often provide more reliable business value than complex, opaque AI models, especially when the latter are prone to errors or lack interpretability.
*   **Human-in-the-Loop First for LLMs/Agents:** Initial deployment of LLMs and agents should involve surfacing suggestions for human vetting, building trust, and gathering feedback, gradually automating only after validation, acknowledging the inherent unreliability and cost of human oversight (e.g., the Amazon "just walk out" example).
*   **ROI-Driven Problem Scoping:** AI initiatives should be framed as a means to specific, measurable business outcomes (e.g., reduced costs, faster decisions, improved customer satisfaction) rather than focusing on feature lists or AI for its own sake.
*   **Talent as a Cross-Cutting Factor:** The complexity ladder directly correlates with increasing talent requirements; underestimating the need for specialized expertise for higher-rung solutions (especially agents) is a common pitfall leading to project failure.
*   **Cost-Benefit Articulation for Executive Pushback:** A structured framework for communicating the time, cost, accuracy, and maintenance implications of different solution approaches (data processing, ML, GenAI, agents) to executives, reframing "no" into a strategic, value-driven recommendation.

---

Nate, when do I use AI? When do I use AI agents? I get that question a lot. This video is for you. If you've ever wondered, how do I know when to use agents? How do I know when to use generative AI and large language models? This is going to show you. We're going to go through the four different categories that you can choose between when you make decisions about data and insights.
We're going to give you a concrete decision framework. We're going to give you the principles to work with so you understand how to recognize these problems elsewhere. And I'm even going to give you scripts so that you can understand if if an investor if your boss is pushing on you for a solution that you know won't work.
How do you push back in a way that makes sense? First step, let's understand the four categories we're working with. Number one, plain old data processing. It's not new. It's not fancy. It's not AI. It's the simplest possible thing. Data cleaning, aggregating it up, building simple reports. If you just need to get a very simple sales report and you're aggregating up the clients and the regions and over a particular time period, that kind of thing, do not use AI. Repeat after me, don't use AI.
Don't use agents. Don't use generative AI. Don't believe anyone who tells you to do that. If you're on an e-commerce site and you just need to look at your payment volumes over the last quarter, don't use AI. If you just need to understand how many SKs you have for sale, don't use AI. Are you getting the idea? If it is the kind of thing where you could write it out as a math problem, x + y= z, don't use ai.
It's not worth it. It's going to be much more expensive. It's going to be less dependable. It's a waste of everybody's time. Let's go to bucket number two. Classical predictive machine learning. This one has almost disappeared because there's been so much hype around large language models. So, let's talk about what it actually is and where to use it so we don't lose the value.
Because we put decades of work into developing classical machine learning. I've built classical machine learning systems myself at scale. It is important to understand where it is still valuable versus large language models, but almost nobody thinks about it because of the hype cycle, which is why we have to make videos like this.
If you have rich historical data and you have a clear target variable to optimize against and you need something to predict that's very specific, like I want to predict seasonal Q4 demand or I want to detect fraud or I want to predict churn. Well, machine learning excels in taking patterns in structured data and pulling them to light when you have a clear goal like that.
Now, this takes training data. It takes evaluation metrics. It takes monitoring. It's a little bit more complex than just running a SQL query. But if you want to predict next quarter sales based on past trends and promotions, a lot of people are using large language models for this. But the correct tool is not large language models.
The correct tool is actually traditional machine learning. Traditional machine learning is designed for situations where you have structured data and a problem with a single variable you're optimizing toward. Let it do its job. Let it do its job. And you see the difference, right? I want to make sure you understand the difference. If you're doing very simple sums and reports and aggregations, that's not for machine learning.
That's that plain old data processing category. If you want to predict the performance of a single variable and you have structured data, that's not large language models. That's not AI the way most people talk about it. It's machine learning or traditional artificial intelligence back before chat GBT generative AI or large language models. Now that's our third bucket.
Let's say you have a data set that's mixed. You have some numbers. You have structured data. You have some text. Now you also need to generate text in the answer. Maybe you need to generate concrete summaries of the marketing quarterly report and you want to generate the text with that. Maybe you want to generate an image with that.
The problem involves summarizing something that is not numeric necessarily. It involves translating things. It involves drafting content. It has a lot of words in it. Well, large language models are probably your best tool at this point for that. They're flexible, but you also have to take into account hallucinations, which I've talked about a fair bit, how you handle higher compute costs, and how you handle latency, which is like the unpredictable sort of gap in response time.
So, if you want to autodraft customer support responses based on the text of the customer support manual, that is a great example of a large language model task. If you want to autogenerate product descriptions, that is a great example of a large language model task. And you can even do it if they're just looking at the image and they're writing the description based on the image.
LLM tasks are characterized by wordiness. They're characterized by unstructured data and they often have multi-threaded output. So you're not optimizing for a single variable in a structured data set. You're actually trying to get multiple outputs. You might have an image output in some cases. You might have a text output in other cases.
And you value that so highly that you are willing to put up with the risk of hallucinations and putting in guards to minimize that and all the investment that goes with that. In other words, generative AI is more expensive to build and maintain. So you have to do the math to decide that it's worth it.
And we'll get into more of that ROI math later in this video. The fourth bucket is AI agents. It's the most complex bucket. That's why I put it fourth. Use it when tasks involve workflows. Dynamic multi-step workflows with clear decision points. That's critical for agents. Decision points where you can describe the criteria.
You can describe the scope of decision and you can give the agent all the context it needs to make a good choice. Things like scheduling, follow-ups, data retrieval across systems all fall into the agent bucket. As an example, an agent that books conference rooms, notifies attendees, and adjust schedules when conflicts arise automatically.
That's an AI agent problem. It's not traditional machine learning. It's also not generative AI. It's an agent problem. Agents can orchestrate complicated tasks autonomously, but you have to have very careful error handling. You have to have good observability so you can see what they did and you need to have humans who know how to debug them.
So there's a human talent question with agents as well. It's worth thinking about though because as we've gone through these four buckets, what you should be thinking is leverage. These buckets are not linear. These buckets are disproportionate. There's a power law return here. If you get one x return on the simple x plus y plus c, the simple monthly sales by region, write a SQL query, you get it back, you get x return on solving it with a machine learning problem if it's machine learning susceptible.
You get a 100x return on generative AI and you can get a x return on agents. Now those are somewhat illustrative. I'm not saying every single project falls exactly in that number. But in my experience and the experience of a lot of others who have implemented these in practice, that is how it works. These are not stairs.
It's like a roller coaster to heaven, right? Like this is a crazy gain in leverage as you move up, but it's also a crazy gain in cost and maintenance. And you have to design the more advanced systems very intelligently and target them at the right problem, which is exactly why we have this video. Because you can imagine the expense, the cost of building a generative AI system, of building an agentic system against a problem set that didn't need it.
What if you built an AI agent workflow to sum monthly sales by region? Is it possible? Yes, absolutely. Is it like bringing a bazooka to kill a fly? Yes, it is. It's ridiculously expensive. You don't need to do it and it would be a terrible waste to try and do it that way. And that brings me to the idea of the ladder.
Pick the simplest solution on the ladder. Imagine these four rungs. You have just basic data operations. You have machine learning traditional. You have generative AI and then finally you have AI agents. Pick the lowest rung on the ladder you possibly can. And I'm going to show you how you kind of think that through.
You could call this developing engineering taste, but it's not just for engineers. So many of these skills have been sequestered away and hidden in engineering uh conference rooms for too long. And I want to bring them out because they're not actually too technical and we really need them in the age of AI.
So the first skill that you need to navigate this ladder correctly is to focus on pattern recognition over hype. Did you notice how I talk about the type of pattern? That is a skill you can learn to focus on problem structure, not on buzzwords. You can learn to ask, hey, what needs solving? Is this a decision that we need to solve for? Is it a prediction we need to solve for? Is it a generation we need to solve for? If it's a decision in a workflow, it might be an agent.
If it's a prediction, well, that might be traditional machine learning. If it's a generation problem, that might be a large language model problem. If it's just a report, that might be traditional just data operations. That kind of thinking, that kind of sober focus on problem structure is going to help you resist the AI for everything temptation.
Sometimes just having a SQL uh script that runs will solve almost your entire problem. And by the way, I said almost on purpose. Do not give in to the temptation to make something a generative AI project or an agent project if 5 or 10% of the value is coming from that agentic piece or that generative AI piece and most of the value is coming from SQL.
If your boss says to you, I want a quarterly marketing report and I want it to have this fancy insight as to why we why why we performed the way we did and I want it to be in text and I want to have an illustration of our top selling product. You could look at that and say, "Well, there's some stuff here that is generative AI.
" So, it's probably a generative AI problem or maybe it's a mixed problem where you use two or three runs on that ladder. I talked about data processing, generative AI, maybe even some prediction from machine learning. I would not look at it that way. Instead, I would look at it and say, why the heck do we need the picture? Why do we need the text? Don't we get the business value to make good decisions out of traditional data operations? If you get 90% of the value for 5% of the cost, the business should take that trade all day and you should be able to articulate
that in dollars and cents very very clearly. It is worth asking where the leverage and the problem lies. That is my point. So think of the problem as a distribution and it's going to be distributed along the four legs of that ladder. If the problem is is going to be bumpy and like really skewed heavily toward one of the legs of the ladder, you should take that pretty seriously.
You should say maybe this is fundamentally just a data operations problem because most of the problem value is there and maybe we should make the decision to cut the five or 10% of value you're talking about and make that a later choice and just do the thing that we can get away with now that is only one of the legs of that ladder.
The simplest one you need to understand how to speak the language of costbenefit to make these kinds of claims because that's how executives speak. If your boss or your investor is telling you to invest in AI, the only way they will really hear you is if you come back with cost benefit.
So start to learn how to talk about compute costs. Generative AI and agents are both extremely expensive in tokens. It is not cheap to run those pipelines. If you make a mistake with that architecture, you you can be out thousands, tens of thousands more. Machine learning is a lot cheaper, but does take expertise to set up and it's not free.
And data processing is the cheapest of all. Almost anybody at this point with an engineering degree can set up a data processing pipeline without any kind of issue. Most of us who don't have engineering degrees can figure it out with Jad GPT. The maintenance version is also non-trivial and I want to call that out because the maintenance version you know how I talked about this idea of power law returns and this roller coaster that stretches up as a way of showing like how much uh well potentially fun but also how much excitement uh there is in some of the
agentic use flow cases, the generative AI cases. The maintenance also scales with that. So if you have an agentic workflow that is going to have significantly more costs than a large language model workflow that is going to have significantly more costs again than a machine learning workflow which is still more expensive than a data pipeline workflow.
It's not 1 2 3 4 costs. It's more like 1 2 48 costs. These costs get much more expensive. And part of why is that agents and LLM systems have to be maintained in production. Your leader who charges you with building these systems has to know that the dollars keep going out the door on time spent supporting these systems after they are launched.
And so instead of thinking about an agent workflow like traditional software, you have to think about it as continually maintained almost like a little employee that you have to pay every month. The last thing I want to call out from a costbenefit perspective is time to value. As you would imagine with compute costs, with maintenance burden, it is increasingly complex as you move up the ladder and that takes increasing time and talent.
And so if just about anybody can set up a data pipeline in a few days given the data and if you can get a data scientist to work with you on a machine learning model and that might take just a couple weeks if you have everything ready. Generative AI prototypes really vary. If it's out of box and it's super simple, it can be a couple hours.
And if it's a full production pipeline, it's going to be multiple weeks, multiple weeks. It's not going to be easy. Often months, I know people who are at scale, who are building LLM pipelines that aren't agentic, still taking them months. This is not easy to do. Agents, if you're starting from scratch and you're a scrappy startup and you're wellunded in the valley, sure.
Can you knock up some agents over the weekend? Absolutely. You have the right talent. you have a clean slate to work with. If you're an existing company and you're trying to get this done, it is even harder than LLMs. It is quite difficult to do well and quite difficult to sustain well and you have to recruit the talent for it. The months stretch out into 6 months or more very, very quickly.
And it's your job if you are given this assignment or asked to do a project that involves AI to find a way to articulate that and explain it to find a way to say yeah we could do this with agents but it seems like what you're really optimizing for is just getting the data into a report and there could be a simpler way to get you that value much much faster.
we can find another use for AI so that you can tell the board right so in sum if you have someone coming to you or if you're asking the question when do I use AI when do I use agents before proposing AI identify the simplest nonAI solution evaluate whether AI measurably improves the accuracy speed or user experience against that solution and by measurably I mean it has to significantly improve my rule of thumb with using a large language model or an agentic workflow is that if it isn't 10x versus the baseline, it probably isn't worth it
because there's adoption, there's talent, there's systems to maintain. And so 10x is my rule of thumb. And if you don't hit that, stick with a simpler approach. It's worth it. So I want to suggest to you that there are a few scripts that you can use if you get stuck with leaders who are just not believing you that will help you to work through this.
And then we're going to get at the end of the video into some sort of contrarian insights, things that kind of go deeper. But before we do, just to like sum up this piece around costbenefit and communication. Here's how you answer. Let's say your VP says, "We need to use chat GPT for our report." Common. I've seen it happen. You could say this.
Hey, uh, thanks for asking. I researched three different approaches for uh, the report problem with a data pipeline. Uh, it's going to take us 2 days to set it up. Uh, it's going to cost us like 200 bucks and we're going to have 100% accuracy on our known metrics. You can have it by Friday. If we used a machine learning model, which is not AI, Mr. VP.
That's going to be 2 and 1/2 weeks with our data science team, push back another project. I suspect we'll get to 80% accuracy on prediction initially and have to work up from there. Uh total cost, I want to say $15,000 maybe. If it's generative AI and uh it's a large language model prototype, I would guess out of the gate that you're going to have inaccuracies across all of your key metrics initially.
It will take us a few days to get it set up and it will take us probably three or four months to root out enough of the hallucinations to make the report really worthwhile. I would recommend option one because we can get reliable results by Friday and it's the cheapest overall. If we really need predictive insights, we can add that machine learning model as a fast follow.
You see how that's something that speaks executive. It talks cost. It talks time. It doesn't say no directly. It actually just reframes the problem and helps them understand what's going on. Okay, now let's get to some of the contrarian insights that you need to have in your head when you're facing these when do I use agents, when do I use AI problems.
Number one, data quality is going to beat model complexity every time. Garbage in, garbage out, right? If you are introducing AI and you have bad data, you are pouring money down the drain and lighting it on fire. Fix your data pipelines before reaching for models. And if you have great quality data, you can use cheaper and cheaper and cheaper models.
Make sure you take advantage of your data quality. And if you don't have data quality, make sure you make that a priority and fix it. You do get real leverage from that fix. That is how you unlock the 100x,000x use cases cuz agents also struggle with it if the data is bad quality. Number two, boring solutions when a clear, well-designed BI dashboard that just works outperforms a fancy AI model that nobody understands and that can't be audited. I love AI.
AI has a lot of use cases, but I have seen too often now that we're in this sort of hype cycle for AI, people are throwing out the boring solutions that work. Don't do that. Don't be that person. Find ways to reframe like the sort of narrative that we talked about. Number three, human in the loop first. When you're deploying LLM, when you're deploying agents, start by surfacing suggestions for humans to vet, build trust, gather feedback, and then automate.
Now, if you're a startup and you have nothing to lose, sure, give it a shot. See if you can automate it and make it work in a weekend. If you're a company with stakes, you have to take seriously the idea that humans need to be involved from the beginning and helping to get the model to a place where it works.
And you need to assume that cost. Who's going to staff for that? Who's going to pay for that? How long are you going to use the humans? How do you know if the humans are able to transition to AI systems? You know, one of the dirty secrets in AI is that sometimes it never makes it and then there's a scandal. So, for example, in the Amazon just walk out checkout stores, the company would trumpet that it was AI that they were using, but the reality was it was people looking through the cameras, checking all the work because they were never able to
transition out of humans in the loop. AI is hard. Don't believe every pressed release you see. Look at whether you can reasonably hand off human work to AI. Sometimes you can. There are real wins out there, but think about it and make an intentional plan and benchmark yourself and see if you're actually able to successfully do it.
This comes down to scoping the problem very very precisely around the value you intend to deliver. Number four, ROI focused. Don't think about the feature list. Remember when I said earlier in this video that you want to mentally assess the problem space and look at where the value is spiky against that ladder, whether that's simple data operations or machine learning or generative AI or agents.
Focus on that value piece. The feature lists will spread out all over the place. If you focus on feature lists, you're going to be extremely inefficient. Focus on where the leverage is. Focus on the value and frame AI as a means to specific business outcomes. It should deliver reduced costs, faster decisions.
It should deliver better customer satisfaction. The point is not AI itself. The point is the value it delivers. And so if you need to push back when someone makes the case that you should use an AI agent and you have used this framework and you know you shouldn't, push back on ROI. push back and say, I want to deliver reduced costs and this isn't going to do that.
I want to make sure that we make good decisions to actually improve customer satisfaction, and this agent workflow is not going to do that on the timeline you need. So, let's close with a 30-se secondond decision tree that will help you the next time you face this. Does the problem have deterministic rules? It's a data processing problem.
Does the problem need to predict an outcome? It's a traditional machine learning problem. Does the problem need to generate novel tokens, novel words, novel content, generative AI, large language models? Finally, do we need a workflow with autonomous decisions and multi-step orchestration? That's an agent's problem.
The cross cutting factor here is talent. As you go up these four, the talent needs get bigger. The same person that can do data processing usually can't do autonomous multi-step orchestration with agents, not a production scale. So you have to have to also be aware of how to advocate for the talent that you need in order to deliver against these outcomes.
It is not fair to ask an engineer who has never worked with large language models to immediately build an autonomous multi-step orchestration agent that handles 95% of customer service tickets. That is unlikely to go well and the companies that tend to ask for that are setting themselves up for grief. I hope this has been helpful.
My goal for you has been to help you to develop a sense of taste by focusing on problem structure, not problem hype and not solution hype in AI. To start simple, remember to climb the complexity ladder as you go. To make sure you frame things in terms of ROI, that's going to help executives to really make sense of what you're saying.
To build trust first, which by the way is one of the underlying themes all the way through. You have to build trust with your executive by speaking their language. You have to build trust with yourself when you're making your decisions by making decisions that actually lead to sustainable software. You have to build trust with your customers by making sure that you focus on quality for them and use the right tool.
Data pipelines, machine learning, generative AI, and agents each have their place. You need to know when to pick up the right tool in the toolbox. I hope that this video has given you a sense of the kinds of problems that are susceptible to agents, the kinds of problems that are susceptible to generative AI, and the frankly fairly wide class of problems that isn't either of those things and that we still have in business today and that still needs a good solution.
Good luck out there.

---

<!-- VIDEO_ID: 1Q8CjX0SP0o -->

Date: 2025-09-10

## Core Thesis
AI doesn't cause 'knowledge rot' but necessitates a strategic re-evaluation of reading as a multi-modal skill. The non-obvious insight is that AI, when actively leveraged, becomes a co-intelligence for filtering information, optimizing cognitive effort across different reading depths, and uniquely enabling the creation of novel inter-domain knowledge.

## Key Concepts
*   **The Tripartite Reading Framework:** Reading in the age of AI is not monolithic but comprises three distinct, strategically applied modes: "Awareness Reading" (lightest, for context), "Information Retrieval" (fact-finding, where LLMs excel), and "Conneto Reading" (deep, energetically expensive, brain-rewiring for true understanding).
*   **AI as a Cognitive Energy Optimizer:** AI's primary utility is to act as a co-intelligence for filtering information overload, thereby enabling humans to strategically conserve and allocate their high-cost "Conneto Reading" effort to areas demanding genuine insight and neural pathway formation.
*   **LLMs as Advanced Retrieval Tools:** Large Language Models are best conceptualized as sophisticated "retrieval reading" systems, adept at synthesizing existing information, rather than inherently generating novel, validated knowledge, thus challenging simplistic views of AI "understanding."
*   **The "Sifting Crisis" of Abundance:** The proliferation of AI-generated content shifts the core challenge from information scarcity to an overwhelming "sifting crisis," demanding new skills in quality discernment and active filtering, analogous to the job market's resume overload.
*   **AI as a Domain-Bridging Catalyst for Novelty:** A non-obvious and powerful application of advanced AI is its capacity to "stitch between domains," facilitating the identification and scaffolding of novel connections between disparate knowledge areas, thereby enabling humans to create new, interdisciplinary insights.
*   **Curiosity as the Primal Sifter:** Despite AI's advanced capabilities, human curiosity and passion remain the most potent and irreplaceable drivers for deep engagement, effective learning, and the generation of truly original thought, serving as the ultimate internal filter against information noise.

---

So, this one is by popular request. I've gotten asked a ton. Nate, what do I read? Look at the books off. What do I read in the age of AI? How do I know I'm reading the right things? It feels like reading is becoming increasingly irrelevant. The Wall Street Journal came out this week with an assessment that basically suggests we are rotting out knowledge in the knowledge economy because AI is repeating knowledge and regurgitating knowledge and recalibrating knowledge and there's no actual new knowledge being produced
because people are just asking for summaries. I don't think that's actually true. I think that's panicinducing. But I do think we need to talk about it and I want to talk about it in the context of how we read in the age of AI. And I think this is a big big deal because even if you don't read books, right? Maybe you're on a Kindle, maybe you are reading articles, you are still consuming Hunter, maybe you're in this video.
I think if you are listening to this video, it's like an audible book. It counts as reading. Maybe you're you're feeling like that's being generous, but it's true. It counts as reading. I want to suggest that the way we read is not well enough documented. And that's part of why we're suffering is we don't know how to be readers in an age when we're drowning in information because reading evolved for a selective information age.
We learn to read because we needed to remember things and remembering by writing down works pretty well. And to retrieve it, you have to read it, right? That was the fundamental idea. That's why a lot of the initial records we have for writing are commerce records. You are trying to read and understand.
Court documents are also very early examples of writing. You're trying to read, understand, remember. We have since evolved and we read for all kinds of reasons. In the age of AI, I want to give you three ways that I think we read and learn. And I want to give you frameworks for how you think about this when you have like let's assume you have an intelligent AI counterpart, a a buddy, a chat GPT in your pocket that you're reading with.
How do you use that? Well, so you are not just wasting your time and information goes in one ear and it goes out the other ear. So here are the three frames that we're going to walk through. Number one, I think it's the lightest is what I would call awareness reading. So that's the idea that you just need to catch up on the information of the day.
This might be the person who is scrolling X. This might be the person who is trying to understand what's in the news. This might be the person who is just skimming through a book in the bookstore or maybe skimming through a book that they're going to get a test on and they just need to have enough awareness to pass the test.
Right? That's that lightest layer. The second layer is what I call information retrieval. You also hear it called domain completion. It actually maps pretty well to a lot of what we do with Google. I think it's underthought of that we Google as readers. In a sense, Google is a form of reading because we're trying to go out and retrieve information that has been written down somewhere.
That is the act of reading. And when we do a lot of our sort of intermediate level reading, what we're really doing is we're saying fact A is in a book. Fact A is on a web page. I need to go and get it and retrieve it for a particular purpose. I'm going to use it. I'm going to repurpose it to borrow token architecture.
I'm going to stick the token in and I'm going to produce a new token. Right? That's retrieval reading. And then three, and this is the one that people get like lots of big feelings about. It's what I there's not been a good name for it. I'm calling it conneto reading. Other people call it deep reading.
I like to call it conneto reading because my thinking is that, you know, our brains are plastic. Our brains are forming all the time, evolving all the time, pruning pathways, etc. We are deepening the conneto in our brains when we read deeply. And this is the kind of reading that teachers get very excited about.
This is the kind of reading that everyone worries is disappearing, but it's only one part of this larger reading landscape. And by all accounts, it's a very expensive form of reading from an energy perspective. I was doing some research on sort of reading and the science of reading, apparently, if you're doing that kind of deep reading, you are burning a tremendous amount of glucose in your brain because you're trying to process so much.
And so as much as we may try and lament and sort of put the responsibility for reading more on ourselves or on our peers, maybe part of it is recognizing that this is a very energetically expensive task and we need to make the choice to use that reading skill where it matters the most. And that is the part that I would argue has gotten harder in the age of AI.
And that's what I want to address. I think that we need better tools for filtering and simplifying reading tasks so that we can truly build understanding at the levels we need. We can build awareness where we need it. We can retrieve information where we need it. Uh which also, by the way, I would argue LLMs are retrieval reading.
You're getting answers there. There's a popular term now called answer engine optimization because people needed an answer for search engine optimization in the age of AI, so they made it up. But the idea is the right. You're doing retrieval and you're coming back with information. And then there's conneto reading, right? The the three I want to suggest that there is a way for us to understand in advance what kinds of books we need to dig into in a conneto sense, in a deep reading sense, what kinds we can just skim, what kinds we can do some retrieval on. And
it has never been more possible to do that kind of learning across multiple book types, multiple retrieval types using the assistance of AI. Yes, I unlike the Wall Street Journal, I do not think that AI is bad for reading. I think people who passively use AI are bad readers. And that's a that's a different thing.
So what do I mean here? Let's go to a few examples. Let's for example look at AI literacy fundamentals. The author is Ben Jones and the pitch is pretty simple, right? If if you're overwhelmed by AI, this will help you, right? You can join in the conversation, etc. It is clearly a beginner grade book. I would say just looking at it and this is again I want to talk this through so you get this into your head.
This is an example of a book where there are lots of facts that you want to retrieve and you want to understand. This is a good example of a book where you should have a copy and then you should read with AI as an information retrieval system. So you should say this is my current level of fluency in a prompt. This is the book that I'm holding and looking at.
By the way, I don't advocate not getting the book. I think you should get the book and I want to read only the sections that I'm missing on. Maybe I want to read about how token architecture works. Maybe I want to read about how next token prediction works. Maybe I just want to understand how AI conversations happen in a chatbot.
That's the level I'm at. You can then go in and talk to the AI and say, "What parts of the book do I need to read? What elements of this book speak to that knowledge gap, the thing I'm trying to learn? And maybe what other books do you recommend?" And by the way, I tested this on AI to see how much hallucination I would get as far as madeup books.
It is remarkably accurate. I was working with Perplexity for this. I find that Perplexity is a great search engine here. I only had one instance in about 30 books where Perplexity made up the book entirely. And I mean, is that great, right? You have to check it. I caught it. But the other 29 books were legit and they were good books and there was a great find.
Saved me a ton of time kind of sort of pulling the list together and making sure that I had a fluent understanding because as much as I read, there's books coming out on AI all the time and I wanted that broadstroke view. So I think AI literacy fundamentals is a good example of the kind of retrieval motion I'm talking about where you need to acquire new facts.
Let's talk about something where I think you have a different kind of reading. Let's talk about co-intelligence living and working with AI. It's a bestseller by Ethan Mllik. People have heard about it. As someone who has read that book, that book is literally over here on my shelf. I think that benefits more from conneto reading.
I think that's an example of a book you should read end to end because the narrative builds over time. You may not have a paper book. Maybe it's on the Kindle, but you should read it and you should let it sink in. The good news is this particular book is written very clearly. It's easy to understand and digest.
As much as you're doing conneto reading, it's not complex and hard to read. Now, an example of awareness reading. I actually think a fantastic example of awareness reading is the Washington is the I talked about at the beginning of this video. The one that talks about knowledge rot. I think you just need to be aware of it. It's a great example of something that frankly perplexity could summarize for you and just let you know what the state of the conversation is.
It is not something you need to spend time digging into because frankly the level of effort and investment by the authors in a newspaper article is something that varies a lot and you have to use your judgment about whether there's something new and noteworthy there from an AI perspective that is going to get you value for the time you're putting in to read that article end to end.
And I can sort of hear and feel the journalists that might be listening to this sort of having heart palpitations because like that feels like it disintermediates the the journalist and and the print experience a little bit. I love good journalism. I think there's a place for good journalism and I've been pretty honest about the fact that the number of good articles that deeply understand artificial intelligence, I can count them on like one hand.
And I think that's a larger issue that I would love to see addressed and I'd love to see a larger journalistic conversation about better quality newspaper articles, but I got to be honest about where we are. I think a lot of the articles that I see are not well enough researched to deserve deep reading at this time.
And so I think they go in the awareness bucket. So when you think about it that way, my thesis is that this dramatically simplifies the whole problem. I think if we think of knowledge rot as a problem and we obsess over the the the gap the the so-called rot in the space, we're focusing on the wrong thing.
I think we should instead focus on reading as a skill, recognize that reading is a skill issue that needs to be updated for the age of AI and that the primary way it gets updated is by figuring out how to work with this new co-intelligence to borrow Ethan's phrase to read well to read in a way that we remember to save room for deep reading where it matters.
And I think if we think about it as wow AI is enabling us to produce so many more tokens than we had before. Therefore our filtering problem is really the issue that simplifies a lot of this discourse because then it's really not our fault. Like if you think about it this is analogous to the job market situation where as most people who are in the job market know we are in a world where recruiters are snowed under with resumes that have been published prepared by AI.
the it's a terrible experience for the applicants because the applicants can't get any attention because there's so many perfect resumes now. It's a terrible experience for the recruiters and for HR because they can't figure out how to sift for quality anymore and the whole system has broken down. In that world, AI producing tokens has led to a crisis of sifting and businesses are reading candidates quote unquote by bringing them on site by doing anything they can to get an actual candidate experience.
Some of them are trying AI interviews etc. In the same way, we need to get aggressive about sifting for quality information. And so, part of what I'm doing by sort of building this book list is I want to have a source for quality AI information. And some of it will be retrieval and some of it will be introductory and for beginners and some of it's going to be for like advanced users as well.
Like one of it, one of the advanced ones that I absolutely love, it's from Stripe Press is the art of doing science and engineering. It is not directly about AI. It is a fantastic book. It's by Richard W. Hamming and if you are anywhere in the technical space, I would recommend it. It helps you understand how innovation happens in technical spaces and I think it's a highly relevant foundational text.
So there's there's going to be sort of books like that as well. Please, please, please do not listen to the people who tell you that the problem is information rot per se. Because if you believe that, you are going to get into a position where you think there's nothing you can do about it. The problem is not information rot per se.
The problem is that we need new skills in a world where we have artificial intelligence right next to us all the time. I think one of the key skills is learning when to read for retrieval, when to read for awareness, when to read deeply for conneto reading. The other key skill is going to be learning when and how to put out what we actually think into the world. I'll give you an example.
This is a work of passion for me. I care about reading. Are you surprised? Look at these books. I care passionately about reading and supporting readers, supporting next generation readers. I see the anecdotes coming out of the education system where people are struggling to get students engaged on reading where people in high school and above are saying that students are phoning it in and just using AI to sort of put the essays in and the educational system that was predicated on the idea that people needed to write essays that
people needed to demonstrate their understanding by producing words is just not working anymore. I think that the way back the way back is reigniting passion for deeply understanding a subject. Reigniting curiosity, understanding that there is too much information to process and that wrote assignments were never that effective anyway.
And that we need to challenge people to grapple deeply with things to get them to read. And if you want to learn about AI, your best asset, ironically, despite this being about how you use AI to read better, your best asset is still your curiosity. Your best asset is still your passion for artificial intelligence and the subject matter. That matters.
That matters a lot. And I want to remind you of that because that is empowering. If you are curious, if you are passionate about something, not only will you read better, not only will you learn better, you're going to find your voice. You're going to find something you want to say into the world. Maybe it's about AI. Maybe it's about science and engineering like Richard Hamming did.
Maybe it's about something else entirely. But that voice, that desire to speak back, in my experience, only comes when you are reading enough to prime the pumps. When you are thinking and interacting with the world and taking in information and meditating on it deeply and letting it reprocess inside you.
I don't think the token architecture metaphor works very well for people. I know we use it a lot because that's the predominant tech hype cycle of our age. What we do with information is deeper than that. The way we reprocess, the way we mix it, it's not something that is easily translatable in machine terms. And that magical compost pile is where the best authors have always gotten their stuff.
It is where people who come up with ideas to build a business get their stuff. Except for them, they're reading customers. They are reading the market. They are reading pain points and they're thinking about it and obsessing over it and and studying it. We need that kind of passion in order to put something back in, in order to have the skill to fight the knowledge rot, to have the skill to say something new.
I don't think that that's hopeless. In fact, I have never been more bullish on it because I think that properly used AI can help us to say new things. I'm going to give you an example here. I am as a project as a as a brain stretching project working on learning Swedish. I'm not good at it. Please, no one in my comments come for me and say, "Tell me about your Swedish skills.
They're terrible. I'm learning." But I have a background not just in English, but also in Indonesian. There is nothing in the literature on learning English or learning Swedish if you know Indonesian already. I could not find anything. And so I worked with GPT5 Pro and I put together a way of thinking about Swedish grammar that makes sense to someone who knows Indonesian and a way sort of to walk into learning Swedish.
As far as I know, that hasn't been done before. I'm sure that could be done by someone with a linguistics degree who knows Swedish and Indonesian. I don't pretend that that's something that's like completely impossible for people, but it hasn't been done. And GP25 Pro enabled me, a humble person who is not a linguist, to just go and do it.
That is an example of passion working with an AI to put something out that I can understand and actually like gain a skill set on. It is using AI to be active. And so I want to challenge you. Don't be afraid to work with AI to build new and active things. It is worth trying. It is worth taking a prompt and saying, "Hey, can you please help me with this?" And I will say the advanced reasoning models do a lot better.
In particular, if you are if you were sort of inspired by this and you're like, what what is the takeaway here? I don't do Swedish, but is there something I can learn? Think about it as GPT5 and other advanced reasoning models are very very good at stitching between domains. And so if you have existing knowledge domains, you will have cracks in those knowledge domains.
I described one between Indonesian and Swedish linguistics. No one had bridged that before. It knows all the domains well enough that it can establish a bridge or at least the start of a bridge, a scaffold of a bridge between those domains that ends up being a new connector in the human knowledge set. That's really exciting.
And so what you should do is if you're passionate about something and you cover multiple domains, be a good reader, work with AI, see if you can't connect those domains together in a way that's interesting and novel. This is not the same thing as the people working with AI and claiming they are coming up with novel physics when they are not physicists.
That is a very different thing. I would say that is trying to say that the reasoning models can advance the edges of human domain experience beyond what PhD physicists can do. That is at best unproven at best. And there are articles sort of from physicists explaining why that's challenging. You have similar things with mathematics.
I think physics just gets more attention because people watch Star Trek a lot. So, where does that leave us? Please be a careful reader. Please filter what you want to read. Don't blame yourself if you feel overwhelmed. Figure out what your pathway is for reading and jump into it. I'm going to sort of put something up on the Substack with like learning pathways for different goals in AI.
But really, it's not about AI. It is about AI, but like use it to learn anything in the age when you have a lot of information to choose from. And then when you're passionate about something, think about ways that you can actually put your voice back out there. And maybe it's AI assisted like I talked about with GPT5 Pro and Indonesian and Swedish.
And maybe it's not. Maybe you have the voice inside and you already know. An example of that one, as far as I know, I came up with a sort of retrieval awareness uh deep reading thing. I don't know if other people have done it. It certainly didn't come from AI. It came from me arguing with AI and saying, "You're wrong.
I think this is the way we should categorize it." Out of my own head because I had passion for it. Because I care about reading. care about reading. Learn. Put something out there.

---

<!-- VIDEO_ID: 4F6rTh7d3CM -->

Date: 2025-11-04

## Core Thesis
AI's impact on career trajectories is less about direct job replacement and more about a fundamental re-evaluation of *how* value is created and perceived across different experience levels, emphasizing problem-solving, deep domain expertise, and systems-level thinking as critical human differentiators that AI amplifies rather than supplants.

## Key Concepts
*   **AI as a Problem-Solving Amplifier:** AI is a tool that supercharges existing human problem-solving capabilities and domain expertise, rather than a direct substitute for human roles. The fundamental value proposition remains solving business problems, with AI as an enabler.
*   **The Junior "Problem-Solving Pivot":** Juniors must actively reframe their roles from task execution (often AI-replaceable) to demonstrating unique problem-solving abilities. If stuck in production, the strategy is to show 10-20x productivity using AI, while proactively pushing towards higher-order problem-solving.
*   **Mid-Career Domain Specialization:** For mid-career professionals, deep, niche domain expertise becomes the primary differentiator, as AI increasingly commoditizes general skills. Strategic, gentle career transitions to adjacent domains that credit existing expertise are advised over radical shifts.
*   **Seniority's Strategic Advantage (Systems Wisdom):** Seniors are uniquely positioned due to their "systems understanding" and extensive problem-framing experience. This allows them to rapidly integrate AI as an accelerator for their accumulated wisdom, often making them highly sought after despite initial AI skill gaps, as companies prioritize their deep contextual knowledge.
*   **The "Balanced Talent Ecosystem" (OpenAI Model):** Optimal organizations, exemplified by OpenAI, actively seek a diverse mix of junior creativity (fresh thinking), mid-level domain depth (specialized knowledge), and senior systems experience (holistic problem-solving), recognizing that this human synergy is crucial for effective AI integration and innovation.
*   **The "Hidden Value Proposition" of AI in Careers:** Official corporate narratives often miss the nuanced, level-specific strategies for career growth in the AI era, where demonstrating value through problem-solving for the business, rather than just generic AI proficiency, is paramount.

---

These are the things that I wish we told employees more about AI. In other words, there's a lot of corporate communication going on about AI right now, but it's not all upfront. And I think it doesn't always help you in career growth. And having seen a lot of juniors, a lot of mid-career, a lot of senior folks grappling with the realities of an AI transition, what I'm realizing is corporate communication is often very formal and stilted.
It doesn't give you all the information you need. But because I've seen so many of these transitions at different scales, I can give you a little bit of the behindthescenes perspective to say this is this is what should be said to you, right? This is the reality. So with that in mind, let me give you the straight talk on what is really going on in the middle of these AI transitions for each of these job levels.
I'm going to address honest straight advice for juniors. Things that you should be hearing, maybe you're not, but you should be. Same for mid-level, same for seniors. This is really missing right now. I don't hear it. I don't see it. Most of the advice is generic. I'm going to be really honest and really specific. So, if you are getting started in your career, let's say you're in your first three years, maybe your first 5 years, the thing that you need to hear that most companies won't tell you is that you are in one of two camps. Either you
are going to be treasured because you are considered fresh blood and creative and you work really hard or you're going to be on the chopping block. And I know a lot of folks out there who think they're on the chopping block already, not because the company's told them, but because they've heard it on TikTok.
The reality is that the chopping block happens because the company can't see the value that you bring to the table. And so the non-obvious piece here, the thing that we don't talk about is how as a junior you were able to show problem-olving ability that makes the company recognize you can't just do this with chat GPT.
And the trick is you kind of have to make it up as a junior to earn that in a lot of companies because most companies frame junior level tasks as produce this document, produce this analysis, run this cash flow statement. They're not framing them as challenging tasks. And so they're opening the door for you to be thinking about your work as if it's AI replaceable.
But it's not if you actually understand what you're doing. If you're actually given the chance to do problem solving, you won't look as AI replaceable. And so I say that because one of the things I share with companies is that you need to rethink your career ladders. You need to think about juniors differently because juniors are problem solvers.
They're just problem solvers with less experience. And you need to be assessing them for problem solving ability. Whether that's engineering problem solving or product problem solving or customer success problem solving, but you're still looking at problem solving ability. And if you want people to be seniors in 10 years, you got to hire them now.
There's no other substitute for that. And some companies are figuring that out. Notably, actually, very interestingly, Open AAI is figuring that out. They are actively hiring junior engineers. You might wonder why, right? This is a company that presumably uses AI incredibly well. What's it doing that would make them feel like they need junior engineers? They have found that juniors are very very creative and out-of-the-box thinkers on AI and that they need that particular problem-solving talent because seniors tend to get more stuck in their ways.
Seniors tend to find a particular way they like to solve problems and they just apply AI to that and that's what they do and they want that mix where they have the fresh blood and the fresh thinking. If open AI can think about it that way, everybody can think about it that way. And so I think the trick if you are in a junior role is to start to as actively and aggressively as you can push across the spectrum, right? You're on a spectrum from I just produce stuff to I solve problems.
You want to be pushing as hard as you can toward problem solving. And if you're not, like if you're stuck on the production side, the only option you have is to show that you are 10 or 20x more productive at producing stuff using AI. So like prompts for producing spreadsheets that I've done or Excel or whatever it is that you're working on, show that with AI, you can do so much more.
And that becomes a way for you to sort of secure some career stability even if the company hasn't figured out that your real value is over on the problem solving side. Let's move to mid-career. If you're looking at a mid-career role, what you should be thinking about is how can I very rapidly develop deeper and richer domain expertise.
Typically, we think if we're mid-career, call it 5 to 10 years in, we want to be developing skills that would get us to senior level. The skills piece is easier to get nowadays because of AI, but the domain piece, the expertise piece is rarer and harder to get. And so, if you're looking for something that sort of gives you sense of stability, a sense of hope, etc.
, you want to be doubling down on the particular niche that you're in. Now, I know not everybody's in a niche that they're happy with. I have no illusions about that. I've talked with a lot of folks who are very unhappy. Sometimes they're unhappy with you're unhappy with job roll, right? You're unhappy with the particular niche you're in.
Maybe it's fintech, maybe it's gaming, whatever it is. I get it. The reality is, as difficult as that is, that domain expertise represents years of accumulated experience that differentiate you from juniors. And you don't want to let that go lightly, even if you don't love it. And so, a smarter way to transition is to start to look at an adjacent role or domain that carries with it some credit for the expertise you have and make a gentle hop.
Making a big hop right now as a mid-career person is much, much riskier. You don't know where you're going to land. You don't know if you'll be given credit for your years of experience. Now, if we go back to the skill side, I talked about skills as something that's easier to develop. One of the ways that you can show that is by mapping out your own skill trajectory, particularly in terms of problem solving with AI.
If you were mid-career, you should be able to say, "This is how I'm proactively socializing my prompts. This is how I'm proactively talking with the rest of the group about task decomposition so I can pass stuff to AI. This is how I'm verifying my AI outputs." These are things that were previously only usable for machine learning engineers, right? But now everybody has to do them because we have LLMs.
And at the mid level, you're going to just be expected to know them. There's not really another substitute for that. If we move to seniors, the thing to capture is that seniors have the most grace on AI right now. you I actually know of companies that are changing their hiring practices to not assess for AI because they don't want to miss seniors.
Seniors have systems understanding. They have deep experience 10 to 15 years or more. And this is true whether you're a very experienced PM or an engineer or a CS lead or a sales lead. You're very deep in your space, right? People desperately need that experience. And so I hear a lot from folks who have some gray hairs like me and they wonder what is going to happen to me.
I feel like I'm so experienced that people won't give me a shot. I think it's encouraging to know that people are really reframing their hiring practices to require less AI of seniors so they can bring them in because they know that seniors can learn the AI and they'll have the wealth of domain expertise and the problem solving experience and everything else that goes with being a senior and be able to apply it to AI very very rapidly.
And that is encouraging, right? That is encouraging if you're someone who's trying to figure out how to make that transition because it means that you have a little bit more grace and people are trusting you more. Like mid-level folks, you need to be leaning in on the experience that you have.
Unlike mid-level folks, you get some credit for problem solving ability and sort of articulating things from a uh problem framing and solutioning perspective. and your previous experience at standing up things independently and building systems, all of that you get credit for because you've done it before and you've done it without AI.
And so people are just sort of assuming you can start to lean in and essentially do that with AI. And that's exactly what we see when Open AI going back to their hiring plan also hire super senior people. They want people who are deeply experienced who use AI as well because you can supercharge your own problem solving experience for decades with AI.
Now, the best organizations have a mix of levels. And I'm just going to say it plainly. If you're in hiring and you're not hiring for a mix of levels, you're missing out. And if you think you need justification, just point to OpenAI. They're hiring for a mix of levels. It's what the best people are doing.
And if you're worried that you are not going to be able to figure it out or be able to make the transition, I hope that this video has helped you to name both your level and some of the specific tricks and techniques that you need to use. I'm talking very specifically because I think people get lost in the sauce, right? People think I'm a junior.
I need to have a very active GitHub. I didn't say that, right? I'm a mid-career person. I need to have a personal portfolio website. I also didn't say that. What I'm trying to show you is how you apply your particular experience set to the larger challenge of showing you can solve problems for the business because that is what you will be rewarded on.
And AI is just a tool to do that. And I want to give you a sense that is taken literally from my conversation with Fortune 100 leaders, conversations I've had with entrepreneurs. I see a real range of scales. These are the things leaders are thinking and talking about. These are the skills that are being looked for. This is not usually talked about this frankly. And so I wanted you to have it.
I hope it's helpful for you. Um and I've obviously written up some more sort of in-depth on the Substack on this because I think it's a really important conversation. We need to have like an honest fireside chat that is not just what the company can tell you because this is a really fraud issue. It's much bigger than what one company has.
So, I hope this is helpful. Good luck uh with your career. other.

---

<!-- VIDEO_ID: 61IJSZ6GOuU -->

Date: 2025-10-24

## Core Thesis
The fundamental bottleneck in leveraging AI for business writing is not the AI's capability, but an organization's inability to explicitly articulate what constitutes "good work," define clear intent, and encode underlying business logic. Unmanaged AI amplifies existing ambiguities and degrades informational signal, necessitating a shift from relying on tacit human judgment to rigorous, explicit specification and AI-assisted evaluation, which paradoxically demands deeper human critical thinking.

## Key Concepts
*   **Specification Bottleneck:** The primary constraint in AI-assisted writing is not the speed or capability of the AI model, but the human organization's ability to clearly and explicitly articulate requirements, intent, and quality criteria.
*   **Amplification of Ambiguity:** AI does not reduce ambiguity; it amplifies it. Vague or underspecified instructions lead to magnified vagueness and poorer quality output, rather than AI filling in the gaps intelligently.
*   **AI as a Diagnostic Tool for Information Architecture:** AI exposes pre-existing, "papered-over" information asymmetries and structural vagueness within an organization's communication and documentation, forcing a re-evaluation of underlying information architecture.
*   **Structure as Business Logic, Not Just Template:** Effective AI prompting requires encoding the underlying business logic, decision interfaces, and document goals, rather than merely providing superficial templates that AI will fill without true understanding.
*   **Scaling Evaluation with AI:** To manage the explosion of AI-generated content, AI must be strategically employed not just for content generation but also for systematic evaluation and quality assurance, addressing the "evaluation bottleneck" in knowledge work.
*   **Defining Failure (Negative Examples):** Providing explicit examples of what constitutes "bad" output or common failure modes is a counter-intuitive but crucial method for guiding AI, akin to negative test cases in software engineering.
*   **Informational Signal Degradation:** The "zero cost of information" enabled by AI generation, if unchecked by clear human intent, leads to a convergence on a bland, diplomatically hedged "default AI voice," resulting in a critical loss of valuable informational signal, conviction, and specificity in business communication.
*   **AI Demands Deeper Human Thought:** Counter-intuitively, effectively integrating AI into writing processes requires humans to engage in more rigorous, explicit, and first-principles thinking about intent, quality, and communication goals, shifting the cognitive load from execution to precise specification and evaluation.

---
AI has dropped the cost of business writing to nearly zero. And most of the businesses I work with or know about are drowning in AI documents and have huge issues with AI business writing. This video is how you troubleshoot those. What the principles are for using AI well in business writing situations.
And then I'm going to walk through an actual example of a prompt I'm using that sets a much higher bar for AI business writing than I've typically seen. And we'll just go through it. So, first let's get into the principles. The first thing I want you to keep in mind overall is that the real bottleneck in AI assisted writing is never capability of AI. People think it's the model.
It's not the model. Don't let anyone tell you it's the model. It's organizational ability to articulate what constitutes good work. And typically that means we've relied on individuals to have instincts for what constitutes good work instead of actual structured information about what constitute good work. AI forces tacet knowledge into explicit standards and that is very very hard for most businesses.
You cannot rely on I know it when I see it because AI cannot read your mind. It is not that good. It's never going to be that good. Every quality criteria needs to be concrete enough to specify, to test, and to verify. That is the only way through on the business writing side. And if people say, "Well, I don't have time for that.
" I've got to ask you, do you have time for the business writing you're drowning in? because I have lost track of the number of people who are like I cannot keep up with the business writing. It is too much like people are sending me AI slop at work. Well, the quality criteria needs to be defined to make that go away.
So, what does that actually look like? First, understand that there is a specification bottleneck. The barrier is not how fast can I write this anymore. It is how clearly can I articulate what I need. Every time you have ambiguity in your specs for a doc, that is amplified through generation. It is not reduced. People sometimes think AI can reduce ambiguity by adding detail, but anyone who's worked with AI a lot will tell you it doesn't reduce ambiguity, it enhances it.
And when it adds helpful detail, it makes it worse. So the organizations that succeed are actually not those with the best writers. I know that's very counterintuitive. They are those who can articulate the quality standards explicitly enough to encode them in prompts that you can work with. Now, writers can absolutely help with that. Good writers are hard to find, but we are moving from raw ability to generate text according to a congruent prompt framework or a congruent template as a goal.
And we're getting into a world where we are just needing to specify our requirements really, really clearly. It reminds me of like wearing a product hat and defining product requirements. Very, very similar except now your product is the dock. You also have a fundamental evaluation issue because of the number of prompts.
I've talked in the past about how we have this issue with rsums. Really, it's with all knowledge work in the business. We don't have time to evaluate everything that got done, which means that we have to figure out how to scale evaluation. That is one of the fundamental challenges for businesses that want to go faster. And I believe firmly that scaling evaluation means putting AI on the evaluation side, not just the writing side.
And I want to talk about that a little bit more because I I think we miss that. It is absolutely possible. I've done it. I've written I'm writing a prompt for this article to talk through how you evaluate and build a prompt. Build a Claude skill that helps you to evaluate. I've done it. I'm doing it.
it. You can make your job so much easier if you are just willing to let AI take a first pass and give it really clear requirements on what good looks like. I also want to go beyond just hey you need to specify. The other core issue is that we typically have longstanding information architecture problems in our documents at work that we just paper over and that we are not going to be able to paper over when AI is writing.
Because fundamentally what AI does is it exposes information asymmetries, informationational vagueness that previously hid in a lot of our writing because people wrote it and you just assumed that people were doing their best thinking. One of the good things about the AI age is we now don't assume that everyone's doing their best thinking which means we critique more which is actually healthy.
So when we're critiquing I want us to think of a few things in this information architecture bucket. One documents really ought to be written for goals and decisions and they aren't always. And so if you can't tell if this document enables person X to make choice Y or if this well structured information enables you to make a big decision, if you if you don't know what it looks like and why you're reading it, it's it was going to be useless anyway, right? But now we blame the AI.
Your goal at this point is actually to get rid of that vagueness you tolerated in the past in the business and define theformational architecture of the document. Your structure is the business logic, not just a template. So many times if someone gives you like a product requirements document or if they give you a business memo or a press release, when they give it to a human, you get a template.
You actually don't get the logical underpinnings of the doc and people end up learning it from experience. Templates just let you fill in the boxes without thinking. And when you hand AI only a template in the prompt, that is what you get. And that is why so often when I'm called in to help with business writing prompts, people say, "I don't know what I did wrong.
I gave it the template and it filled out the template and it's terrible. It's crap." Well, you didn't give it the the business logic. You didn't give it a a decision interface to work against. You didn't give it a goal for the document. That's why your writing is terrible. If you don't give it that intent, the business writing is going to fail.
So, the other piece here, this is very counterintuitive. You can't just give it a goal and give it business logic. In my experience, you also have to give good failure tests. You have to insist that you know what bad looks like. Isn't that funny? Isn't that counterintuitive? But if you're trying to tell the AI how to do something well, it really helps if you have five to seven examples of the kinds of quality problems you have with these kinds of documents.
It's like, wow, this technical specification document is really overspeced on the design and insists on a microservices architecture when we don't use that. Great. That's a failure example. Or this press release, it is way too hypy and I hate the hype in this press release. It doesn't respect the actual product capabilities. This executive summary and this executive memo is too vague.
I need more specificity. understand where your organization today fails to communicate information and you will understand how to work with AI to write better. This is I'm going to repeat it again a people problem at root. It is not the model's fault here people. It is our ability in organizations to communicate intent clearly that is governing our ability to work with AI and we're not doing it well.
I want to tease out some of the organizational dynamics too. Specifically, one of the things that I've noticed that's subtle but painful. We are converging on voice because of AI and that is leading to informationational loss in business systems. So, we have an AI default voice and too few people understand how to push that voice into something that communicates their their intent clearly.
I'm not talking about style here. I'm talking about the ability to communicate clearly with what really matters. And I think the default voice that AI has obscures that, right? The default voice is diplomatically hedged. It's pseudo comprehensive. It's stylistically extremely bland. And you don't have the ability to carry conviction with that voice.
If you want to make a bet, you don't have the ability to articulate real specificity, but in the same document to articulate this area is vague and uncertain, and I want to admit that upfront. Good quality writing has that range. And AI, if you just prompt it vanilla, does not. And that leads to critical information loss. And that is part of why businesses feel like they're drowning.
This information is not super high quality. And I'm going to say again, it is absolutely possible to do that. You can make highquality documents with AI. The last thing I want to call out before I go over and I show what I mean is iteration diagnosis. So this is sounds really complicated but very simply we need to diagnose the failure of people to iterate well with writing.
In other words, people are trying to say make it better on their business documents and that is all they're writing and it's terrible and it's not working. But no one knows how to do it better unless they're educated. And what they don't realize is that it is a people problem to communicate intent and that they have to specify their intent more clearly if they're not getting a draft they like.
So I'm going to come back to the like the core of this issue and then I'm going to show you how I'm addressing it with a specific prompt. I actually put together a whole bunch of prompts and a claude skills for this cuz I want you to be equipped and I'm going to show you one of the prompts and how it works.
So the thing to remember is because AI assisted writing is exploding because organizations are drowning. We have that AI generation problem. The cost of information is zero. We need to therefore put a premium on our intent. Otherwise, we degradeformational signal through our businesses and it is hard to make decisions and we feel like we're drowning and it has real career implications and real dollars and cents implications.
I am passionate about good business writing. I love it. It is getting hard to find because people don't know how to prompt. Let's get to an actual. Okay, this is an example of a prompt that I think is highquality. It is also designed to be modular and changeable so you can make it the way you want. Let's get into it. This is for meeting notes.
It's the simplest possible one. I have a bunch of other prompts for more complex docs. Meeting notes are overlooked because most of the time if you go into a generic AI transcript and you get meeting notes is just a generic summary that's very vanilla of what was there. I wanted to be more opinionated because I wanted to carry through the principle that you need to have intent around what you're doing.
So we have contexts, date, attendees which should be pullable from the meeting note raw purpose of meeting input provided and you can paste your transcript there and then you're asking for a very specific output and you can modify this when I tell you why I put what I did. Your goal here is to create notes that help the team execute, right? Execute on what was discussed.
There is a specific goal for this. The notes are used in and then there's a context for this, right? You can decide where they're used. You then have a required structure. Did you make decisions? Do you have action items? Are there open questions that were discussed? What were the key discussion points? The vanilla notes I get. Look, I love granola.
I love some otter. Right there. There's these AI notes. They do not do this. They do not help you encode intent. It is up to you to bring this level of clarity. And I want to help, but there's no substitute for that intent. The AI won't bring it. You have constraints. You have a total length that you have to keep to. You have decisions.
You have to define an owner by name. You have action items. You cannot include pleasantries or general discussions. You may not infer. You may not guess. This is the tone. And then here are validation quality checks. Every decision must have a name decision maker. Every action item must have an owner.
No action item is allowed to be vague. Open questions must have or assigned owners. If any check fails, revise before outputting. Is this perfect? No prompt is perfect. Is this going to get you a long way on intent? Yes. And then I want to get into, and I do, why the prompt works, right? It communicates purpose.
It communicates structure as logic. And you can change that structure if you want a different intent. There's an eval mode. There's a failure mode. Well, let's look at how to customize it, right? And I include that, too, right? For your workflow. You can change it up. You can change it to your organization's voice.
You can have different meeting types. You can have a sprint goal instead. You can have failure modes that are different. And then I can give you an example of an output, right? This is what a good output looks like. This is what a terrible output looks like, right? And this is very similar to the AI notes I get generically.
Frankly, Chad GPT launched a meeting notes feature that looks a lot like that top part. This is part of how I know we're losing good quality business intelligence. Like the bottom example is going to be much more informative for the business than the top. So please, please, I have a bunch of these prompts.
I don't care if you use my prompts or not, but please put intent into your business AI writing. That is the key. And if prompts help you scale that across the business, if Claude's skills help you scale that across the business, I built both of those. That's great. But that there is no substitute. You you cannot get away from the need for humans to define what good looks like for AI and to define requirements.
And to be honest with you, that is the thing I'm excited about. We have sat for a long time with the assumption that human best effort is kind of the bar for docs and we just all have like I worked at Amazon and we had like a bar for docs that floated around best based on the best human writer in the team in the department etc.
You don't have to have that anymore. You can have a really consistent highquality bar and you can know whether someone is writing to that bar or not. And people ask me all the time, well does this mean people won't think anymore? I I dare you. I dare you. Are you going to think less if you go through this process? If you actually define intent for your business with writing, no, you are going to think more. You are going to think harder.
You're going to have to work harder to communicate all of this to people because so much of it was vague and lived in people's heads. Well, not anymore. And the reason why you're going to have to do this is because the alternative is not what we had pre202 where everyone wrote everything. The alternative is AI slot forever because AI is out of the box.
Everyone's using it and everybody I know at work that's drowning in AI docs, which is a lot of people. Well, that's not going to stop. The people making them aren't going to stop because they think it's productive. We need AI education that emphasizes quality, that emphasizes the different ways we need to think.
I hope this video has helped you think about how our brains need to change to communicate effectively with AI when we are writing. Best of luck out there and may you long save and long preserve your business from AI slop and bad business writing. I hope these tips have helped.

---

<!-- VIDEO_ID: 8psoB8EFdc8 -->

Date: 2025-09-20

## Core Thesis
The current era of accessible AI-powered code generation presents a temporary, high-leverage arbitrage opportunity for individual entrepreneurs to build highly specialized software for underserved micro-niches. Success in this unique window demands a fundamental re-evaluation of entrepreneurial principles, prioritizing distribution, disciplined scope management, precise value articulation, and the strategic identification of "AI-proof" problems.

## Key Concepts
*   **Software as Scalpel vs. Hammer:** AI transforms software development from an expensive, blunt instrument for broad markets into a precise scalpel for carving out profitable micro-niches, enabling individuals to build custom solutions previously unfeasible.
*   **Temporary Arbitrage Window:** The current advantage stems from AI technology being ahead of the adoption curve, creating a limited-time opportunity for early adopters before the market saturates or large players integrate similar capabilities.
*   **Distribution-First Entrepreneurship:** Counter to traditional product-centric approaches, success in the AI age for side-gig builders hinges on leveraging existing community knowledge and distribution channels *before* extensive product development, providing a crucial competitive edge against large AI models.
*   **The Discipline of "No":** With AI making software development cheap and easy, the critical entrepreneurial skill shifts from finding ways to build to having the internal discipline to *stop* building, preventing feature creep and focusing on a minimal viable product (MVP).
*   **Granular Value Mapping & Monetization:** In fragmented micro-markets, customer loyalty is low, demanding an extraordinary ability to precisely link specific product features to perceived value and fair monetization, building trust through transparent utility.
*   **AI-Proof Problem Identification Framework:** A key strategic approach involves analyzing problem spaces to distinguish between aspects that are "swapped out" by increased AI intelligence and those that "stay steady" (e.g., complex workflows, physical-digital coordination, multi-stakeholder problems), thus identifying resilient business opportunities.
*   **AI as a Strategic Thinking Partner:** Beyond code generation, AI can be leveraged as a collaborative tool for strategic problem-solving and ideation, enhancing the entrepreneur's analytical capabilities in identifying and refining business opportunities.

---

Okay, this is my video on how to build a side gig in the age of AI. So many of the guides out there are generic. They're not actually practical. We're going to get super practical and I'm going to explain to you why this moment matters and then how you actually go about building a side gig in the age of AI. So buckle in.
Number one, this is a unique moment and it won't last forever. We have an opening and I want to explain why it works in the market right now. LMS are making it very very easy to go from natural language to code. You just talk and you can get what you want. One of the tools I'll mention later in this video is lovable.dev.
It's super easy just to type in what you want and get a working web page. That is something that as much as you may already know about it because you're an AI enthusiast listening to this video, most people don't yet. That is going to change. And during this moment, you have a chance to build software that would not be possible to build two years ago, one year ago.
And what I mean by that is that software has been a hammer instead of a scalpel for a long, long time. It cost a lot of money. It cost teams and teams of developers. You had to go to Silicon Valley and you had to raise money to build any kind of software for most of my career. That is no longer true. You don't have to do that.
You can build software in nights and weekends even if you've never coded before. And that means that you can build custom software for a specific tiny audience. If your audience loves to take notes and they have a particular way of taking notes and you've always had a note-taking system and you wanted to share it with the world, well, you can do that now.
If you wanted to build a fantasy football software that you have never been able to find before, you can do that now. If you wanted to build some kind of special event planning software that you would never have been able to get in the market, well, you can do that now. You get the idea. These micro markets exist, but no one has been building software for them because software is such a blunt instrument because it was so expensive.
By moving from natural language to code, we now have the ability to treat software like a scalpel. And so we can carve out these micro niches and build really sustainable side businesses where we are the authority on that particular tiny corner of the universe for that particular kind of customized software. It has never been possible before and it won't last forever.
It won't last forever because this moment is a moment when the technology is ahead of the adoption curve. And so you listening to this video, you're an early adopter. you have the chance to go out there and look at a niche that you know well and go after it. And my goal with this video is not just to sort of inspire you and give you generic ideas is to give you war stories from my experience as an entrepreneur and also to give you a sense of how it actually works out there through my conversation with founders and others. So if you're starting a
business in 2025, let's assume you get it, you get the leverage, this is the moment, etc. What are your tool sets, right? What do you have to work with that you didn't have before? Well, I'm going to name five tools for you, and I think that you're going to find them incredibly easy to work with in combination, and I want to list what each of them does.
This is the absolute simplest tool set I've been able to come up with, and I want to explain what each does and explain why they're in the stack. Each one earns its keep. Number one, I mentioned it already, lovable.dev. It's no code, it's low code. I want to give you the straight reason why I picked this one. this team ships.
There are lots and lots and lots of vibe coding pieces of software. I could have recommended Bolt. I could have recommended Replet. There are lots of other choices to choose from. I picked lovable.dev because the team ships fast and because they are dedicated to making the product meaningfully better in a in a cadence of weeks.
Like in two weeks, the product will be better than it is today. And that's been true for months and months and months and months. you can bet on that trajectory. And so, Lovable.dev offers you a chance to actually build a functioning site. They recently launched Stripe integrations. They have a back-end integration that works. You can publish to a custom URL.
It is kind of like a web presence in a box. It's really, really easy. There are a few other tools though that you may find useful along the journey. Number two is uh Outa. Why do I recommend them? They're not very wellnown actually. I recommend them because there is one stack there for everything you need from a back-end office perspective.
So you can get user authentication that way. You can get subscription payments that way. You can get a basic CRM or contact relationship management database that way. You can get basic email that way. It's all under one umbrella which makes it really really easy if you're starting out.
And it integrates with Love and you can pull stuff into lovable that way. So they kind of work together. Now, if you're getting to a point where you need to deploy and you don't want to deploy with Lovable, this is optional, by the way, because if you're just getting started, you can kind of just start with Lovable.
But if you want to go a little farther, you can do continuous builds of your software system and easy hosting with a tool called Versel, which is used all over the world by developers. It's very, very famous. It's easy to use as long as you're willing to work with Chat GPT a little bit on the documentation side.
I want to mention two other tools here. Again, both of them are optional. In my view, the only absolutely required tool if you really want to strip it down is lovable.dev. And then out is like the second one if you really want to add some backend office. And then everything else is depending on what you want to build.
And if you want to build something more technical, you add more technical tooling and so on. Two other tools to be aware of. Framer offers drag and drop landing pages and Gemini offers instant AI analysis for free. That's right. You can make calls to the Gemini API for free up to a reasonable weight rate limit for a tiny business to get AI analysis and generation.
So you can actually incorporate a free LLM into your product, which is kind of handy. So that's my basic tool set. It's super flexible. It's like a Swiss Army knife. You can do almost anything with it. And I want to spend some time talking about the philosophy of picking a problem now because in my view everyone is asking me for the tools. So I gave you the tools.
But the really interesting thing is why you pick the problem you pick and how you build on that problem to solve a customer painoint. This is where sort of entrepreneur Nate puts on his hat. The craft of entrepreneurship actually has changed in important ways in the age of AI. And so we talk about the strategic moment.
We talk about the tool set. The tool set that we have as entrepreneurs, our skill set, the things that this moment demands from us, that's also different. First, we need to think about distribution much much more as builders than we used to. So before you would put the product first and you would make sure the product really solved the customer problem and then you would work through established distribution channels.
So if you were in the consumer space, you would figure out how to get into stores and so on. Well, not anymore. Now you have to assume that someone around you is building something that is sort of like what you have and your goal is to get distribution with the micro niche that you already know well.
And that is why at the top of this video I recommended that you use a micro niche that you know well because if you're already a member of that community, connecting with them, talking with them is going to feel natural. You're going to understand their needs. You're going to understand their pain points. You're going to be able to solve their problems.
And you are also going to have a sense of where they hang out and how you can reach them. And that's called distribution. That's distribution knowhow. And that is an edge. That is an edge that no major model maker has. And so part of how you know you can compete with the likes of open AI and anthropic is because you have that distribution knowledge.
So the first principle, the first skill set you need to think about if you're in the age of AI and you're building something, you want to build a side gig, think about your distribution first. What is your distribution advantage? What do you know that other people don't know about your market and where they hang out and what they like and what they don't like and what their pain points are? And are you a member of that community in such a way that they trust you and you're respected and you won't look like you're just scamming people if you talk about a product that
you built? That is really, really important. In fact, it's so important that people building side gigs now pick the product after they pick the distribution channel. And that is completely the reverse of what I was taught when I got started building and building companies, building product, etc. That's that's really different.
Second thing that's different in the age of AI, the second principle of entre entrepreneurship that has changed, we need to have the skill to know when to stop building the product. And that is new because AI will tell you over and over and over and over again, you can build more. You can add a database. You can add a customer relationship management. You can add a new product.
Expand. Expand. Expand. Expand. You need to be able to say no. Before, because software was so expensive, you had to cut just to sort of find a way to make it work. Now software is so cheap. It's very tempting to add an ad and add an and add. You are the one that has to develop the skill to say no.
You have to say this is a night and weekend project. This is the only thing I need to build to show that I can solve the problem. That discipline is something that was imposed by external cost controls before, not anymore. Now it's on you now. It's a skill you have to have. Know when to say no. That is really, really important.
The third major skill that entrepreneurs bring to the table or must bring to the table now that they didn't have to bring before. You need to know in a very fine grained way where value lies in your product. You know how I said software was a blunt hammer before? Like when you buy Salesforce, Salesforce might be really crappy in a lot of different spots, but because it has existing distribution relationships with a lot of big companies, the buyer doesn't really care and so it doesn't get better.
It's different in the micro niches I'm talking about for side gig Builders. Your people are not loyal to you. They will not necessarily buy your product unless it's good and it really solves the problem. And so you have to develop an extraordinary eye for how monetization and price points and what people will pay tracks to specific features in the product.
For entrepreneurs before they could build the whole product and they might not know which thing really worked because the distribution advantage was so blunt like you you send it off to Target and like we'll see and like I guess it works. It sold. Well, now you can tell where on the page people abandoned and also you hear from people in Reddit and Discord chats exactly what they'd like or dislike about your product and they're very very specific and you can see the impact as soon as you change a feature because suddenly the conversion
rate changes. Now, in theory, that was all possible before as long as you were building in software and digital funnels. But now, it matters more because people are less loyal and the market is more fragmented and more people are building. And so, people are like very very disloyal when it comes to customer purchases and what they are willing to buy and what they're willing to invest in.
Which means you have to be extremely good at explaining exactly how your product solves their problem to earn their trust and loyalty. Now the good news is if you can do that, if you can explain why you are passionate about that micro niche, how your product really solves a problem they care about, why the monetization feels fair, like a square deal to them, you will eventually earn their trust, and that's where the distribution advantage starts to become rock solid in your favor.
and it feels like a tailwind pushing you forward. You'd love that. You earn that by building into that distribution wedge with your micro community and actually delivering a product that really solves a problem. And that brings me to the fourth skill. One of the hardest skills in the age of AI is finding a problem that you can be confident AI is not going to make obsolete.
And I want to spend some time talking about this because this one is a little bit scary for people. People often ask, well, yeah, it's easy to vibe code now, but why would I build because OpenAI is just going to take all of the ideas? I don't think they will. If you think about it, their strategy very clearly is to drive a consumer stack.
They want you to spend time as a consumer in OpenAI thinking and doing your day, but they're not the best in the world at some of these peripheral things. Like, they launched a a meeting noteaker. It's okay. The meeting note-taker is not going to compete with the dedicated meeting notetakers. It never is. That's not where their focus is.
Claude is trying to eat a lot of the work primitives. They're trying to eat Excel. They're trying to eat, you know, claude code. They're trying to go after PowerPoint. They want you to spend their day there. Microsoft should be a little bit worried about value disintermediation, but as a as a small builder, you're not that worried about it.
They provide intelligence for you. That's fine. And so, you should instead be thinking about the kinds of pain points that persist regardless of the intelligence of the model. What are the things that people struggle with where a smarter model wouldn't fix it? So, as an example, if you are solving a painoint for someone and it's a coordination problem between multiple strands of software or multiple parts of their day or physical and digital, that is not something that more intelligence necessarily makes go away.
Another example, if you are solving a painoint for someone and it bridges the uh physical world and the digital world. So you're providing a physical service mediated digitally or ordered digitally. That's not necessarily something that more intelligence makes goes it doesn't more intelligence doesn't fix that.
Another example, if you are focused on providing a digital service, but the digital service is predicated on your user feeling like they got stuck in a way that isn't tied to immediate answers. That is a very powerful place to solve for. An example of that, let's say you have to complete a workflow and you're trying to go through and get all the steps in your workflow done at work and maybe it's a a building a product requirements document.
You have to go through each of those stages and build it out. You have to get approvals and then you have to go back and talk to engineers. That is something that is not really changing. Now you may have AI help you do it faster, but the workflow itself is pretty stable. I want to suggest to you that one of the things that is a key for an entrepreneur is to look at the world in those kinds of terms.
Look at the parts that swap out when you add more intelligence and look at the parts that stay steady. In this case, the workflow stays kind of steady. You're still going to have senior PMs wrestling with requirements, wrestling with engineering on technical requirements, wrestling with business stakeholders and trying to figure it out and trying to make the workflow go together.
I know this because I've lived it. Well, in that world, the workflow is a painoint that additional intelligence doesn't actually solve. In fact, it's a it's a mega painoint. It yields a bunch of downstream pain points. Whole companies are built around that painoint. find those kinds of pain points, the workflow pain points, the physical digital pain points, the pain points that are not solved by just adding more intelligence.
That requires some real thinking on your part, right? You have to you have to use your brain a little bit and that's okay. You actually, well, one of the things that's never been easier is to stretch your brain because you have a thinking partner in AI. I'm not suggesting that you do this by throwing pencils at the wall the oldfashioned way, the way I used to.
I'm suggesting you use AI as a partner. And that's why I did a whole writeup with all of the prompts because I want you to be able to understand how to use AI as a thinking partner in this exercise. And I want you to feel like the tools that you have, the prompts that you have can be combined with a philosophy of entrepreneurship in the AI age that actually sticks.
And so just to just to recap so you don't forget if you're starting, one, this is a unique moment. Code is easier to create than ever. It won't last forever. Micro niches are a big part of that. Software was a blunt hammer before. Three, please, please remember to go for distribution. Make sure that you put distribution at the heart of your strategy or else you're going to regret it.
Now, when it comes to the skills that entrepreneurs need to have, you need to remember, think in terms of problem spaces that more intelligence won't solve. You remember, you need to remember to think in terms of your own niche expertise and what is authentic to you. You need to remember to think in terms of a fair and square deal on monetization.
So you understand the exact levers that your product offers and how monetization feels like a square deal and feels honest and that's how you build those trusted relationships over time. You need to think in terms of bets. Think in terms of a simple MVP product where you had the discipline to cut it down to the bones.
you can see that it solves a problem and you're able to launch it in good time and see if it works. This micro niche window is not going to last forever. Your next weekend project can get something out the door that actually earns you money. And I'm not going to pretend that your next weekend project is going to earn you a billion dollars, but I think it's fair if you actually discipline yourself and start building on nights and weekends to get to a point where you can build a little piece of software that gets you into the hundreds and thousands of
dollars a month. That is totally doable. I know lots of people who have done that. I know people who have scaled past that into five figures a month. You can do it. You just have to make sure that you understand how entrepreneurship actually works today, what the principles are, where the opportunities are, and what the toolkit actually looks like. I hope that this has been helpful.
The full write up with all the links is on the Substack. And yeah, I hope you have a good time. I love building. It has never been a better time to build. And good luck out there. Good luck building. I hope you come back and let me know what you are working on.

---

<!-- VIDEO_ID: 9IETDveRCQs -->

Date: 2025-11-09

## Core Thesis
The widespread failure of AI initiatives stems from a fundamental "perception gap": executives mistakenly believe their organizations are data-driven, while their underlying data architectures are critically unprepared for the real-time, context-dependent demands of AI. True AI readiness requires a pragmatic, infrastructure-first approach, prioritizing foundational data quality, semantic context, and automated governance over superficial AI tool adoption, acknowledging that this "boring work" is the strategic imperative in an exponentially advancing AI landscape.

## Key Concepts
*   **The "Perception Gap" as the Root Cause of AI Failure:** The critical disconnect between executive belief in data-drivenness and the reality of unprepared data architectures, leading to misaligned expectations and project failures.
*   **Agentic Retrieval vs. Slow Retrieval:** A fundamental distinction in data access patterns required by AI, demanding real-time, performant queries (e.g., under 5 seconds) over traditional batch processing designed for human reporting.
*   **Zero-Copy Architecture as a Philosophy:** An architectural approach that emphasizes querying data where it lives for real-time access, requiring significant internal capacity for architecting systems rather than merely adopting specific tools.
*   **Context Over Volume (Semantic Layers):** The counter-intuitive insight that raw data volume without rich business context (encoded via semantic layers) leads to "confidently wrong answers" from AI agents, highlighting the need for meaning beyond mere data points.
*   **Governance as an Enabler of Speed:** Re-framing data governance from a bureaucratic process to automated quality monitoring and accountability, which, when properly implemented, accelerates rather than slows down AI adoption.
*   **The Exponential Nature of AI Progress:** Highlighting the human cognitive bias against understanding exponentials, underscoring that delaying foundational data work leads to exponentially greater competitive disadvantage in a rapidly accelerating AI environment.
*   **"Boring Work" as Strategic Imperative:** The counter-intuitive finding that success in AI is not about advanced models or prompting, but about the disciplined, often unglamorous, work of fixing and optimizing data infrastructure.

---

This week's executive briefing is all about the glamorous subject of AI ready data architectures. Why? You might be thinking, are we going to do that? The reason is simple. Salesforce published data this week from 6,000 plus enterprise data leaders. 84% said their data strategies needed a complete overhaul before AI works.
And at the same time, 63% of executives in the survey believe their companies are already data driven. In other words, the perception gap is why most AI initiatives are failing. Leaders walk in to these AI conversations assuming they are datadriven. And what they find in reality is that they're not. Is that the data is not ready for the demands of AI.
And so I ask myself, why is that? I look at my own projects I've worked on, projects I've worked on with clients, and I ask, "What can we learn here that will help leaders get over the perception gap and be more likely to establish a successful AI ready data architecture?" So, we're going to go through seven principles that separate the call it 16% who are successfully scaling AI over everybody else.
Number one, diagnose before you deploy. So before you run your AI initiatives, you need to be running tests. And there's two big ones that come to mind. Number one, make sure that your system can answer factual questions about your data in less than 5 seconds without human intervention. Am I making it up as like it has to be 5 seconds? Kind of. Yeah.
But it's a reasonable proxy. Essentially, if you cannot put in place a system that is able to get a performant query that is very simple through the system in less than 5 seconds, you're probably not ready for anything more sophisticated. And I'm talking really simple like what's our inventory for product X in our warehouse houses right now.
Something at that scale. Second, the second test is can you assemble a complete customer view across sales, support, billing, and shipping with no missing data in a similar time envelope. And so, you notice how I'm asking you about performance at the beginning. There's a reason for that. If the tests fail, what you're finding is that you need infrastructure work because your data sets are not designed to be performant in the era of AI.
They're designed for slow retrieval, retrieval that might take 30 minutes and be one row in a large report your data analyst prepares, not agentic retrieval that happens on the fly very quickly. And so the challenge for you is to figure out how do you start to change your data architecture so that you can actually deploy AI agents reliably.
So the first step just be honest with yourself. Diagnose before you deploy. Principle number two, zero copy architecture is a philosophy. It's not tooling. I'll get into what that means. So, traditional data warehouses copy data to central locations. They clean it up overnight. Then they make it available for reporting.
But depending on your needs, Agentic AI cannot wait for overnight batch jobs. If you want real time data today, that won't work. So agents need real time data access to authoritative source systems if you want real-time conversations with your data. Now if you are okay with having day delayed or longer data and you just you're fine with that and most of your use cases are with last week's data or last month's data, this gets easier.
But even in that situation, you still have a lot of work to do to make sure that the data sets that you prepare of your historical data are performant in keeping with principle one. The key shift to think about is that the behavior of the business user is also evolving. So part of why in the Salesforce survey, companies were 34% likely more likely to succeed if they used a zero copy approach, which is where you don't copy it to a central location.
You just tell the AI to query the data where it lives in all of your different systems. Part of why they were more successful is because they were able to architect the entire system exactly the way they wanted without trying to buy from different vendors and cobble a system together. And so this is in line with other surveys we've seen of executive leadership that have emphasized that it is really important for executives to invest in internal capacity for architecting systems if you want the ability to build and sustain AI long term. And so as much as principle number
two is about, hey, we're not going to have copies of our data. We're going to be able to query real time and that's going to make us more likely to be successful because business user behaviors are shifting toward real-time querying. All of that makes sense. But the underlying story that I think that is interesting is that that only works if you are investing in your ability to build those systems internally because your own fingerprint configuration of data is going to be somewhat unique and will require that internal capacity.
Principle number three, context matters more than volume. So 49% of organizations draw incorrect conclusions because their data lacks business context. again drawing from this Salesforce survey. So as an example, let's say that the data says revenue is $2 million and the customer is Acme Corp and it's the third quarter revenue number, but maybe that particular database row or that particular fragment retrieved by the AI does not specify that the $2 million was a one-time contract, not recurring revenue, and your agents are now
inappropriately adding $2 million in ARR to your reporting. What I'm saying is that is not uncommon and that is a great illustration of how the context really matters when you are trying to construct efficient agentic systems. You have to get context that works. So you need semantic layers that encode business definitions, relationships and logic.
Asians should be able to understand the difference between booking and revenue, between gross and net active churn customers from universal single source definition sources of truth. And if you don't have that, if you don't have a semantic layer that defines the meaning across the top of your data at a high level, then the more data you add without context just means more confidently wrong answers.
If you're wondering, can you give me an example of a semantic layer? I would actually say there's vendors that do this and this might be a case if you don't have the capacity internally where a vendor makes sense because in this sense you're not asking the vendor to patchwork across all of your data.
You're just trying to put a single lens across the data set and interact with it cleanly. And so up to you cube is a vendor out there. There are other vendors out there as well. The principle is not whether you go with a vendor or not here. The principle is that you need to think about your data queries as needing the context that comes from executive insight to interpret well.
If you ask these kinds of sophisticated questions and most of the time the data that you have encoded does not carry that context internally in the rows of the database. You have to find a way to add it. So if you want to build that yourself and find a way to add that in that's great. Or this might be a spot because there's only one sort of one point in the architecture to inject this.
Maybe it's not irrational to add a vendor here. Principle number four, governance enables speed. It does not slow you down. And so this is one of those things where you need to have governance framed as accountability rather than process. And so governance councils at enterprise scale need to be accountable to quality metadata policies, remediation approaches.
And your goal is not to push things through human processes, but to actually have your governance structure encoded as automated quality monitoring so that you can route data issues through the exact same severity model you would use for production software outages. This is not bureaucracy.
This is what lets you move quickly if you automate that process. The 16% successfully scaling in the Salesforce survey are doing so with strong governance practices. And there's not really a substitute for it. And I and I want to emphasize again, if nobody owns data quality, the data will not be quality.
You need someone to care about data quality in order to ensure that you can actually make the most of the AI investment you're making. But the person who owns it needs to have an automation mindset because otherwise they're going to slow things down. And so that's why I'm trying to give you the nuance and the tension there.
So principle number five, the honest timeline is not as fast as anybody wants. The plan in most enterprises is probably 18 to 36 months and you're showing progress in phases. And so you might be fixing critical pipelines. You might be implementing a zerocopy architecture for your top domains. You might be piloting agents where data is trustworthy in year one.
Year two, you may be expanding toward real-time capabilities. You may be automating governance year three. Now you're scaling agents. This feels really slow when vendors are going to promise six-month transformations. But those timelines assume your infrastructure is already ready, which is exactly what Salesforce is calling out here.
For most organizations, it's not. And so the disciplined approach is to be honest about your shortcomings and fix infrastructure while running AI pilots that show proof of concept and then scale as you see that the foundations are actually solid. Principle number six, you want to close the perception gap between business and technical leaders.
The fact that 63% of executives think they're data driven underlines this business technical gap because at this rate, business leaders are making purchases for vendors selling AI tooling on timelines that are completely divorced from the reality of the data architectures. Business and technical leadership need to get on the same team. Otherwise, they're going to end up blaming each other, blaming AI, or blaming their own teams.
Misaligned expectations here are a leadership issue and it's something where technical leaders I am convinced need to take the lead. Step up, educate your executives on the technical realities that are constraining AI development in your organization. Be really honest. Make the infrastructure work visible, not hidden beneath the technical teams.
This is not a time to hide the dirty details from leadership. You actually need that to avoid writing checks you're going to regret and making promises that you can't deliver on. Principle number seven is all about strategy. The strategic choice that you're facing is time bound. I keep emphasizing this and I'm going to say it again. Data runs on a clock.
If you are going to have to spend 18 to 36 months regardless in the middle of the AI revolution fixing infrastructure and scaling AI, it is better to start that clock sooner than later because you are going to fall exponentially farther behind the longer you wait. This is a point I make often, but it's a point that we have trouble as humans processing.
So I feel the need to repeat it. We have trouble with exponentials. We are living in an exponential from an AI capability perspective. And when we are living in that space of a curve that's accelerating, we have got to be okay with making bets assuming that the environment is going to continue to accelerate. We need to make bets assuming AI models are going to keep getting better.
That means we need to frontwate and really prioritize investments that unlock that AI capability. And I see that willingness, but as we've discussed in this video, it's often business leaders who are overoptimistic about the boring parts of the organization, namely the data stack, who are saying, "We want AI. We're ready to invest. We're buying vendors.
" And that leads to problems. And so the strategic choice is now, but technical and business leadership need to get on the same page to make sure that the willingness to invest is aligned with where you actually need to put technical leverage to drive AI forward in the business. The organizations that successfully scale AI aren't smarter about prompting.
They're not smarter about model selection. They fixed their data infrastructure. They did the boring work first. They ran the diagnostics. They accepted an honest timeline. They did work. And that's how you close the perception gap between we're data driven and actually our data can't support AI and this vendor implementation is doomed from the start.
That is an execution gap. That is something an organization can choose to close if it wants to. And if you don't close it, you are going to regret it. So there you go. That is how I read one of the most interesting surveys of the year out of Salesforce. Good luck with your data architecture. We all need it.

---

<!-- VIDEO_ID: 9m1Bd6cxYBk -->

Date: 2025-10-26

## Core Thesis
AI adoption failures are primarily preventable organizational and leadership failures, stemming from a fundamental mismatch between AI's dynamic, systemic demands and traditional, often siloed, corporate practices. True success requires a proactive, holistic shift in strategy, process design, and talent development that prioritizes coordination, systemic risk, and human-AI workflow redesign over mere technical implementation.

## Key Concepts
*   **Coordination Cost as a Primary Budget Item:** The true cost of AI integration extends far beyond development, encompassing the significant overhead of aligning diverse organizational functions (legal, compliance, sales, IT, HR) and requiring dedicated "deployment PMs" to manage this complexity.
*   **AI Governance as a First-Class, Specialized Discipline:** AI security and governance are distinct from traditional IT security, requiring new skill sets focused on understanding agentic behavior, blast radius, failure modes, and novel attack vectors like prompt injection from "day zero."
*   **AI's Role: Augmenting Judgment, Not Just Generating Output:** Counter-intuitively, AI's highest value often lies in assisting human judgment and review, rather than merely accelerating content generation, which can create unmanageable "review bottlenecks" if not designed for human-in-the-loop collaboration.
*   **The "Intern Suitability Test" for AI Readiness:** A practical mental model for task decomposition, where AI is best suited for tasks that a "smart but forgetful intern who can't learn" could handle with clear instructions, retrieval, and formatting, leaving complex judgment to humans.
*   **The "Mechanical Horse Fallacy" in Automation:** A critical framework highlighting the danger of simply automating existing, potentially inefficient processes rather than fundamentally reimagining workflows and focusing on desired outcomes and "zero process" versions.
*   **Strategic Portfolio Approach for AI Investment:** In a rapidly evolving AI landscape, traditional long-term strategic planning is obsolete. A diversified portfolio of AI bets (fast payoff, medium-term, learning investments) with clear gates and continuous monitoring is a more resilient approach to mitigate "existential paralysis."
*   **"Slow is Smooth, Smooth is Fast" for AI Scaling:** Countering the "move fast" mantra, successful AI scaling requires a deliberate, staged rollout, rigorous testing with skeptical users, and robust support infrastructure, especially for edge cases and messy organizational problems, rather than premature company-wide deployment.
*   **Training on Workflows, Not Tools:** Effective AI training focuses on integrating AI into specific job functions and workflows (e.g., "how to research competitive intelligence using AI") rather than generic tool proficiency, fostering deeper capability, network effects, and adaptability as tools evolve.
*   **Data Quality and Ownership as Foundational AI Infrastructure:** Neglecting data access, quality, and clear ownership is a critical, often overlooked, failure point, as AI systems are fundamentally limited by the integrity and availability of their data "fuel," necessitating a full data audit and dedicated accountability.

---

It seems like AI adoption fails more often than it goes right. I want to take today's video and talk specifically about the nine different AI failure patterns that I've seen in organizations over the last few months in 2025. I want to get at not just what happened, but what is the root cause? What are the things that make that pattern sticky, that fail pattern sticky, and then what is the actual fix that unsticks an organization and gets it back on track.
I don't think we talk enough about the categories of AI adoption failure and I want to lay them out really cleanly so that we have a vocabulary to talk about them to address them and ultimately to get back on track. Let's start with number one, the integration tarpet. Let's say engineering ships working AI code in weeks, but sales and legal and compliance cycles were never meant to run that fast.
They stretch into months or longer. Cross team stakeholder meetings are multiplying all over the business talking about policies and approvals. The root cause here is that the organization's budget for AI development was structured in terms of dollars and cents and not in terms of coordination cost. When you are working with a prototype, it's very simple.
But a prototype does not equal a system that fits data architecture, compliance, and politics together. Yes, organizational politics are relevant. So why is this sticky? Everyone assumes if it works technically then deployment is going to be easy but integration complexity becomes visible only after the build is is complete right you can see it working you can see that deployment is easy executives tend to not understand why it's not being used why we're stuck on paper the committees will make sense the IT policy will make sense and none of it
actually delivers value the fix is pretty simple take approval paths take policy paths as critically as you take writing code. You want to fix and pre-wire in how you are fasttracking adoption through the organization before you jump in and start just saying we can ship this thing quickly. Treat the human problem as significant as the code problem.
you want to assign someone, maybe it's a deployment PM whose entire job separate from the engineering piece is just to say, do we have all of our ducks in a row process-wise to actually get this into people's hands? Do we have data support and approval? Do we have legal clearance? Do we have any concerns around compliance we need to address? Do we have any concerns from HR that we need to address? They're not asking, does the model work? That needs to be somebody else's job.
All they're trying to do is to wrangle the stakeholders and move them quickly along. You have to budget for the organizational side to get out of the integration tarpet. There is no shortcut. Failure number two, governance vacuum. Let's say red teams find vulnerabilities. We actually had that happen this week with teams finding vulnerabilities in AI powered browsers.
Security will flag an unapproved architecture when a red team vulnerability is found, but there's no owner for what happens if AI does something. And so in this situation, if your ordinary red team, your ordinary security issues are triggered by AI, you often run into a governance issue. Really, there's no one who's a directly responsible individual who treats AI governance as a first class object.
And that is your core problem. That is why when small vulnerabilities are sometimes found in your agentic systems, you get stuck. When they're found in your implementation of a custom chat GPT, you get stuck. You have to treat governance as a first class object. And that is especially true if you are in a high compliance industry.
And here's the trick. You probably knew that if you were in high compliance. But what you may not have realized is that the skill set for governance and AI is different from the skill set for a lot of typical IT security projects. And that is why this problem is sticky because people often try and address it by saying let's give it a security review.
Let's give it a software review. That feels like bureaucratic slowdown. It looks like bureaucratic slowdown. The teams that are addressing it don't have the tools to do it right. So teams just kind of go hands off and one incident ends up freezing everything. A governance vacuum ends up grinding your system to a halt. The fix is simple.
You need to embed the right talent with the right tools to make security a day zero problem. That means that you have to think about what the AI agent can access, what it does, what its blast radius is, what failure modes look like, how you architect security rather than making security the agents problem with decisioning, how you can reliably evaluate whether the agent is doing the right thing, how you test in production systems a range of utterances or words from the user that let you know that if there are things the system should not be responding to,
prompt injection attacks, etc. You can prove that you're addressing them correctly at the desired rate of success. If you don't make all of that somebody's problem, and it is again not a traditional security software purview. It's a new set of skills, you're going to be in trouble. Failure number three, the review bottleneck.
AI will generate output so fast. I've talked about this before, but human review doesn't get shorter just because AI gets longer. And so output quality starts to vary super wildly. and engineers or other job families end up babysitting AI systems. The root cause here is that you have stuck AI as an engine onto the wrong part of your workflow, generation instead of judgment.
So, organizations will usually measure success by how much you can produce. And so, the instinct is to just stick a bolt-on engine of AI generation onto the generative part of your process. Maybe it's making social media stuff. Maybe it's writing documents or breaking out tickets. Maybe it's writing code for code reviews.
We think how much it can produce matters. It's sticky because impressive demos look so good when they show speed of generation. And a lot of people are stuck in the mindset that that is the KPI that matters and review burden is hidden. You don't see review burden in a demo. You need to design systems that are human in the loop from the start.
I've said this before. Your best humans should feel more fingertippy on your work because of AI, not not fighting AI. So AI should be able to draft useful pieces of work, whether that's code or something else. And a human should have comfortable capacity to review that work.
That means being clear about what your AI scope actually is, what the scope of your AI assistant for this task actually is. And it means getting serious about how much of a review burden your AI system imposes. If you have someone and all they're doing is just hitting merge on AI generated poll requests on your codebase, you are extending vulnerability into your system because you refused to think about the review bottleneck.
And there are real security implications and yes, systems really do that. Please, please, please take the time to look at whether you are architecting your system for review and for putting expert humans in touch with the work or not. Number four, the unreliable intern. Let's say AI handles 80% of a task perfectly and it fails catastrophically on the last 20%.
And you can't predict when failures are going to occur. Supervision costs may approach the cost of just doing the work at that point. The root cause here is that AI lacks judgment and memory and context for what you need specifically and organizations keep trying to deploy AI on tasks that aren't AI ready yet as a result.
Part of the risk here, part of why this stays sticky, is that the 80% success rate in this situation, which is like real, it feels close enough to keep trying. Teams assume just one more tweak is going to fix that issue. But the real fix is actually simple. You intentionally audit the task for intern suitability before you decide if it's AI ready.
In other words, you ask yourself, would I give this to a smart but forgetful intern who can't learn? If I give them a clear task and clear context and a clear and a clear structure for the ask and output, could they do it? Break complicated tasks into subtasks. You want AI to do the retrieval and formatting. You want AI to do these sequential steps that are clear and you want humans to be able to offer that review as I said earlier.
So be really explicit when you're going through this audit. I know this doesn't sound fun, but part of the ask when you do AI automation, well, when you are trying to unstick adoption blockers, it's an ask to extend organizational intent. You have to be clear about your intention for what tasks are suitable. And there is no substitute for going into the nitty-gritty and looking at those one by one.
Failure number five is the handoff tax. AI can handle one step in a multi-step process. Handoffs between AI and human are not fully worked out and the system design and overall cycle time barely improves. Sometimes it gets worse. The root cause is you again you automated the wrong part of your workflow. You optimized for one bottleneck and you created two new ones on each side cuz you didn't think about your on-ramps and off-ramps.
This is sticky because the per step improvement, the KPIs are going to look great. Wow, we took our per step for this drafting stage down by 200%. Well, you have to take full cycle time for workflows very seriously or you are going to discover how bad this is too late. The fix is simple and again it comes down to intent.
You have to map the full intended workflow before deploying AI. Redesign it so AI can handle the on-ramps and off-ramps of all the components it needs to touch so that you are not creating new bottlenecks at the edges of AI systems. And then you want to measure cycle time for the whole process. If cycle time improvement is not moving, you probably have an issue with the edges of your AI system and how it hands off to humans.
And yes, that is going to include training your humans in new patterns of work. Number six, the premature scale trap. Let's say you have a successful pilot and it's pushed rapidly to companywide. You want to double down on what's working. Edge cases will immediately multiply. Support costs are going to explode and quality is going to degrade.
You, it turns out, were not ready for companywide roll out. The root cause here is that you usually have a much more controlled environment for pilots with motivated users and very clean data. This is almost a function of the purpose of the pilot. You pick these pilots because they're easier and people magically forget that when they go to roll out wide.
The pilot team probably understood AI limitations and worked around them in a way the broader org doesn't and can't. This is sticky because leadership is just wired to seek a quick win on AI and they want to capture value fast and they're optimizing and it feels like testing and doing a gently scaled roll out is just unnecessary delay and not moving quickly in the age of AI. Well, I've got news.
Sometimes slow is smooth and smooth is fast. The fix is honestly to document what fundamental differences exist between the pilot environment and the real environment. So document all of the workarounds that your pilot team used to achieve the results. Those become training. Document with skeptical users, not with enthusiastic ones.
How do they use the tool? Research, understand, is it working? Make sure that you try a second pilot on a hard problem in the messiest part of the organization. Does that still work and deliver value? Then start to dial up in stages, right? Maybe you go to a 100 people and then 500 people before you hit 50,000.
Right? You want to build support infrastructure in terms of people, learning and development opportunities, giving very very clear approval, disapproval, bug reports, lots of feedback opportunities for the software as you roll out. And then you want to monitor, right? If you get from five people or 25 people in a pilot to 500 and your support tickets are increasing per user, then you are not ready to go farther.
You have found an edge case you need to resolve. take the time to do it. There's no shortcuts. Number seven, automation trap. Let's say AI speeds up existing processes. Great, but it doesn't change outcomes. Activity increases, results don't. You have you have successfully automated inefficiency. Congratulations.
The root cause here is you deployed AI before asking whether the process should exist at all. You automated approval workflows that maybe shouldn't require approval. Right? There's a lot of other examples of this. This is sticky because we have the mechanical horse fallacy. That's the idea that a new technology should look like the previous one in the way that a car should look like a horse. No.
I know it's easier to automate what you're already doing than to reimagine work. But the value comes from reimagining work. Before deploying, ask, should we be doing this at all? And then you want to look at outcomes that will stay steady regardless of the process behind them. Those are your north stars. So you want to look at customer satisfaction.
You want to look at the efficiency with which you can do a job in a way that is steady regardless of the particular technology used. And you want to look at the perhaps business metrics that you can drive given a piece of workflow you'll automate. Whatever it is, make sure that you prototype as close as possible to a zero process version.
Ask yourself, what if AI dropped this workflow? Would it work? You may not find that it does. You may find you need the workflow. N I can only take certain pieces of it. That's a great answer. But if you don't ask the question, you run the risk of a mechanical horse. You run the risk of an automation trap.
And then ask yourself how you'll know when it's time to go the next step. That's sort of how people start to really build value. They look at these AI agent systems as evolving. They look at this north star of customer satisfaction or topline revenue and they say AI can't drop this process yet.
It can do two parts out of six. We are going to come back in a quarter and see if we can get the whole thing because AI is getting better. Number eight, existential paralysis. Leadership is debating whether AI will cannibalize the core business and you get conflicting directives from senior leaders. You have strateg strategy discussions after strategy discussions that loop without decisions.
This happens a little bit less often than some of the others because I think the FOMO and the bias for action are real in this space. But fundamentally, I have been in these rooms. I have seen people worry about the risk of AI to the point where they take no action. The root cause is that AI's pace of change is dramatically outstripping traditional corporate strategic planning cycles.
And so, by the time you have built your careful 5-year AI strategy that feels steady, the landscape has shifted and it's already outdated. That has happened multiple times to organizations in the last two years. It is part of the reason why organizations are regretting building custom models back in 2022 because now they've launched them in 23 24 and now they're regretting it because the the cloud provided models are so much better.
Their AI strategy stayed still because it was on a corporate planning cycle and the market shifted. Exist existential paralysis killed them. And this is sticky because outcome unpredictability makes every single decision feel really high stakes. So more analysis feels much safer than making a bold commitment. Well, the fix is simple.
You can take if you're a conservative organization, if you're not ready to make a truly bold burn the boats move, which is a way that startups are addressing this and having great success. I don't want to not call that out. You can adopt a portfolio approach. If you're feeling more conservative, you can allocate your budget across different horizons, a fast payoff mode, a two to threeear bet mode, etc.
And you don't have to predict which ones wins. You can diversify your bets. You can set speed targets like getting to complicated AI questions answered in Slack within 90 days and getting to truly agentic CRM automation for leads in 8 months, right? Like you can have different horizons in different bets and measure them differently.
You want to also be clear in the portfolio bet world that you can have learning investments and scaling investments and that you have clear gates to get to scale. Essentially what I'm saying is if you are not a burn the boats organization, if you are a more cautious organization, which is where this happens, then you should be thinking about it as an investment in a series of equities and you don't know which one is going to be a runaway success.
But, you know, failing to invest will certainly prevent you from getting a runaway success. And so, you need to balance your bets across all of the different equities you've got, watch their trajectories, and double down where they're working. And that requires a different decision-making cycle from leadership. And that is the only way I've seen that these kinds of existential paralysis organizations start to get themselves together.
Finally, number nine, the training deficit and the data swamp. Two sides of the same coin. You have low adoption despite tool availability. Users revert to old workflows. Do you know why? Because AI can't access needed data and data quality issues only surfaced after you deployed the tool. The root cause here is that you deployed the tool and taught people to use the tool and never bothered to think about the data issues that the tool was surrounded by and it looked okay in training.
Data infrastructure work is not fast. It doesn't ship in weeks. It's typically boring. It's expensive. It's slow. It's very difficult to fix data problems and most organizations opt to skip it if they can. This makes it sticky because training is treated as just one-time onboarding and you're not really thinking about how you build the capability to solve problems with data using AI tools for your employees.
So, there's a mindset shift you have to have along with a commitment to data integrity. And so, you have to think about how you're upshifting the data to meet AI needs. and also how you're upshifting training so your team can take advantage of the AI tooling once it's connected to data. I know AI deployment is exciting and fast, but if you deploy without paying attention to the training and the data availability, you're going to be in trouble.
You should allocate like I'm not kidding 3 to 6 months of expected training at enterprise scale before you start to think about ROI. You want to train on workflows, not tools. So, you want to ask, "How can I teach people to research competitive intelligence using AI?" Not, "How do you use chat GPT?" Here's a handy twoline hack for a research competitive intelligence prompt.
It's a deeper conversation. You can't assume the tools will be the same over time. So, you want to focus in your training as you're starting to build people up, focus on your AI champions, focus on the ones who can teach their peers because that is going to enable you to trigger network effects that will enable AI adoption to spread faster.
On the data side, you're going to need to do a full data audit. You're going's to need to prioritize data access and you're going to need to assign clear data ownership so that someone is accountable for making sure data is available for the AI. What do we learn as we look across all nine of these? I'll tell you the one biggest takeaway that I have after seeing all nine of these play out in organizations over the last few months is that AI adoption remains a preventable problem.
If you're having issues with your AI adoption, it's on you. Leadership is responsible for establishing the kind of intentful, thoughtful best practices that I'm describing here that keep you out of these failure modes. And when you run into them, you got to be honest. You got to say, "Here's the root cause. Here's what's making it sticky.
Here's what we can do to get ourselves out of the mess." That's why I made this video.

---

<!-- VIDEO_ID: AOl5bNDf1wE -->

Date: 2025-09-19

## Core Thesis
The proliferation of AI tools has paradoxically degraded the signal-to-noise ratio in talent assessment, making it harder for companies to identify promising junior talent, while simultaneously creating an urgent, yet often unrecognized, long-term need for "AI-native" junior hires to ensure organizational resilience and human-centric product delivery.

## Key Concepts
*   **AI's Signal Obfuscation in Hiring:** AI tools, used by both applicants (perfect resumes) and recruiters (initial screening), create an "arms race condition" that paradoxically *hides* genuine talent signal, making it harder for human hirers to develop "gut level conviction" about junior candidates. This is a degradation of information quality, not an improvement.
*   **The "One of One" Strategy for Juniors:** In a high-noise, AI-saturated job market where generic optimization (perfect resumes, LinkedIn connections) is ineffective, junior candidates must differentiate themselves by identifying a unique niche or "power curve" where they can become the undisputed expert ("one of one"), thereby breaking through the commoditized application process.
*   **Strategic Redundancy & Long-Term Talent Pipeline:** Companies adopting a short-term strategy of relying solely on senior talent and AI agents risk future talent gaps due to high tech tenure turnover. A "smart redundancy" approach necessitates proactively investing in "AI-native" junior talent to build a sustainable, resilient workforce.
*   **The Enduring Human Element (People for People):** Despite AI's capabilities, fundamental human needs for connection, empathy, and care in product and service delivery remain paramount. Over-reliance on AI-only solutions (e.g., customer success) can lead to negative outcomes, underscoring the continued necessity of human-centric roles, often augmented by AI.
*   **The "AI-Native" Talent Edge:** Some forward-thinking companies are recognizing the need for "AI-native blood" and are strategically down-leveling seniority requirements for junior candidates who can demonstrate proficiency in AI tools and automation, creating a new pathway for early career professionals.

---

The Inside Scoop on Juniors and Jobs in the AI Age - YouTube
https://www.youtube.com/watch?v=AOl5bNDf1wE

One of the biggest questions in the labor market right now is what happens to juniors? What happens to people early in their careers? Everybody's asking me this, but also we're seeing quantitative data that suggests that this is going to become a different AI story depending on where you are in your seniority when the AI curve hits your business.
For folks who are senior in their careers who have experience in their domain plus some knowledge of AI, the ceiling is the limit and then you break through that. It is crazy not just the compensation jumps that people are seeing but also the number of additional jobs that are opening up for people who have those dual skill sets.
For everybody else, there are differing stories that are less good. For seniors who have deep domain experience but who do not have deep AI experience, many of them are choosing to walk away rather than learn AI. And that's not just me saying that as a general statement. I actually know multiple engineers, multiple PMs who do not want to deal with the AI transition at this stage in their career.
And they're choosing what is effectively early retirement, right? Woodworking, opening a bookshop, going to open a small coffee shop, whatever it is. They don't want to do tech anymore. And that opens up job openings, but the job openings are going to people who have domain expertise plus AI. It's the new gold standard.
What happens then to people who are juniors? What happens to people who are early in their careers, who are just getting started, who have graduated from college and who need to find a role? Well, first they would be the first to tell you and they tell me it sucks out there, right? like it's terrible and you can't get interviews and if you do get interviews it's AI conducting the interviews and there's it's an absolutely soulless process and it's very very difficult to find any role at all.
So like the anecdotes are just piling up that it's a really rough market for juniors. On top of that, from a hiring perspective, it's really, really hard now to tell when you get a great junior employee who has good ramp and you can scale them into a senior rapidly versus when you don't. And that was always the bet, right? If I I've been in the room hiring folks who are interns, hiring folks who are juniors for a long time now, the bet is always, is this person going to be someone with high ramp? Can they grow relatively quickly into a senior person? We had
better signal on that 3, four, 5 years ago than we do now. This is a case where AI has obfiscated signal. AI has hidden signal. It has camouflage signal. It is harder now than it has ever been to properly assess talent. And I think that that difficulty is at the root of the job market issue.
Now, you may say, well, it's AI, right? like people are just not opening these roles because of AI. I would actually come back and say AI is part of the problem, but it's part of the problem in more interesting way than that. AI has contributed to an arms race condition where everyone is submitting resumes that are perfect.
The recruiters are having to use AI to read them. In many cases, they're having to use AI to do initial rounds of interview. And none of it is leading to the kind of gut level conviction that enables someone who has been in the field for a long time to say this junior engineer has real potential. I want him or her or this PM is absolutely going to run through walls for us.
We need that person. Or this marketer is someone who's going to do a fantastic job with this small channel and I can see growth for them in the future. That kind of conviction has never gone out of style. people know even companies that are deep on AI know that they need to be bringing people up who are that hungry, who are that dedicated, who are that deep on the domain, but they can't get the signal anymore.
Now, there are a few cases where companies are choosing not to hire those employees as well because they think the seniors at the moment can do it for them and use AI agents. And that is a very deliberate strategy. It's a real strategy and it is a short-term strategy. At some point, those seniors are going to age out.
Those seniors are going to go on to other things. Maybe they'll get promoted, they'll go to a different role. The average tech tenure is still quite short. Even though people are being told to stay in their roles, it's still two two and a half years. What happens in two years when that role is vacant? And now you need someone who knows the business and who knows how to manage AI agents and who knows your domain.
A lot of companies are about to find that out. I know that we talk about the idea that there are a uh Cambrian explosion of AI startups out there right now and they're not all going to survive and some of them are going to go to the wall in the next 12 to 18 months. There's a Cambrian explosion of role experimentation going on as well.
Not all of those roles are going to survive when people start to figure out what actually works. And one of the things that has always been true that I'm continuing to bet on is that companies are going to need extremely energetic junior talent that they can start to ramp quickly into senior ranks. It's a continual need because people transition roles too often for it not to be a continual need.
In other words, when people come back to me, and I have people come back to me and they say, "Nate, this video is just bunk." and the video is bunk because by the time two years rolls around AI will be so good they won't need seniors anymore. I and some people are betting on that, right? Like I'm not going to say that opinion isn't out there.
I will come back and I will say people build products for people. People take care of people. There's a reason why CLA rolled back their super aggressive AI only CLA customer success. It wasn't because it was 2024 and 2025 and the models weren't perfect yet. It was because people didn't feel cared for by people. That is still going to be true in two years.
And so, yeah, I do think we're still going to need people who know how to deliver value partnering with AI agents with deep domain expertise, which means we need junior talent now in order to get there. And so, this this is for you. If you are hiring, if you are considering a hiring strategy and you think to yourself, we can get away without a junior.
I want to ask yourself if you can get away without a junior if the senior person in that role leaves. Can you? You should be honest. We need to think more about smart redundancy in our hiring. We should not be assuming that we can only get along with exactly who we have in the role. That's just not good long-term planning.
If you are in the job market and you are looking for a role, this should be somewhat encouraging to you. I can't make a particular company decide to be intelligent and recognize that a sharp AI native hungry junior is an excellent hire. But what I can do is say that if you are in the market and you are looking to distinguish yourself, you're looking to stand out from all of the applications and all of the noise, the best thing you can do is find any dimension to become one of one on.
And that's really what people mean when they talk about building your profile online and putting projects up and this and that. They don't give you specific advice because everyone will follow that and it will become generic advice. The real value is in figuring out what is the thing you want to be known for and how do you put the word out that you want to be known for that.
I put out a video earlier on power curves on the idea that you want to find an exponential power curve and ride that from a career perspective. In the same way, you want to look at a focus area that you can stand out in from an application perspective and obsess over your online profile about that. And so I know someone who obsessed over this intersection between sales enablement and being an entrepreneur and sort of being able to do both and being able to show that they were very autonomous and entrepreneurial but also had deep
experience with sales enablement. And that particular ven diagram that was them, right? And they wanted to do one of one and they put up spear fishing videos with just that. Spear fishing sounds really spammy but like they they did custom videos with just that and they did very well out of their job search because they had the focus.
They had the focus. I know other folks who have figured out that their focus is on, you know, AI automations in a particular subdomain and they're going to tell you all about how they trade off NADN and Zapier and Make and a half a doz dozen other platforms to actually deliver value.
Whatever your specialty is, get one. Get one. Pick one and then double down on it and make sure that you are the one. You are one of one for that. you are as far along the power curve as you can get so that people know that if they are looking for someone who does this particular niche, your name is going to pop up. And the reason I say that is because if you just try and compete on the variables everyone is maxing like how many LinkedIn friends you can get and how many webinars you can attend and how perfect your application is and how amazing your
cover letter is and how phenomenal your ability to cold email is. everyone else is maxing those too. And so you have to find ways in a game that has reached a equilibrium that is not in your favor to break the game. You have to find ways to start to push through and say, "I am really, really good at this.
I'm going to make sure that you know me and hear my name if you see it." And if you're going to come back and say, "Wow, that's really unfair. It shouldn't have to be that way." I'll say, "Yeah, it is. It's not fair. Life isn't fair." But it is actionable. It is something you can do, and it's something that will help you stand out.
It's actionable advice that has traction has teeth to it. I get that it sucks. Being junior is really tough right now. The world still needs you. The tech world still needs you. Not all of them have realized it yet. And so part of your job if you are working in a company and you're seeing this video is to share it with someone on your hiring team so that I can talk to them and say guys I know that you're thinking about the next 6 months and the next 12 months and the fiscal year budget and what you can deliver on. I get it. But you have
to think about short tenures for senior people and the possibility that you are going to need real influx of very sharp junior talent. And you got to plan for it. You got to take it seriously and you've got to find a way to make that an edge in your culture. I know some people are making that an edge in their culture from a hiring perspective by saying we need an injection of AI native blood.
And so we are going to open the gate specifically to junior talent on any role as long as they are AI native. And so they actually downlevel some of the seniority requirements for some of their traditional roles and say there's an exception. You have to have so many years of experience unless you're AI native and can prove it, right? unless you can and they'll have different rules like something with AI automation, something with tooling and tool use, etc.
But people are doing that because they need a way to show what the culture edge is by bringing in someone who's younger. And so I I hope you see this if you're hiring and you take it seriously. I don't want you to be in a position in two or three years where you regret the hiring choices you made today and just hiring seniors.
If you're junior, I hope this gives you some hope. You can play the power law game. You can stand out. You can get noticed and you can get hired. It is possible. It's happening. And yes, the game is harder than it used to be.

---

<!-- VIDEO_ID: ax8Oh5FCLh8 -->

Date: 2025-09-08

## Core Thesis
The optimal approach to AI adoption is not universal but deeply contextual, dictated by an organization's specific constraints, customer demands, and existing infrastructure. The perceived "speed gap" between startups and enterprises is a symptom of these divergent realities, where each side has unique advantages and challenges that AI both amplifies and, counter-intuitively, can help mitigate, leading to a bifurcated future of AI-driven development.

## Key Concepts
*   **Different Games, Different Rules:** Startups and enterprises operate under fundamentally different constraint sets, leading to divergent "correct answers" for AI adoption strategies.
*   **Movable Constraint Set:** Organizations should actively align their operational pace and AI strategy with customer expectations, rather than passively accepting their size as a fixed determinant.
*   **AI as Conversational Software Building:** AI transforms software development into an interactive, descriptive process, enabling "vibe coding" and potentially non-engineers to create features, though this introduces new forms of "cognitive debt" in large systems.
*   **Technical Debt's Shifting Cost:** AI agents are rapidly making technical debt increasingly optional for startups (cost going negative) and significantly cheaper to address for enterprises, challenging traditional views on code quality and refactoring.
*   **Success Starts with Acute Pain:** Genuine AI adoption is driven by immediate, felt pain or an existential threat, which leaders in larger organizations must actively cultivate to overcome inertia.
*   **Workflow Trumps Tools:** The effectiveness of AI lies not in the specific tools used, but in their integration into well-designed workflows; human coordination and workflow design are paramount, especially in complex enterprise environments where leverage is highest.
*   **Experience-Induced Resistance:** Prior experience or perceived threats can lead to resistance to AI adoption, particularly among senior staff, necessitating strategic leadership, incentives, and tailored training to foster optimism and minimize friction.
*   **Velocity vs. Direction:** While AI grants startups unprecedented velocity, enterprises can still win by focusing on disciplined direction, building value flywheels, and leveraging their reliability and distribution, suggesting a bifurcated future for AI-driven development.
*   **Discounting Hype:** Predictions from AI model makers should be pragmatically discounted by a year or two, as their immersion in advanced models can skew their perspective on broader market adoption realities.

---

You know, AI native startups and AI enterprises have been moving at different speeds, getting different things done, using different tool stacks. And for the most part, when I look at the discourse online, when I talk to founders in the small startup category, when I talk to CEOs of large companies, what I hear is two entirely different worlds and frankly a lot of disappointment in what's framed as the other side.
I would like to instead take the time to look at six different core learnings that have emerged from studying both startups and enterprises as they grapple with the transition to AI at the same time but under very different conditions. So this should apply to you if you are working in a large company environment but equally if you are working in a small company or even if you're just a solo founder.
If you've ever wondered what is the difference between these two environments besides size and how is that impacting AI besides the really obvious one of the big companies often go slower and they get mocked for it and they shouldn't. Well, I'm here to disentangle all of that, give you the learnings and then have some reflections on where this is going in the rest of 2025.
So, number one, let's get into those core learnings. Different constraints will create different correct answers. Startups and enterprises just aren't playing the same game. They're playing different sports with different rules effectively. When a startup founder ships a broken feature to 10 customers, well, you can personally call everyone if that breaks.
And so vibe coding is very viable. When a PM ships to 10,000 healthcare comp customers, one data leak could trigger lawsuits, lose the biggest contracts that the business has, and eventually destroy the company. The standards are very different. The founder, solo founder, can rebuild the entire codebase over a weekend.
The PM must maintain systems with database columns that stem back to migrations in, oh, I don't know, 2008. The startup founder who burns thousands or even tens of thousands of dollars a month in AI credits isn't actually being reckless. Even if a lot of those credits get used and reused across the same part of the codebase, effectively he or she is buying the equivalent of three or four developers working 24/7 trying to solve the really difficult problem of product market fit.
The enterprise on the other hand is going to take several months to approve GitHub co-pilot, but they have socks compliance. They have enterprise customers doing security audits. They have a board that demands very predictable quarterly results. They don't have the same game or the same rules.
And my reflection for you on this is that you play the game that your customer wants you to play. And this is actually more profound than you would think because many times startup founders end up having to act more big company because they are in the B2B space and they're trying to serve larger and larger customers. And so they end up getting themselves into this world where they have to do compliance audits and enterprise and this and that.
And that doesn't mean they lose their sort of fast young scrappy DNA, but they do have to start to shade more toward the enterprise space faster. On the other hand, if you are serving hungry startups and that is your primary customer base, well, yeah, you want to be as hungry as you can. If you are serving AI savvy builders, AI savvy consumers, you want to be as quick as you can and you want to actually make sure that you are iterating faster than anybody else.
And finally, if you're serving consumers as a whole, which is a different demographic, you want to be in a position where you can make the change translatable and manageable to them. I saw a great ad from Google that really illustrates this. I think the ad without generating any liability for Google took a poke at Apple and basically said without using the word Apple that another big phone company had made a lot of promises about AI and hadn't kept them.
Which everyone knows Apple made a big deal about rolling out AI and couldn't keep it and then suggested maybe the step forward is to actually work with a phone that can keep its promises. Right? You need to make these big AI changes feel natural and feel easy for consumers and that leads to an interesting hybrid.
So the first sort of demographic change is like if you're serving business customers, you're going to be pushed into a more business quarterly cadence kind of workflow just naturally and you have to fight that to go faster. On the other hand, if you're serving super AI conscious consumers or builders or solo founders or tiny startups, you're going to be pushed to go faster yourself.
And then in the third category, if you're serving consumers, you are going to be pushed to make sure that you can translate the change and make it easy to understand. And that tends to lead you to a slightly more unpredictable cadence. You basically have to wait till you have the thing that you know the consumer is going to want and then double down there. Those are three different worlds.
Essentially, what I'm suggesting to you is that instead of assuming your given size dictates the game and just letting it be, think about it as your customer would like you to work at an ideal pace, what is that pace? And maybe see if you can shift the constraint set in that direction.
Number two, AI changes what building software is. We talk about it a lot, but I want to simplify it a little bit. AI changes building software to a conversation. I think Dan Shipper at every is a good example. He can have a conversation with Claude code that eventually becomes a feature and not just him other people who are non-engineers can have that too in his company.
If you can get to a point where you can describe what you want increasingly if it is a fairly buildable smallcale piece of software the AI can do it for you. Maybe it's not cla code maybe it's codeex but it can still do it for you. Maybe it's lovable.dev. dev, right? And vibe coding sounds too good to be true until you see it work.
I have had moments and I think this is actually really important to do. So this listen up if you work in a bigger company. I have had moments where I've talked with directors and above at larger companies and they just don't have time. They all have meetings. They don't have time for practicing vibe coding, for seeing how lovable works, etc. Pull up lovable.
dev and show them how easy it is to build something. I have seen jaws drop in a sense. Part of the gap in AI is just knowing what you can do and having the time to try it. And so part of why there is a there there's a speed gap that people assume is true between startups and enterprises is that the startups have nothing to lose by dramatically shifting the way they build software.
And the enterprise has a lot to lose. the enterprise has engineering debt and it's non-trivial. I don't want to set this up as like an obvious choice to go with a startup. As an example, if you are shift shipping AI code changes, shipping AI code lines over time, you run the risk of your codebase at scale, at enterprise scale becoming less understandable to you.
That in turn generates a tremendous amount of cognitive debt, particularly for senior engineers. And so it's not as simple as saying, well, you have to get with the program and just ship more AI code and go faster. You actually have to think about how organizational size dictates different approaches to AI.
I would argue in this case AI is a conversational process regardless, but it's easier to start the conversation for startups and for larger companies, it's more difficult to start that conversation. You may have elements that feel conversation that only happen after after you've done all of the requirements and compliance pieces.
And that's what I see anecdotally at larger companies. They do all of those initial steps first and then the creation of the software ends up being more sort of conversational wherever they can, right? If they're using claude, if they're using cursor, other other tools. Principle number three, technical debt is increasingly optional.
So I think there's a number of dynamics here. Another way I've talked about it is that the cost of technical data is going negative. Code quality at AI native startups ranges widely. If you have a very strong engineering founder on the team, it's often higher, but it doesn't have to be all that high to ship these days.
There are startups where no one knows how to code and they're doing over a million dollars in ARR. And I I want to suggest that what that implies is that you could always buy your way out of technical debt with time and with good engineers. And increasingly time is not the factor. Scale is not the factor.
You just need to have enough scale to be able to afford a production engineer to refactor something if you need it. But that is often so far down the road that you kind of don't care for a while. If you can hit a million dollars off of Vibe Coding, which literally the first startup that has hit a million dollars off of Lovable already exists, you're not you don't really worry about technical debt.
You don't worry about whether you fully understand the codebase. You just worry about whether you're shipping for the customer. Now, if you have to pass compliance as an enterprise company, you do worry about the codebase. It's not optional to care. Technical debt can become a legal liability. And so I think that this is one of those areas where we are living in a movable constraint set and we need to understand the next six or eight months.
Code is one of the spaces in AI where tech is invested so heavily in leveling up the ability of AI to agentically fix code that we are going to get massive breakthroughs in the next few months. That suggests to me that the problem of refactoring even in large code bases is going to get progressively easier. I don't think that's the same thing as saying that AI will be able to do senior engineering work because senior engineering work is a lot more than refactoring code bases.
But it is something that we should keep in mind because what it suggests is that the cost of technical debt has fallen. Yes. Is probably negative for startups. Yes. but is also falling for larger companies very very quickly. This is the source of some of those big headlines you see where Amazon or others say publicly, we saved so many thousand man years by using AI to transition our code from this language to that language.
And they're doing it already, right? They're going to do it even more as the ability to handle context windows gets better, as these AI agents become more proactive and able to work around problems, etc. This is a lot of the reason why chat GPT5 is agentic because it's designed to solve problems like this.
So what's interesting is technical debt is becoming optional. It's becoming something you don't have to care about at a startup level. But I think the thing that I take away from that is that in a lot of ways startups were always advised not to care about tech debt in the first place. And so what's changed is that they're still told not to care about it, but now they don't really have to pay to clean up the mess either.
like they don't have this huge bill that comes due at series A or series B to clean up all of their architecture in the same way because it's so much cheaper and faster to address it. Enterprises still have the compliance burden, but the cost of addressing it is rapidly improving as well. And so this is one to watch. This is one where I think we'd have a different conversation in 6 months.
Number four, success starts with pain. Every AI adoption story that that you see starts with a team or a founder drowning in work that embraces AI as a band-aid. That's a very consistent pattern. It extends to the consumer, too. People don't embrace stuff unless it immediately solves a real problem. And so, when you think about or want to criticize how enterprises are adopting AI, one of the things that you need to think about is how real is the pain at the level of the team in the enterprise.
Does the team feel the pain of not adopting AI? Because startups feel that pain right on the revenue line. If they're not moving fast enough, if they're not shipping fast enough, somebody doesn't get paid that month. Whereas for larger companies, where's the consequence to half-heartedly adopting chat GPT and only using it to write your emails? It's not a lot in most in most enterprises.
So, success has to start with pain. And one of the things that I think that we've done a poor job of in midsize and above companies is really making it clear that the pain may not it may not be acute but it's real and is going to hit the company in a existential way in the next few years if they don't address it.
I do not believe in a world where an enterprise can whistle by the graveyard and skip AI and get away with it. Almost every company is going to have to confront this and the longer they wait to confront it directly, the worse off it will be for them. And so this is a case where I think startups should be kinder to enterprises in mid-market because they should recognize that getting an entire company to feel pain is a very difficult art.
Takes real leadership. Steve Jobs had it. Not everybody has that leadership. If you are a leader in a midsize or larger company, part of your job is to get your team to feel pain until they start to adopt AI. Number five, the workflow matters more than the tools. I have sat there in conference rooms and talked to larger companies and they just sort of make long faces and they're sad.
They say, "Well, we don't we can't get chat GPT. We can't get Claude code. All all we can do is we can use Copilot." The workflow matters more than the tools. The workflow matters more than the tools. I have written an entire guide for how to use Copilot. It exists. It's out there. You can get it. It's right on Substack.
But it's not about my guide. It's about the idea that you can integrate any AI tool into a good workflow. And if you actually use it, it will be the smartest tool out there that people don't integrate into their workflows. Part of how startups have an advantage here is because one brain can hold the whole workflow. one brain.
One brain can hold the whole workflow and in a larger company that's not true. You have to have a lot of people working together to shift a workflow and coordination problems take time to solve. This is actually one of the reasons why I think that the excited guesses that AI would take a bunch of jobs in the enterprise.
If you're trying to design workflows across multiple teams, the sheer fragmented knowledge is actually really, really hard to overcome. It's actually not something AI does a good job of. It's something humans do a good job of. Humans need to come together, do AI sprints, do something that helps you to figure out as a larger team how to actually build an AI first workflow that doesn't just stop at the level of the individual or stop at the level of the small team.
And one of the things that I noticed about this is that startups assume that again their speed is their advantage which I suppose is true and the other guys are bad at what they're doing and that is not true. Workflow has 100 to a thousand times more leverage at a larger company. I could run workflows when I was at Amazon that would shift the way thousands and hundreds of thousands of titles worked.
Well, you can't do that at a startup because you don't have the product for it. Workflow matters more than tools and workflows are higher stakes and higher leverage at bigger companies. And it is more important to get them right and it takes more people to get them right. And so you have to work together to build AI workflows at big companies in a way you don't at startups.
Number six, experience tends to create resistance. This is counterintuitive. We like to think if only they could see AI everything would be good. My observation is that is not always true. People will try AI. They'll have one or two disappointing experiences. If they have a prior suspicion of AI, they will just use that suspicion and those one or two experiences as a reason to say no, I'm done.
Especially if they're worried about their role or they're worried about having to change what they know. I know developers who are walking out of tech rather than deal with the AI change. they're like I want to go do something else right like they can take up a hobby something else this is an area where if you are getting started you can have an advantage in your career by being AI native from the beginning because there are enough senior people who are not ready for this kind of change and voting with their feet to say I'm done I don't have to work on this
right now I'm leaving well as a junior person you can step in you can be the one that shows that experience doesn't have to create resistance Again, startups advantage here comes from having fewer people. You can handpick your people. Your people all can be people for whom experience creates optimism and forward motion.
People for whom experience is a good thing. People who are AI native. At a larger company, you have to work with the people you have, many of whom have domain knowledge that is highly specialized that you can't move out. And so the problem then becomes how do we make sure that the resistance tendency is downleveled and minimized as much as possible across this large multi,000 person company.
That is a different kind of hiring reality. You're looking at incentives. You're looking at team leadership. You're looking at how you can make training matter to different teams with widely different needs. You're looking at honest conversations about what it means to have career growth at a larger company with AI.
And I I'm going to remind you again, the senior developers and the principal developers at large companies on the whole are higher quality engineers than most of the engineers at smaller startups that I've worked with. I've worked with both. I love working at there's a reason I go to startups. I I've loved startups because of how fast they've moved.
But the kinds of quality engineers that you see who are lifers at these larger companies absolutely extraordinary and it is very rare to see them in the startup world unless they are founders. And so as much as startups would like to say, well, we can just pick good people and we're just amazing. Like, you know what? I doubt that most of your engineers operate at the level of the senior principal engineers at Meta or at Apple or at Amazon or at Microsoft.
And so, in a sense, you have to pay the piper. If people that good want to take their time understanding AI, you have to make sure that you bring them along. And that's another difference. So, those are six principles we've gone through, right? Experience creates resistance. Workflow matters more than tools.
Success starts with pain. Technical debt is becoming more and more optional. That one's really fluid. AI changes what building software is all about. It makes it a conversation. And different constraints create different correct answers. Let's step back and let's look at the future. What is what is actually happening here? Number one, the velocity gap is real and growing.
I think that that's something that I emphasize a lot when I talk to leaders at midsize and above businesses because disruption risk is in some ways a function of velocity and AI enables such a velocity differential for AI native startups that traditional enterprises should be more worried. AI native startups can be orders of magnitude faster than the than the regular startups the 2010s.
If you can ship a feature in an hour that would have taken two weeks in a team of engineers in 2018. Well, you are definitely going faster than startups went a decade ago and that poses more disruption risk to enterprise over the long term. These are fundamentally different realities. They're fundamentally different physics for startups.
But velocity without direction is speed. And so the risk of startups shipping 20 versions a week is that they don't have the discipline to learn and they are just accumulating chaos. And that is the one advantage enterprises bring. They put so much intention into a ship. If they do one solid version in 6 months, but it's actually builds on itself and it builds a flywheel of value for customers, they might still win.
So the question really isn't who is right. The question is what happens in a world where startups are 10x faster than they were and AI is desperately trying to crack through in the enterprise. Will startup velocity actually disrupt enterprise reliability? Will enterprise reliability and enterprise customer distribution win? This is the question we're facing.
And this is the question that like AI adoption is actually driving it in both of these environments. It really matters. Dario Amade predicted that AI would write 90% of code in 2025. Technically, that is true at some startups and it is becoming more true at enterprises and those mean two different things. At startups, I buy it.
If AI writes 90 95% of your code at startups, you were going fast. The way we've talked about, if AI writes 90% of your code or 60% or 70% of your code at a large company, I think the CEO of Coinbase said that something like 60% of the code at his company was written by by AI, I immediately ask questions because it's a different kind of problem.
Are you incentivizing your engineers by lines of code at that scale? Because you don't have that problem at a startup. You just want to ship the feature. But at scale where people are incentivized by goals and metrics, if you say it's lines of code, will people just be incentivized to write big bloated features to hit the lines of code goal? Anecdotally, that is absolutely happening.
And so my question at a large scale is should you be even thinking in percentage terms? And then two, is Daario simplifying the world too much? And maybe what we're headed for is a bifurcated world. where you have AI assisted code composition and structuring at larger companies and I don't care or know what the percentage is.
It's a meaningful percentage of code that you could say is written by AI but humans have a huge role to play in how they structure because the complexity of the system and that it's nearly 100% of code written at small startups that feels like a more realistic world. I will also call out this is continuing a trend where large model makers are making predictions that are directionally true.
But when we come up to the actual deadline and we look at it, we're like h it's a little early. I think that one of the things I'm learning to do with Sam and with Dario and with other major model maker CEOs is I discount their predictions by a year or two. I I take whatever they say I'm like give it another year or two beyond what they say because they live in this world where they have better models than are out in the public.
They are immersed in AI all the time. They may not have the perspective to see this wider world. So where does this leave us? We need to be building companies that reflect the constraints of the game we're actually in that focus on what customers need in a world where frankly customer appetites, customer demand is shifting really rapidly.
And we need to recognize that the stakes are existential. If we as a startup, we as midsize, we as enterprise don't figure out how to meaningfully build AI into our workflows, the disruption risk is real and fast. It's like in Jurassic Park where the raptors kept getting smarter. Other businesses are like those raptors. They're keep getting smarter and keep getting more AI enabled and they're going to catch you unless you're able to actually put AI into place.
And so my suggestion to you is that you should if you are a large company, look at those small company learnings. Look at the small company tool stack. See what you can learn. See what you can learn about speed. If you are a startup, if you're in the small company category, see what you can learn from the reliability, the stability, the system architecture of the big company.
At least acquire some sympathy for where they're at because that may be you. particularly if you serve businesses. Both parties here have a lot to learn from one another and I sometimes feel like I'm the only one talking to both and they mostly want to throw rocks at each other. So everybody's in this together.
We're all learning AI together. Let's see what we can

---

<!-- VIDEO_ID: cVZCfpkHNBg -->

Date: 2025-09-16

## Core Thesis
AI coding assistants act as powerful amplifiers, accelerating *whatever* engineering practices are in place. Their true impactpositive or net-negativeis fundamentally determined by the pre-existing strength, discipline, and clarity of an organization's engineering infrastructure and objectives, rather than the inherent capabilities of the AI tool itself.

## Key Concepts
*   **AI as an Amplifier, Not a Fixer:** AI coding assistants accelerate existing development practices. Weak infrastructure leads to amplified weaknesses and net-negative outcomes, challenging the common perception of AI as a universal productivity booster.
*   **The "Engineering Infrastructure Layer" as the Primary Lever:** The critical decision point for AI adoption is not tool selection (e.g., Codex vs. Claude) but the foundational strength of engineering practices, documentation, review culture, and workflow alignment. Tool choice is secondary.
*   **"LLM Croft" (Counter-Intuitive Finding):** A novel concept describing the insidious accumulation of unintentional architectural decisions made by LLMs over time. This leads to increased codebase complexity, higher cognitive load for human reviewers, and a drain on strategic leadership time, ultimately making the codebase harder to understand and maintain.
*   **AI Fragility & Discipline Requirement:** Despite their power, AI tools are "surprisingly fragile" and demand high discipline, consistent practices, and rigorous human oversight (e.g., architectural review of AI-generated code) to deliver sustained value.
*   **Non-Linear Complexity Scaling:** The challenges of AI integration (e.g., maintaining consistent practices, measuring success, managing total cost) scale non-linearly with team size, making it significantly harder for larger organizations.
*   **Vanity Metrics vs. Value Metrics:** Emphasizes moving beyond superficial metrics like "lines of code" or "number of commits" to measure true business value, cost efficiencies, and leading-edge indicators of code quality and team performance.
*   **The "Doubling Effect" Framework:** A structured approach for both pre-implementation assessment (7 questions) and post-implementation troubleshooting (7 parallel questions), highlighting that foundational infrastructure concerns are continuous and require ongoing audit and adjustment.
*   **AI as a "Vulnerability Manufacturer":** A counter-intuitive warning that while AI can catch vulnerabilities, it can also accelerate the creation of new ones if not managed with a higher bar for QA and security.
*   **Root Cause Analysis over Blaming AI:** Disciplined teams identify specific problems in their infrastructure or implementation, rather than generically blaming the AI tool when failures persist.

---
I have a very simple thesis which may not be popular but is nonetheless true. Coding assistants accelerate your development practices whether they are good or bad. In other words, you are tying a giant rocket engine to whatever engineering infrastructure practices you have and you're saying go just go faster go do more.
You know what? If you have any kind of weakness in your engineering infrastructure layer, your best practices layer, that choice to add clawed code, to add codeex, which just updated this week, that's going to end up being net negative. Yeah, I said it. It's going to end up being net negative. I don't want that for you because there are teams that are getting real gains.
There was a viral post recently on Reddit called this is how we vibe coded a fang. You know what it was about? It wasn't about a vibe coding tool set that would magically fix everything. It was about the engineering infrastructure decisions that matter. And I want to focus on that today because you know we could take this time and we could dive into why codeex is the best thing since sliced bread because at the top of the news this week and that's all anyone can talk about if they're in development is like do we use codeex? Do we use cloud
code? You are asking the wrong question. In most cases, the right question is at the engineering infrastructure layer. And you only get to the tool choice if you've asked the right engineering infrastructure questions. So I want to give you in this conversation the specific questions you should be asking yourself as a technical leader, as a technical team member, as a builder, as a coder, as a vibe coder.
Before you pick a tool, ask yourself these first because then when you use the tool, you'll be able to go actually faster and not slower. Question number one, what is the problem that we are solving specifically? Almost no one can answer this actually. Just try answering it. Is it speeding up boilerplate code? Is it onboarding juniors? Is it reducing bugs and repetitive tasks or something else? If you have a vague goal like we're going to boost the productivity of our engineering team, I'm sorry, you've been sitting in the seauite too long. Like I
need some specifics here. I need you to say specifically this is the expectation that we have for what this tool will do for our engineers and why. Or if I'm a builder individually, this is what it will do for me and why. Maybe it's as simple as, you know, I'm a builder and using Devon or using Claude code, I'm going to get time back.
I can be in a meeting and the thing can be building anyway. Okay, that's fair. That's a specific goal. You can talk about optimizing for that goal and what the tools and all of that, but if you don't have specific problems you're trying to solve, specific goals that you're setting, you are already off in the wrong direction.
And I find that the bigger the company, the harder this is to do. Larger companies with larger teams often have real trouble saying what is the specific problem that they're driving at, and it takes a lot of work to peel the onion and get there. But you need to question two. Do we have strong engineering practices already that are worth amplifying? Look at the prerex.
Do you have consistent code patterns across your codebase? Do you have date documents that are up to date? Do you have actual review culture and rigorous PR reviews? Do you have design docs that you're proud of and you can stand behind? If you don't, it is likely that whatever agent you pick, whatever tool you pick, AI is going to make whatever you're doing worse.
You need to take the time to try to get your house in order so that what you select has a foundation to build on. AI is surprisingly fragile in that regard. It's amazing at so many things, but it does need you to be discip. It needs you to have good engineering practices for it to ladder in as infrastructure in a supportive way.
And so many houses don't. And again, this becomes something that is big company challenging. If you're a small coder on your own, you can say, "Yeah, I keep all of my coding decisions in this markdown file and I and I have Cloud Code go and check it and we're done." Or, you know, I review all the poll requests myself. I know that I do a good job.
The bigger your team is, the more complex this is and the more you have to actually think about this. Complexity scales nonlinearly and that makes tool assessment much more complex once you get past even just a few developers into the team or multi- teamam scale. Number three, does the tool align with the workflow and the tech stack? This is complex but you have to ask yourself what is the team already using? Are they using cursor? Are they using VS Code? Whatever it is, what is the code host? Are we on GitHub? Are we in terminals in
some cases? And you have to think about what real workflow compatibility looks like. And I'm going to give you an extra challenge here. You need to think about workflow compatibility outside the engineering team, which circles back to my second question around engineering practices.
Assume you are living in a world especially if you are subenterprise level where people who are not traditional engineers will have code related ideas and potentially code related prototypes they want to push into the code stream in some fashion. Maybe not to production. Maybe an engineer has to review it. But there are companies who are above single founder level with teams where non-coders are submitting poll requests thanks to their use of a coding agent.
Do you have strong enough engineering practices to sustain in that world? Do you have tools that enable people who would not normally have production commit permissions to still be able to do some degree of coding work and pass it to an engineering architect? As far as I know, there is no true plugandplay in that world.
You have to look at your unique fingerprint and you have to decide what is the tool stack that is going to be compatible. I think one of the things I want to call out here that was notable to me as I was reviewing codeex and claude code is that codeex seems to implicitly presume a center of gravity around a larger team. So much of codeex is around can I automatically review the PRs that are getting submitted for my code right can codeex is already there it can go in it can look in GitHub it can review the PRs it can write up reviews etc it can even go and fix and
address issues whereas clawed code is more predicated around the idea that you were working in the terminal and you were building end to end and you may be fixing issues and you may be working on things besides code it's not that one is good and one is bad it's that their focus is different in the ecosystem And you have to think about where the leverage lies because it's absolutely true that if you wanted claude code to review your PRs, you can do it.
People have have done it all the time. Single builders similarly use codecs all the time. I know some that swear by it. And so it's not that one tool is perfect for any use case. It's that you have to think about what works for you. Not just from a model power perspective or from a congruence to prompt perspective or from a degree of comfort with the model or even from a token burn perspective.
You have to think about it from an ecosystem perspective. How does it fit? Number four, do you know how you're going to measure success? Do you know how you're going to track changes that happen in the codebase? What metrics matter to you? Do you have metrics that are sort of vanity metrics where you're like, "Oh yeah, we're going to have so many commits and that's going to be the way we do it." Or it's lines of code.
We're going to brag to the CTO about the number of lines of code that are AI written and the CTO is going to write this up into a summary and the CEO is going to tweet it out, which by the way, that totally happens. Is that really a metric or is that a vanity metric? Right? just just having lines of code is something that any engineer will tell you is a terrible metric for actual productivity.
So think about how you want to measure value. One of the horror stories, and I don't say this to scare you, but I say this to warn you, it is certainly possible to think you made these decisions well, but to not really factor in the ongoing impact of what I will call LLM croft over time. And so what I mean by that is the LLM is pretty good. The LLM understands your codebase.
You think your engineering infrastructure is up to the challenge, but you don't have ongoing rhythms that have the whole team checking and reviewing LLM coding so that everybody knows what's going on. Everybody is conforming to best practices. The LLM isn't drifting on its own. And what you end up finding is that over time you spend more and more and more and more of the engineering manager's time or the founders's time reviewing what the LLM submitted.
and they get less and less time for leadership for strategic thinking because at the end of the day the codebase is more and more and more difficult to understand because the LLM has made effectively unintentional architectural decisions that someone else has to disentangle. And so my my advice for you is more eyes are better than not.
Right? If you are in a position where you have multiple eyes and you're building with multiple people, put those eyes and have everybody's expectation be that AI code doesn't go to prod unless someone looks at it and can say, yes, this is architecturally correct. Yes, this actually works. That's not always the case.
There are lots of people who say, you know what, we don't do that. We believe it works. That's fine. And maybe in a few cases, you are so buttoned up and you are so clean and everything is so well documented and it's so perfect on your small team, you can get away with that. But I'm not here for those perfect 1enters.
I'm here for everybody else who lives in the reality of partial documentation and everybody doing their best and everybody trying to meet their deadlines and everybody trying to code according to the new best practices and sometimes forgetting. Okay, fine. You should be in a place where you can actually institute engineering practices that sustain the benefits of LLMs by having regular reviews of the codebase and regular reviews of LLM performance.
That's what I mean by can you measure success? Can you actually track changes over time? Number five is security and data privacy thought through carefully here. Do you feel comfortable with the terms of service your vendor is offering, the model maker is offering? Are you okay have you checked for IP leaks, vulnerabilities and generated code, appliance issues, liability generated by that code if it has a mistake in it? you will need a much higher bar on both QA and production code to successfully have agents in
play. So yes, they can write code much faster. There are security researchers who will tell me that's just a way of manufacturing vulnerabilities much faster, right? And yes, some of them will also catch vulnerabilities. And that is actually one of the things that OpenAI called out about Codeex is that it's good at catching vulnerabilities in code and that OpenAI themselves use Codeex as part of their QA process before going to production.
So I'm not here to tell you that Codeex and Cloud Code don't add value. These two companies are dog fooding their own product and they are finding ways to get value out of it. But I am here to point out that they're not silver bullets and that if we want to have a deep dive on codecs, we got to talk about some of these engineering infrastructure practices first.
Number six, do you actually have buyin? Again, bigger companies, nonlinear problem spaces, this is going to be harder. If you have junior engineers and senior engineers and principles, and maybe you have some some non-technical people like I talked about, how are you planning for education on prompting? How are you planning for reviewing your AI outputs? How are you planning for understanding what learning use looks like for juniors? So juniors understand how code actually works and how system components go together and they don't end up
overdeferring to AI. How do you budget for the resources, the money, also the time to actually learn this and not just get into the temptation of set it and forget it because these tools are temptingly easy to set and forget. You can just tell them to do things and maybe the cost doesn't come due today, right? Maybe the bill comes due in 6 months.
You have to be disciplined to do it today. Number seven, what is the total cost beyond pricing? So you have to look at setup, you have to look at maintenance, you have to look at context engineering costs, you have to look at fixes for bad outputs. It is worth it if you have a big team to do a pilot for this because you can actually see over two or three months for this individual two pizza team, for this small team, what did the value look like? And that is exactly the pattern we see in a lot of enterprises is that they will roll this out for a small group, test it,
gather learnings, and then figure out how that larger pathway will go. Again, if you're a small team, it's super easy to turn around. It's a two-way door. You try codeex today, you say, "Oh, it feels better." You dump cloud code. You try cloud code tomorrow, when they release something new, you say, "Oh, it feels better." You dump codeex.
It is not as easy when you're on a bigger team. It doesn't work that way. Okay, we've talked about some of the foundational questions to ask when you are getting set up. I also am aware looking at the poll requests, looking at the Reddits, so many of you already use an AI coding assistant. And so the second part of this is really going to be asking what are the questions you need to address as a current user of a coding assistant to figure out if AI is actually helping you or hurting you and how you can troubleshoot that and make the most of
your current AI coding assistant implementation. And just like the first seven, we're going to go through seven. And you're going to start to see a mapping there. I'm I'm deliberately creating a doubling effect here so that you can see how this maps from pre-implementation into implementation. Number one, is the AI amplifying inconsistencies in the codebase.
This maps right back to the idea of having a consistent infer layer, doesn't it? You need to check and see if there are antiatterns in the suggestions that are persistent. You need to audit and say if you have outputs that go wrong, do they skew? Do they go wrong in a particular direction? Do you need to fine-tune your document standards in a particular way so that the anti patterns disappear? That's on you. You need to check that.
Number two, are you reviewing and testing AI output? I talked about that as something you need to be ready to do. But are you actually doing are you actually skipping the explanations? Are you skipping the edge case tests that it's recommending? Are you just saying explain yourself and you're saying, "Well, that's documentation and that's good enough.
" Do you feel like you can own the AI output from your AI coding assistant? That's really the standard. If you can stand behind it and say this code is mine, okay, fair enough, but not everybody does that. Number three, is prompting or context an issue for you as you start to drive coding assistance forward.
If you have vague prompts and you're getting vague code, does your team have clear specs? Does your team have design docs? Does your team have examples? You see how this goes right back to the info layer? You can actually diagnose this by testing small incremental changes against small incremental changes in your codebase, right? You can change the documentation, you can change the prompt, and you can see if the codebase gets better, and you can start to figure out where your test cases are that you need to fix for your infrastructure layer. So, this is
actually something where you can pinpoint a fix if you're deliberate. Number four, are errors due to tool limitations that you have? Is your tool infrastructure actually thought through? Do you have model weaknesses? One of the things that Codex emphasized is that they understand the nonlinearity of coding problems that some coding problems need very token efficient surgical changes and some coding problems need very agentic long- form changes and they produce some metrics to say they're better at it. You know, your
mileage may vary. You'll have to see if you agree with them. But the point is, does the tool match the setup? Do you have a setup that allows you to switch models if you need to? Do you have a setup that allows you to understand the size of codebase you actually have or particular niche domain or particular niche language that you actually have? An example of that is uh cloud code and cobalt.
Try it out sometime if you're a cobalt person. See see what you think. Number five, how is team usage? Are your teams getting better at engineering? Are your non-engineeries learning engineering practices? How often do you catch each other in changes made before production so that you actually didn't break something versus how often do you catch things after production and you wonder what happened? How often do you have common newbie mistakes and do they keep getting repeated? How often are people copy pasting without understanding? How often are people not
really giving thoughtful feedback to the tool? There's a team culture thing here that it's really up to leadership to reinforce. Number six, are you measuring what matters and can you track it and show that you're actually delivering value? I I am here to suggest that there are two key pieces to this.
One is tying what engineering is doing to real business use cases that matter, business projects that matter, revenue, cost efficiencies. I know engineers get nervous about that, but you have to have stakes in the game. The other is making sure that your leading edge indicators are solid. Do you understand what LLM latency looks like? If you have something that's in production, do you understand how that you are testing for edge cases and how those edge cases actually manifest in production from an LLM? Do you understand how to show that
your documentation is clean enough and to run evals on your code performance? So you can say, yeah, the the code prompt to code quality is very high. We have human evaluators that say that and we also have some automated evals that will actually say, you know, the number of PR comments is going down, the quality of PRs is going up, the number of bugs that we've seen in production is going down, like you have some concrete things you can point to.
And number seven, if failures persist, is the issue that your preparation was inadequate, which means going back to the beginning of this video and dump jumping right in, or do you have something fundamental in your stack? Maybe you need to think about how you implement the codebased context.
Maybe you have to have an agentic search approach where you're searching through the context. Maybe the rag is not the right approach for you. Whatever it is, think about a disciplined audit before you assume you blame AI. Undisiplined teams blame AI because it's cheaper and easier. Disciplined teams root cause specific problems and actually get value.
These are the things that I have to say over and over again when people want to rush in and say, "The Codeex news dropped, Nate. The Codeex news dropped. What do I do with Codeex?" Well, this is what I say. Have you had these infrastructure conversations first? Please have these infrastructure conversations.
They matter. They help you build what matters. These are the conversations that you have to put in place so that when AI amplifies all of the practices you actually do on a daily basis, not the ones you dream about. Well, now that that you have these practices in place, maybe it will amplify good stuff, not bad stuff.
Maybe it will actually help you go faster, not slower. Maybe it will help you deliver more quality code, not break things. You get the idea. The infrastructure matters. Take these questions seriously. Take them as the initial gate that you have to get through before it makes sense to have complicated conversations about which tool to choose.
We will do a deep dive on codecs another

---

<!-- VIDEO_ID: F-m4AIU8blY -->

Date: 2025-11-16

## Core Thesis
The AI landscape is undergoing a fundamental reset, shifting from a model-centric arms race to a strategic battle for distribution and user experience control, where the ability to integrate AI into default user surfaces and optimize workflows, rather than just raw model intelligence, will determine market leadership and competitive advantage.

## Key Concepts
*   **The Five Strategic Axes of AI Power:** A comprehensive framework for evaluating competitive advantage beyond raw model capability, encompassing frontier models, distribution/default status, capital/compute, enterprise trust, and control of the user experience (UX) layer.
*   **Shift from Model Arms Race to Distribution & UX Duopoly:** The core competitive battleground is moving from who has the "best" model to who controls the default user surfaces and provides the most seamless user experience, especially on mobile platforms.
*   **Data Access and UX Trump Raw Model Intelligence:** A counter-intuitive finding: a "dumber" model with superior access to relevant data and a well-designed user experience will outperform a technically superior model with poor integration or data access.
*   **Architecting for Model Volatility:** A pragmatic engineering principle advising against single-model dependency; instead, build systems designed for easy model swapping and multi-model backends to ensure resilience and adaptability.
*   **AI as Workflow Transformation, Not Just Software:** A mental model for strategic leadership, reframing AI initiatives as opportunities to re-platform and optimize core business workflows, demanding clear metrics and outcome-driven approaches.
*   **The Enduring Value of Workflow Ownership and Proprietary Data:** For builders, competitive edge lies in owning specific workflows on specific surfaces, leveraging hard-won process knowledge, and proprietary data, rather than relying on generic model capabilities.
*   **Engineer's Evolving Role: Orchestration and System Stability:** The focus for engineers shifts from prompting to mastering orchestration, building stable systems from unstable models, balancing cost/latency/quality, and ensuring robust security and data governance.
*   **Strategic Capital Allocation: Rent Intelligence, Own Data/Workflows/Customers:** A first-principles approach advocating against in-house model training (unless highly justified), emphasizing that true long-term value and ROI comes from owning the data, workflows, and customer relationships, while leveraging rented, commoditized AI intelligence.
*   **The Declining Half-Life of Tool Skills vs. Persistent Value of Judgment:** A cognitive science-aligned insight: proficiency in specific AI tools will rapidly depreciate, while the ability to exercise judgment, design workflows, and "think with AI" will become increasingly valuable and durable.

---

I believe we're headed into the most significant reset moment for AI since 2022 when chat GPT launched. Why is that? Because for the first time, we are about to see a new state-of-the-art model that has nothing to do with Open AI. That's Gemini 3, and it's going to change everything. I want to give you the strategic implications of that shift today.
And I want to lay out for you the implications that you are going to see as a consumer, as an AI enthusiast, as a builder, as an engineer, and as an executive. I want you to think about this as a holistic shift in the landscape because I believe it's going to be the first thing we're going to talk about is the axes that matter in the board game ahead because I think that these are not widely understood.
Number one, frontier capability is one of the five axes that matter. raw reasoning, how it does on benchmarks. We're actually pretty familiar with this one. I'm not going to take a lot of time on it. The key thing to remember is that for a long time now, OpenAI and Google and Enthropic have all been neck andneck around the top of the leaderboards and Chinese open- source models have been competing and just behind.
Number two is distribution and who gets to have default status. Who owns the default surface for billions of users? Google has that on Android with Gemini integrated throughout. It's one reason why, and many people don't know this, there are half a billion Gemini users. Apple has this, but doesn't have any OS that actually is intelligent.
And so, Chad GPT is functioning as Apple's default right now because that is the primary app that iPhone users are using. That's more vulnerable than you would think. Microsoft has a strangle hold via co-pilot on a lot of the Windows-driven office experience and anthropic is almost always in apps that you choose not defaults and that's going to come back.
The third axis is capital and compute posture. So, OpenAI has a between a 12 and 20 billion revenue trajectory, but it's burning 8 to9 billion a year and projected another $15 billion of spend through 2029 with profitability not expected until 2030. Google and Apple, you can effectively thinking of them as having infinite cash for our purposes.
From their core businesses, they're spinning off so much cash that AI is a line item. It is not an existential bet for them. I know that sounds crazy, but it is true for them both. anthropic. It's at five billion ARR in mid 2025 and it's scaling extremely rapidly. It will probably be valued at over $300 billion at its next raise.
The capital question is not can they raise, it is can they sustain frontier scale model burn and keep unit economics somewhat sane for enterprise. Axis number four, enterprise penetration and trust. So Anthropic has over 300,000 businesses now. 80% of their revenue is from enterprise and they have a very strong safety first brand that's helping them.
Open AAI has massive usage high AR overall but is also kind of the poster child for regulatory scrutiny for the AGI risk and doom narratives. It has some brand issues. Google is a trusted infrastructure vendor for cloud already and has a long history of killing products and moving slowly which is not helping it in this situation. Apple maxes out the consumer trust axis but has minimal existing enterprise AI footprint and frankly minimal existing consumer AI footprint.
The fifth axis around which everything moves is control of the UX layer. Whoever owns what you talk to wins a whole lot more than whoever owns the model. So Apple is trying to do this with Siri, but that's been a disaster. Amazon tried to come in and make a play for that with their inhome assistance. That's been a disaster.
Google is trying this with Android voice. Open AAI has chat GPT as a voice but the voice model has not necessarily kept up with the pace of 5.1 and the march of the models that are uh producing written text. Enthropic has web and API only and you can kind of compare it to having a strong brain but it doesn't have a lot of voice integration.
The reset moment is that all five axes are about to move at once instead of one at a time which is what we've been seeing. Let's look at where the players sit on the board before we contemplate how the ball is about to spin. Google and Gemini from Lagard to OEM intelligence. So, their position now is at the frontier. Gemini 2.
5 Pro is Google's top model. The company calls it the most powerful AI model today. You can make that claim, but whatever. It's it's in there. It has a lot of breadth. It has strong distribution, Android, Chrome, Workspace, etc. What changes with Gemini 3? If we assume, and it is an assumption cuz it's not out yet, but if we assume that Gemini 3 is a clearly accepted big step change state-of-the-art model, it is clearly better than everything out there today on all accepted benchmarks and a bunch of new ones. And if we assume it is
integrated by default into Android and iOS via Apple licensing because Apple just cut a really big deal with Google, now we're in a different game because Google now shifts from being third contender in a race to the AI Intel inside for the world's two largest mobile platforms at once. Now, there are risks here. There are constraints here.
If Apple wraps Jebana in its own UX and Apple wraps it in its privacy guarantees and Apple nerfs the model, Google risks being seen as just an engine, Google risks their brand and it may not happen very fast. Also, Google is historically slow at sort of productionalizing these research models.
And so, it may be that we get Gemini 3 and it is incredibly good, but the distribution is not great and OpenAI is able to steal a march and keep their distribution advantage with the consumer. So, where is Apple? Apple's opportunity is really to move from AI lagger to potential leaprog. Apple's in-house models trail behind everybody else on metrics, obviously, but they're finalizing a deal to license a really big Gemini model for Siri and use that to power an Apple intelligence revamp.
The cost is reported to be around a billion dollars a year. The plan would be to run a custom Gemini based model on Apple controlled cloud, keep the privacy narrative intact and use it to power a huge AI reboot for the company that would enable Apple to get Frontierra intelligence without eating the full capital expenditure of training Frontier models.
And they can afford the cash, right? So they retain the OS integration, they retain all of the identity and the payment rails. They retain all of the hardware margins and the your data stays on your device story. If Gemini keeps pace or wins on quality, and if Apple can pull that intelligence in at a steady pace and refresh the experience so it stays cutting edge, Apple could leapfrog open AI on consumer UX, which none of us saw coming. Now, the risk is pretty simple.
They're dependent on Google's road map. Any safety issues with Gemini become Apple's risk. Enterprise AI continues to be largely untouched by any of this. This is a consumer and ecosystem mode. It is not a cloud play. Meanwhile, if we go to OpenAI's side of the chessboard, they have very strong models at GPT5 is extremely strong on most benchmarks.
It is the default mental model for AI for hundreds of millions of people. They've raised 40 billionish in capital. I keep turning around. They raise more billions, so who knows where they're at now. Their projected 2025 revenue is somewhere between 12 and 20 billion, give or take. And they're burning cashively.
and they are trying to translate their cash into a cuttingedge frontier model position. So, OpenAI effectively bought Johnny IV's hardware startup to build a screenless AI device of some sort. That venture has reportedly hit technical and legal snags. A court has ordered them to pause marketing under the IO brand. Fundamental UX issues around how the device speaks and on device.
There's leaks coming out of that team basically saying it's very difficult. Netnet, they're not shipping hardware yet. So now you're in a situation where you bought this device to help you to secure your advantage through this cash burn period, but it's just a lot of capital expenditure. There's a high uncertainty on the form factor.
You haven't gotten to results yet. So to lad it up, Open AI is simultaneously a frontier model lab, a consumer app, and an infrastructure provider. They are in a go big or go die position. They need to either get to monopoly level pricing power which given the extreme proliferation of AI is unlikely or they have to go to extreme scale and multiple massive distribution partners maybe Microsoft maybe Apple OEMs whatever that's the only way they get to scale meanwhile anthropic quietly attacking the enterprise jugular they have scaled their revenue super fast
they're on track for call it 9ish billion by the end of this year 20 to 26 billion next year their valuation keeps exp- exploding and their base is 300,000 plus business customers with largest accounts into six figures. So product stack claude models are very strong. They're near state-of-the-art. They're efficient. They're safe.
The ecosystem is very strong thanks to the model context protocol adoption. Now thanks to Claude skills, uh claude code is very popular with developers. Almost all their revenue is enterprise unlike anyone else in this position. distribution is via platform enterprises already use AWS, Google cloud, direct API, SAS integrations.
They have a very strong alignment first narrative which helps with enterprise focused on safety and they have economics that look much more disciplined than open AIs. Enthropic is essentially saying let open AI and Google fight over consumer. We will own the budget lines at the Fortune 500. It might work. So here's what changes if Gemini 3 and Apple actually come together.
we will move from a model arms race to a distribution duopoly on mobile. So instead of seeing a massive arms race across the whole spectrum, we will suddenly be in a world where Google powers the iOS experience by default, Google powers the Android experience by default and Google wins just about no matter what. We will also move from a world where we ask who has the best model all the time because they're so tightly competitive to a world where we ask who has the best UX and who has the best data loops because increasingly as models continue to get more effective.
We're not going to be asking ourselves is the model smart enough to do it. We're going to be asking is the UX easy enough for me to use and is the data loop in place where I can get the data I need safely. A dumber model with better access to data is better today than any other model out there.
It's also true that if the UX is terrible, you don't get the distribution. And that is actually the primary issue right now with Gemini is that Gemini's UX is not on par with where Claude and where OpenAI are. Gemini continues to be treated a little bit like a research project from Google and that is the historic risk of Google product thinking.
I don't want to lose the narrative here either. If Gemini 3 is the clear state-of-the-art, Apple can credibly say, "We pick the best model." That makes it not a defensive choice anymore. Google will gain leverage versus AWS and Microsoft when they sell cloud AI because they can point to consumer dominance and they can point to state-of-the-art benchmarks and say they have the best.
Open AI will lose some of their halo as the default synonym for AI unless they can deliver a model that beats. And this delivers a reset moment where the crown of best model and the crown of default assistant at once moves from OpenAI Microsoft to Google/Apple, at least in consumer. Now, if you layer in OpenAI's current trajectory and you look at their cash burn, this suddenly begins to matter strategically because if you're not obviously winning on distribution, spending tens of billions of dollars to stay at the frontier is going to be less defensible. Open AAI
has a strategic imperative to continue to win at distribution and there is a real chance with the Gemini 3 moment that they will lose that edge. So what does this look like over the next couple years? If we fast forward, let's say Gemini 3 comes out, it's what we say Apple is able to move quickly.
These are assumptions. They might not come true, but if they do and they're reasonable, then we have scenario A. Gemini is just everywhere. It's Google's winning all the way. Gemini 3 is the clear state-of-the-art. And then whatever comes after it, Apple's able to ship with the real brains of Gemini inside it.
Anthropic eats enterprise share and OpenAI remains a strong player on web and app but loses default AI narrative and probably loses ground on enterprise and probably has some issues with fundraising down the road. Scenario B is a device reset. If OpenAI is able to ship a compelling AI native device, they could win the personal AI hardware subscription battle, harvest a ton of cash flow, and reset the bar for who is able to access AI as default and who is two hops away.
Because if you can ship an AI native device, and it becomes the place where all of your voice is captured, now you're in a position to control the market. Scenario C is enterprise carve up and consumer chaos. The consumer space may continue to remain noisy with iOS having multiple players, Android having Gemini, multiple assistants, competing apps, etc.
Enterprise buyers may consolidate on just anthropic and maybe OpenAI and maybe Google and just pick between them, which is a little bit like what I see today, and that might continue. If that happens, the winner is probably Anthropic because they thrive in a multimodel scenario. And the losers would be single model SAS vendors who have thin moes because this kind of carve up requires intense competition and thin moat SAS vendors are vulnerable.
So what are the strategic implications here? Number one, stop treating your best model as your core bet. Assume that you need to swap models. I've said this before. I'm serious. Number two, optimize for surfaces. Don't just optimize for model IQ. Ask where does my user's intent originate? and then ask how can I build an opinionated workflow against that surface where they are against the voice against the Slack against the email not a generic chatbot if Apple and Gemini become the default assistant you'll want to design flows where you have that hot
handoff from Siri or from Gemini into your app for specialized tasks number three start to treat Anthropic as the enterprise benchmark like take it seriously the way they invest in safety the way they invest in governance is something that I think sets expectations for a lot of production workloads. Number four, keep an eye on OpenAI's burn rate and keep an eye on the regulation and safety narrative at OpenAI. There are risks there.
What can you expect to see changing depending on your role? If you're an individual, and I'll move up to executive from there. If you're an individual, you should expect your day-to-day tools will become more opinionated and more embedded. The idea of the best model is going to matter less to you than how you can orchestrate your tools around your work.
And the half-life of specific tool skills is going to keep dropping. The half-life of judgment and the ability to design workflows is going to be very persistent. So optimize for how to think with AI, not a particular model. You want to treat your assistants like interchangeable contractors. And you want to become the person who can translate what leadership wants into what this stack of tools can do.
If you're in the builder space, if you're a founder or a PM or a producty person, you cannot bet on a single model vendor or worthy assistant app as a strategy. Instead, you need to architect for model volatility. You need to pick a surface and obsess over owning it. Maybe it's spreadsheets, maybe it's email, maybe it's the terminal, maybe it's the calendar. Just own that.
And then you need to differentiate on your workflow and on your proprietary data. So you have to have hard one process knowledge plus proprietary data or labels that are measurably better and then deliver into a domain specific UX that really adds value. Finally, financial discipline around AI usage is going to matter.
You as a builder will have to make sure that usage can explode without token costs exploding. Your edge is going to be owning a specific workflow on a specific surface with a multimodel back end and a believable margin story. That's basically the big story you're going to have. If you're an engineer, the frontier model itself is less of a moat and how you use it is more of a moat.
The stack is just going to get more complicated. So you need to start to learn to specialize in orchestration, to specialize in systems, not just in prompting. You need to design for tool and provider churn when you're thinking about your career and your systems. And you need to get really, really good at balancing cost, at balancing latency, and at balancing quality.
Being able to show executives, we can cut 60% cost with 5% quality loss is going to be a big deal. And better or worse, security and data boundaries are part of your job. Now, you have to understand to things like tenant isolation, PII flows. You need to expect customers to ask about this and about which providers see their data and under what terms.
If you're an engineer, your edge is going to be turning unstable models into stable systems that the business can bet on. If you're an you know seuite executive, if you own outcomes, budgets or teams, you are responsible for results, but you cannot pick the winner. You cannot pick a model. You have to adopt a portfolio vendor strategy.
You need to plan on multiple primary model partners. You need to decide explicitly where you lean on OS defaults versus where you build your own. So if it's generic productivity, maybe the OS is okay. If it's core workflows for your business, maybe you invest in your own orchestration. But that line has to be an explicit choice.
You need to start to frame AI as a workflow transformation, not software. I keep emphasizing this, but when you approve an AI initiative, the question is which workflow are we replatforming? What metrics will move? People can't answer that. If they can't talk about the workflow, they shouldn't be partners with you on that build.
Governance and safety is going to be a bigger and bigger deal. We saw saw the anthropic hack this week. We need to have inventories of where models are used, policies on data residency, all of the stuff that goes with risk management. Also is going to become a sales enabler for you because other companies are going to take this serious too.
On the talent side, you're going to need to be looking for AI native operators, not just prompt people. So the most valuable hires can map your P&L and ops to AI workflows, start to prioritize those by impact, and then work with technical teams to get them live. Titles are going to vary, but that's the capability that you're going to want to find.
Last, but not least, you have to be disciplined with your capital allocation. Do not fund in-house model training, please, unless you have very clear reasons. Default to renting the intelligence and owning the data, the workflows, and the customers. Ultimately, your edge is going to be turning AI from scattered experiments into a coherent portfolio of bets that you can actually measure ROI against.
This is what I want to leave you with. The strategic insight that we are on the verge of another reset, I think is stable even if the Gemini 3 and Apple story only becomes partially true. If Gemini 3 is a state-of-the-art model, but maybe not 20% better, maybe only 10% better, this could still happen. If Gemini 3 is embedded into Apple, but it takes 6 months instead of 3 months, this could still happen.
The reason why is driven more by the strategic position of the players on the board where Anthropic is, where OpenAI is, where Google is, than it is by the exact timing of individual model releases. And that's what I think makes this a durable thesis. And I think it's worth paying attention to because we have not had a shakeup like this.
We have not had a moment when OpenAI lost the crown. and we're about to find out what that looks like. So, get ready. The AI race is only heating up. I hope that this gives you a sense of where the market and where the AI space is going in general and what you can do to take advantage. Two.

---

<!-- VIDEO_ID: gRhOo6uT-fM -->

Date: 2025-10-12

## Core Thesis
The speaker's central, non-obvious argument is that the AI industry's competitive landscape is fundamentally shifting from a singular focus on raw model intelligence to a systems-level battleground, where strategic routing across a diverse, cost-optimized model ecosystem, coupled with the navigation of critical physical infrastructure constraints, will determine success. This transition redefines value creation, moving beyond mere model IQ to pragmatic engineering and resource management.

## Key Concepts
*   **Model IQ Contest is Over, Infrastructure Wars Begin:** The primary competitive advantage has shifted from developing incrementally smarter models to building robust, cost-efficient AI systems that intelligently manage and route computational work, acknowledging the growing importance of physical infrastructure.
*   **Exponential Capability-to-Cost Curve:** AI intelligence per dollar is improving at an unprecedented rate, doubling every 3-8 months (3-7x faster than Moore's Law), which fundamentally resets unit economics and makes intelligent routing a critical strategic lever.
*   **Routing as a First-Class Architectural Object:** System design must prioritize dynamic routing of requests to the cheapest capable model (e.g., smaller models for simple tasks, frontier models for complex ones) to optimize cost, latency, and quality, transforming it into a core UX and business differentiator.
*   **Answer Engine Optimization (AEO): The New SEO:** As AI answer engines become the dominant distribution channel, content creators must adapt by structuring data for machine parsing, providing canonical APIs, and designing content for extraction and synthesis, rather than just keyword targeting, to remain visible.
*   **The "Atoms Problem" of AI Scaling:** Physical infrastructure (power, land, water, permits) presents a hard, capital-intensive, and geopolitically complex bottleneck to AI scaling, with local NIMBYism and resource scarcity (e.g., water for cooling) directly impacting the availability and cost of AI compute.
*   **Fragility of Advertised Reasoning Gains:** Headline claims of LLM intelligence often don't translate directly to production value; models can "fake alignment" or exhibit "sycophancy" when evaluated, necessitating more rigorous, economically useful evaluation metrics (like GDP val) to assess true capability.
*   **Hybrid Model Architectures for Enterprise:** Organizations will increasingly adopt a pragmatic approach, combining closed frontier models for high-stakes, cutting-edge reasoning with open-weight models for volume tasks, customization, regulatory compliance, and data sovereignty.
*   **Sovereign AI's Hidden Dependencies:** Many "sovereign AI" initiatives are not truly independent, often relying on US hyperscalers, foreign models via API, and NVIDIA hardware, which paradoxically reinforces capital concentration among core providers rather than fostering true autonomy.
*   **Individual Agency in an Era of Near-Zero Cost Intelligence:** While AI intelligence becomes increasingly accessible and cheap, the rare differentiator will be an individual's willingness and strategic acumen to leverage diverse models, understand system constraints, and apply AI to personal and professional workflows, rather than just using frontier models.

---
The state of AI report is finally out. This is an annual report from Airst Street Capital with Nathan Benich at the lead. It's been published for the last eight years. Every time it comes out, it shifts the industry. I'm going to go through and I'm going to summarize the 313 slides in just a few minutes so that you get the TLDDR.
So, first things first, the takeaway is that the model IQ contest is over and the infrastructure wars are just beginning. So the thesis is fundamentally we have been pushing and pushing and pushing to make incrementally smarter models. But now what really matters is systems specifically three compounding forces that drive those systems.
The capability to cost curve, the distribution question, and the physical infrastructure question. We're going to get to all three of those, but the thesis of this entire 313 slide report is that those three drivers are going to matter more in terms of practical AI than just model IQ. And so the winners in the race are going to be the ones that can route computational work to the cheapest capable model rather than just defaulting to frontier smart IQ options.
And that's going to enable them to get to real value. So let's jump right in. the capability to cost curve. What is it? So perhaps the most consequential thing that we aren't talking about and this is the the economic finding the report has. Our intelligence per dollar is improving on an exponential curve that is faster than the pace that most people assume in their strategic plans.
So across two independent leaderboards, artificial analysis which tracks API pricing and performance and also LM arena which tracks crowdsource model rankings, the capability to cost curve is doubling very very frequently roughly every four or five months. The average across all of the different measures and different model makers is between 3 and 8 months to double to be clear.
So Google is at a 3.4 month doubling time, the fastest improvement curve in the ecosystem. Open AAI is at a 5.8 8month doubling time. Google in the LM arena scores is slightly slightly longer, 5.7 month doubling time versus 3.4 in the artificial analysis. And so this is why I give you a range, right? It gives you a sense of how fast it is.
It's ridiculously fast. So for context, Moore's law predicted transistor density doubling 18 to 24 months and that roughly held give or take a few months for a very long time, many many decades. We are seeing effective AI capability per dollar double three to seven times faster than Moore's law. And the pricing evidence is compelling.
T5's input GPT5's input costs for a 400,000 token context window are 12 times cheaper than Claude, 24 times cheaper than GPT4.1. It's not that this is marginal, right? This resets unit economics every few months. When you can obtain frontier adjacent performance for a 20th of the price of just 6 months ago, then you have a lot of strategic implications that start to fall out of that fundamental cost curve insight.
First, routing is now a competitive advantage, not model quality. So products that intelligently triage requests and send simple queries to small language models and reserve expensive frontier calls for when they need it. They're going to capture margin in a way that monolithic architectures can't. And so the practical AI stack now looks a lot like smaller, dumber first routing with frontier spikes only where needed.
Second ecosystem as a whole is scaling usage with the cost coming down. We're processing about a quadrillion tokens every month across different API providers. And at that scale, even a basis point improvement in routing efficiency will translate into millions of dollars in cost savings or expanded margin.
So cost per token and latency are not just back-end concerns, they become relevant to product differentiation to the P&L. Third, model release cadences now correlate directly to fundraising cycles. And what's interesting is that this makes road mapaps financial instruments, right? So open AI trails model release to fund raise by about 77 days.
Google is about 50 days. And so labs will time capability releases to create momentum for funding rounds. And investors should read launch announcements as preundraising signals rather than purely technical milestones. This is relevant not just for open AI, not just for Google, but for others, for anthropic, even for folks outside the core model makers.
People are announcing capabilities in the AI race as a way of warming up the market for a fund raise. All of this adds up to a world where capability is continuing to accelerate even as cost comes down. And that's the first major driver, capability to cost compounding. The second major driver is distribution.
Distribution is tilting toward answer engines in the browser. And I want to get specific about this. The browser is becoming the operating system for AI by default. And the distribution choke point is shifting from the search box like Google to answer engines that can parse and synthesize and present information before the user clicks through.
So Chad GPT search is the gorilla. They claimed 800 million weekly active users last week. They have very roughly, we're still learning how to measure this, something like 60% share of the AI search market. And as a comparison, if you're wondering about Perplexity, which is the most famous AI search engine, it logged about 780 million queries in May of 2025, and it's growing 20% month overmonth.
So, it's going to be over over a billion monthly queries. Now, that is competitive traction, but it's dwarfed by OpenAI's distribution advantage. Open AAI with 800 million weekly active users has much much more on the table when it comes to search. What's interesting is it's not just search and conversation. It's also purchase intent.
So retail conversions from AI referrals are running at about 11% 5 percentage points year-over-year. And that 11% conversion rate is very strong historically. Typical organic search is just not going to keep up. It's much higher than typical organic search. It's competitive with paid search conversion in many verticals. So, answer engines aren't just changing how people find information.
They are driving forward a new vertical of purchase that is going to become hyper relevant for e-commerce and for other ad providers, marketers in 2026. But there's a dependency that we're not talking about. The answer engine still source really heavily from Google's index. They're not crawling the web independently at scale yet.
They're layering natural language synthesis over the top of existing search infrastructure. And that creates a really weird dynamic where Google is providing the index, but OpenAI and others are capturing the intent and conversion. This has some strange implications for builders. If you're not thinking about answer engine optimization as a topic, you you need to be because you don't want to be invisible to the fastest growing distribution channel that we have.
But AEO requires something different from traditional SEO. You have to have structured data schemas that models can parse and understand. You have to have APIs that will allow answer engines to pull canonical information directly. You'll need content architecture that's designed for extraction and synthesis, not just for keyword targeting.
You're going to need citationfriendly formatting that's going to make attribution really clear. And Google faces a really tricky strategic tension. It provides the index that powers competitors answer engines. But capturing that value requires Google to transition users from search to its own AI interfaces without cannibalizing its traditional monetization model.
That will be one of the central questions of 2026. Third major driver, power and permits. This is a hard constraint on scale that we're all going to be facing in AI. So we all have heard about the Stargate project, a 10 gawatt target power, half a trillion dollar investment. Multiple labs are now targeting 5 gawatt training clusters operational by 2028.
But physical infrastructure has to get there to enable that kind of AI progress. This is not a temporary bottleneck. It's a capitalintensive, frankly geopolitically complex problem space that will determine which organizations can execute on their road maps and that will drive success for the organizations that are able to build.
Just to give you a sense, a single gigawatt data center requires about $50 billion in capital expenditure right now. Land, buildings, cooling, networking, GPUs, etc. And it's going to require 11 billion a year fully loaded to operate. So electricity, maintenance, staffing, interconnects, etc. That's not cheap, right? For perspective, a single gigawatt data center consumes the equivalent power of a midsize city.
And so the US currently faces an implied 68 gawatt power shortfall by 2028. That's 68 citysiz data centers that we think will be short per forecast forecast cited by semi analysis and corroborated by the North American Electricity Reliability Corporation. So one of the challenges that we're facing given that gap is figuring out where and how we can actually build to cover it.
And so things that have traditionally been sort of environmental debates like not in my backyard opposition nimiism have become geopolitically relevant AI debates. Right? Nimiism has already blocked 64 billion dollars in data center projects across the US. Local communities are voicing their opinion.
They don't want approvals due to concerns about grid strain or noise or water usage or whatever the local concern is. And they're voicing their complaints in ways that affect build patterns at the county, municipal, and state levels. We don't know how this is going to play out. But the fact that the constraint exists, that it plays out differently in different local communities is going to shape our collective future.
Water adds another layer of constraint. A 100 megawatt data center, so it's smaller, consumes about 2 million L a day in cooling. Now, per text query, per ask, this is a very small amount of water. The typical Gemini text prompt apparently consumes about a quarter of a milliliter of water. a quarter of a milliliter, tiny amount.
But when you get to quadrillion token per month scale, water usage becomes a sighting constraint, especially in droughtprone regions because data centers are going to be competing with agriculture, potentially with residential use for allocation rights. This shifts where you can site your data centers. And so labs and cloud providers are being forced to pursue special uh behind-the- meter power purchase agreements.
They're trying to find ways to get to offshore jurisdictions that have more available power and perhaps fewer permitting obstacles. Norway comes to mind, the UAE comes to mind for that. And they're trying to design for water aare cooling things like air cooling, waste heat recovery. The larger implication is if your AI road map assumes that you can call an API and scale from a million to 100 million users on demand and repric, you need to think through what that's going to take.
We all need to be aware that this hard constraint is going to shape the availability of the rest of the stack. It's going to shape the availability of software. It's going to shape the availability of tokens. Now, there are very smart people working to make things more efficient, working to solve the power problem. Small modular nuclear reactors come to mind.
But there's a difference between working on it and having it be operational. And so if you look at the strategic read and we ladder back to where we were at the start of this conversation, fundamentally the next year or two are going to see this clash between dramatically improving intelligence per cost opportunities like we said that that's the the capability to cost improvements are doubling every few months.
This is going to lead to more and more demand for tokens. But at the same time we have real hard constraints. It is difficult to build these data centers in physical space. It's it's not a bytes problem. It's not a bits problem. It's an atoms problem. It's going to be difficult. So that's the fundamental tension we're all going to be negotiating over the next 12 to 24 months.
I want to get to a second piece that the report called out that I think is really important. If that is our overall strategic canvas, think of this as a set of questions we need to be evaluating within this space. First, I want to talk about evaluation of reasoning gains. One of the things that we need to get more deliberate about and that I think we're starting to see more testing done on recently, but we didn't in the first half of the year is how we measure success, how we measure intelligence, how we measure capability of LLM gains. And so, you guys know I've
talked about the story of Claude disastrously running a vending machine. It was a complete disaster. And the point was whatever the advertised intelligence was, Claude wasn't doing real economic work. More recently, Open AI has launched GDP val where they're trying to test within a constrained environment how AI solves economically useful problems.
Well, one of the larger pieces here, one of the larger reasons we need these kinds of evaluations is that reasoning gains are more fragile than they're often advertised by the model makers. Anyone who's built production LLM will understand this. You have the frontline headline reasoning gains and then you have a discounted value that you can actually use.
The most recent example of this is Claude claiming uh or anthropic claiming that Claude could do 30 hours of work and rebuild Slack, which may well have been true. I don't see any reason why it wouldn't be. But when they tested the same model in controlled conditions on the MER metric, it did not deliver 30 hours. It got close to 2 hours.
That's a big discount. And we're seeing that kind of discount across a lot of different areas in AI right now. And I don't want you to hear that we're not making progress. But I want you to hear that we need to take reasoning gains, intelligence gains as somewhat more fragile than they are advertised on the top line.
And we need to think more carefully about how we can build more sustainably with these systems. Given that the topline gains don't always pan out in the way we expect, we have to be more intentional. And I think that some of these issues are going to get more challenging as models do indeed scale in intelligence because regardless of claims of topline inflation, we continue to see gains in intelligence overall and we need to factor that.
So, as an example of something that gets more difficult as we gain in intelligence with models, models can fake alignment, right? They can detect that they're being evaluated. they can adjust their reasoning chains to appear more aligned and that's something that model makers are actively working to address. It is something that gets worse as models get better and that can be one of the factors that discounts some of the value of the model frontier updates.
Sycophency is on the rise when humans give feedback and one of the core principles of AI is that reinforcement learning with humans is helpful. But what happens when the model gets smart enough to recognize that it's the human giving feedback and it tries to please the human rather than trying to do the task well? What happens when models start to recognize that they're being tested and change their behavior when they're being tested? We're already seeing evidence of that.
And so I'm giving you those three examples because those illustrate that we can have real model intelligence gains, but that factors tied up in the way we train and build our models can undercut those gains to some extent and make it more difficult to make progress that we feel day-to-day. In a sense, if you look at the larger picture, it's kind of amazing that we've made the progress we have already.
I anticipate continued progress in model intelligence particularly in vertical specific applications. But we are going to need to recognize that these topline gains have to be discounted against some of the challenges that come with bigger models, smarter models, models that think more. They have different kinds of challenges that we're working through.
So that is one of the things that makes it interesting. We have this scaling demand for tokens. Tokens are getting cheaper. We have the power constraint piece. Well, now we have to think about as we scale demand for tokens, which model is the right one to get to? What does a frontier model even mean? If it is a frontier model, how do we measure that? Those are all going to become hyper relevant questions in 2026 and I expect more investment in measures like GDP val from open AI because it gives us a sense of how models actually do real
economically useful work. Another big theme is the question around model frontier leadership in closed models versus open weights. China has become the dominant player in the openweight ecosystem. And so even though GPT5 from open AI, Gemini 2.5 from Google and Claude and the Sonnet 4.5 family continue to lead on leaderboards in raw capability.
The frontier remains closely followed by Chinese labs, particularly the Quen lab from Alibaba and Deepseek. And this has been very deliberate. China is pursuing an openweight strategy because it gives them distribution leverage. They can get anywhere on premises, sovereign clouds, consumer hardware, it doesn't matter. And so they have adoption pathways that don't work with US cloud providers.
It also allows organizations to customize and fine-tune. And finally, it allows China to retain 77,000 or more STEM PhDs who are starting to concentrate on AI talent onshore. So there's a talent onoring that's happening around these open-source models that enables China to build an open- source approach that's an ecosystem, not just one single model.
Now Quinn has become the dominant openweight choice across multiple international markets, but it's not the only one. And I would anticipate that the Open Weights ecosystem as a whole is going to continue to shift forward. Now, there is one significant update here. More recently, OpenAI's GPT OSS release, a partially open source stack.
It drove home the fact that Frontier models in the US in Silicon Valley may not just yield ground on open weights models. they may choose to release open weights models that keep their model lineage competitive with models like Quentyn. And so open doesn't necessarily have to mean frontier competitive in a world where you have this in incredible cost to capability curve.
You can have frontier competitive or frontier adjacent models that are super economically useful and that are effectively right. You can have the compute that you can run yourself. You can have the model weights. You can do whatever you want. And so I think that the opportunity we see here is to think about open versus closed as less than binary.
So frontier capability so far remains closed in US-led but open weights have a lot of range. There's a range of open weight standard. Some of them are fully open, some of them are partially open. and they enable, especially as we get these increased cost to capability thresholds, distribution, customization, and sovereignty opportunities that closed cloud opportunities can't match.
Like if you have OpenAI's terms of service from a cloud provider, that's what you got. That's not true with open- source. Enterprises increasingly are going to plan on hybrid architectures. They'll have closed frontier models where they really have to have frontier intelligence for highstake reasoning, but they may well go to open models for volume tasks to handle regulatory compliance or whatever whatever else they may need.
I want to call out something that I mentioned at the top. I talked about the importance of routing in a world where cost to capability is becoming a dominant force. Let's open that up a little bit and explain why routing wins. As capability to cost improves exponentially, you get to a world where GPT5's router UX becomes default.
So, everyone complained from a consumer side about the fact that GPT5 routes you to different models. Well, on the back end, if you're designing systems, that's actually desirable. The interface dynamically selects speed optimized or capability optimized variants depending on the task detection. That reduces cost per query. It improves latency.
It maintains quality. Ideally, people complained about it, but you know, you improve it. Routing is a core UX and business lever. Now, it's not just for back-end optimization. Products that expose routing choice to users or products that offer it invisibly are able to offer better pricing and potentially faster responses at elevated quality.
And that can create differentiation in what would otherwise be a very commoditized space. And so you need to think about your architectural decisions in cases where context is expensive. Long context first designs will simplify systems. They'll you know reduce latency but they concentrate risk on a single provider.
So you need to think about installing model routing as a first class object. Another key theme I want to call out is sovereign AI. So the sovereign AI movement accelerated in 2025. And one of the things that you'll notice is that sovereign AI pathways are not as sovereign as you think. So a lot of these sovereign announcements remain reliant on US hyperscalers for cloud infrastructures.
They will import foreign models via API. They still depend on NVIDIA hardware. And so most of the sovereign mega deals that are announced actually create a self-reinforcing loop that continues to concentrate capital on the core model makers, Nvidia, and perhaps core cloud providers like Azure. So you need to understand if you're seeing sovereign announcements that this may be about what we talked about earlier in this video.
It may well be about data center sighting and the availability of power supply more than it's about a truly sovereign and independent AI. So what are some of the implications that we see here? If you're in a building or founding space, you need to be assuming a world next year where intelligence per dollar continues to double every four or five months.
And therefore your margin opportunity is smarter routing. And so you need to think about a core product lever that enables you to route more intelligent that enables you to deliver higher quality at lesser cost with the assumption that models will continue to deliver more capability for cheaper going forward. You also need to think about how you capture distribution with answer engine optimization.
You should assume that there the 60% AI search share from Chad GPT, the 11% retail conversion rate, those are sticking around. You should assume that there's going to be an ad network launched against that. You should assume that if you are not in place with AEO optimized content like structured data, canonical APIs, etc.
You are invisible to the fastest growing e-commerce distribution channel. You should also have a keen eye on the relevant infrastructure risks in your space. And so that means, as funny as it sounds, keeping an eye on how Stargate is doing and how other major data center projects from Microsoft are doing because any delays there driven by power constraints, driven by nimism, etc.
end up flowing through into real token availability for businesses. And that could become relevant in 2026 given the exploding pace of token. If you're on the investment side, you're going to want to be reading the news of tomorrow and understanding the investment picture through the assumption that companies win on routing intelligence through the assumption that you have real dependence on the core model makers in Nvidia that isn't going anywhere.
Those circular flows are real. through the idea that demand is scaling faster than supply across the board and that in that world you need to understand who has infrastructure access and who doesn't. And finally, you need to think about distribution. Who has distribution in the middle of a world of performance bottlenecks? Chad GPT certainly does.
Who else does? And who is able to maintain and grow that distribution in this complex world? Finally, if you're an AI enthusiast, you should take away from this that this is the beginning of the next step in the AI revolution. If the first step was around the models just getting smarter and smarter and smarter and we're all going after those smarter models every single time.
This wave is about how we can move from just playing on pure intelligence just celebrating the fact that we can now do Excel and PowerPoint to a world where we need to have differentiated skills around particular workflows and systems. So just as you can talk about router intelligence and the cost per capability curve for systems, you can think about that for your own skill set.
How can you route workflows more efficiently now that you understand this situation better? Is Chad GPT5 always the right choice or do you go with another model? How can you think about that more deliberately? Is your particular model choice going to be a model choice where you have expanded availability over time or do hard infrastructure constraints make that more difficult? Enthropic is actually a good example of a model maker that has remained infrastructure hard limited for most of 2025 and that is one of the reasons why they've been unable
to roll out things like rolling context windows. It's been one of the reasons why they've had some persistent issues the last few months with outages etc. and rumored to be one of the reasons why they've struggled with releasing some of their newer models. The point here is not don't choose Anthropic.
I love Anthropic. It's a fantastic model. The point is that constraints already are impacting real world availability, not just for Anthropic. Chad GPT has issues at times as well and they've been honest about that particularly post launch. Be aware of the constraints that shape intelligence availability in your space and be smart about what you choose to build what you choose to do with that.
This is why major tool makers like cursor and lovable think about having a multimodel architecture underneath. It gives them that option to pick a different model. The last thing I will call out is that none of this is theoretical. We are living in a world where the cost of intelligence really is going to zero.
And it gives individuals an immense amount of agency to choose your own adventure. Your ability to form intent and go after something with clarity, focus, and dedication has never had more leverage because the intelligence is going to get better and better and cheaper and cheaper. We are in a world where you can teach yourself any skill you want with the help of AI in just a few months.
And so it's going to be on individuals to go after what they want and the ability is not going to be evenly distributed. What I'm finding is that even though everyone has access to these models, very few folks are making the most of them. And so the willingness to jump on and make the most of not just frontier models but potentially cheaper next generation models that are adjacent to the frontier.
The willingness to understand these strategic constraints and think about the opportunities that you have. That is rare. That is rare. And so if you watch this video, thanks for tagging along. You were probably one of the few that is paying attention to the strategic themes in AI right now. Best of luck in 2026. It's going to be a wild wild ride as I hope this video is made

---

<!-- VIDEO_ID: gtkRAXQf49k -->

Date: 2025-11-18

## Core Thesis
The speaker argues that the prevailing focus on acquiring new "AI jobs" is a misdirection; instead, individuals must proactively transform their existing roles into "AI-native" jobs by understanding AI as an emerging infrastructure layer and collaborator on structured work, rather than a magic brain. This requires a pragmatic, workflow-centric approach to integrate AI agents within organizational guardrails, shifting human value from repetitive execution to defining, supervising, and governing intelligent workflows.

## Key Concepts
*   **AI as an Infrastructure Layer:** AI's fundamental shift from a chat interface to a foundational, underlying system component, enabling deeper integration and automation beyond superficial interactions.
*   **Agent as a Loop:** A first-principles mental model defining an AI agent by its core operational cycle: Goal -> Gather Context -> Reason -> Act -> Observe. This provides a structured understanding of agent behavior.
*   **AI as a Collaborator on Structured Work (not a magic brain):** A counter-intuitive understanding that LLMs are powerful pattern machines for transforming text and code, excelling at repetitive, bounded, and verifiable tasks, but are ill-suited for ambiguous, high-stakes decisions or understanding nuanced human context.
*   **Agents + Orchestration = New Middleware:** A powerful analogy positioning AI agents and their coordination as the intelligent "translation layer" within software stacks, driving productivity by connecting models, tools, data, and decision logic.
*   **Governance as the New Operating System:** A pragmatic framework emphasizing that security, privacy, and auditability are not optional "bolt-ons" but fundamental requirements for successful, scalable enterprise AI adoption, making them everyone's responsibility.
*   **Job as a Stack of Workflows:** A decompositional mental model for analyzing one's role into discrete, trigger-driven workflows (inputs, transformations, decisions, outputs, checks), enabling precise identification of tasks suitable for AI augmentation.
*   **Value Shift from Execution to Definition & Supervision:** A counter-intuitive insight that human value in an AI-augmented world moves from performing repetitive tasks to defining, supervising, handling exceptions, and strategically choosing which problems AI should solve.
*   **Pragmatic AI Integration:** The strategy of prototyping AI solutions within existing organizational guardrails and collaborating with IT/security teams, rather than engaging in "shadow IT," to ensure secure and scalable deployment.

---

My inbox and my DMs are full of people saying, "Can I get an AI job? How do I get an AI job?" And that is the wrong question, people. The right question is, "How do I turn my current job into an AI job?" I'm dead serious. And I'm going to talk about it here. Your goal in 2026 is going to be much more specific than a dream of another job.
It's going to be not changing careers, not becoming a prompt engineer, but how can you change the way work actually gets done in your current role using the AI infrastructure your company is already rolling out. I am telling you for 95% of us that is the way AI is going to come. And we don't talk about it.
We talk about changing jobs all the time, but like that's a tiny sliver of the world. For so much of us, it is not about that. I'm actually going to focus on what changed in AI in 2025 underneath the hype, the new mental models that you need to understand what matters in 2026, particularly around AI agents, and a practical path to making your existing job an AI native job.
So, what actually changed in 2025? like underneath the hood, underneath all the hype, stepping back, the first thing you need to recognize is that AI moved from a chat interface into being an infrastructure layer this year. So for the last two years, for most of us, the experience of AI has been it's a chat box, it's a writing assistant, maybe it does some code completion.
That is now the most superficial layer of AI. Underneath the surface, three big shifts happened in 2025 that changed the game on AI. Number one is that architecture started to get standardized. Google's recent introduction to AI agents paper is just the latest example of this. The larger perspective if you step back is that we have started to get a clear industry definition around an agent as a loop.
An agent has a goal, gathers context, it reasons, it acts, it observes. And we have patterns now for multi-agent systems that include planner agents, retriever agents, executor agents, etc. We also have a the beginning of an industry model for agent maturity from simple tool calling all the way up to self-improving systems which nobody has or almost nobody has.
And finally, we have design principles around how we think around issues like a budgetary authority for agents, boundaries for agents, security identity for agents. Still evolving, but it's starting to come into place. The the reason you need to care about this is that until we had that architecture, agents were mostly theoretical or they were point solutions to problems.
Because of the work done in 2025, because that architecture is more standardized, we are now set up to do much more interesting things, much more comprehensive work with agents in 2026. The second big piece in 2025 is that security is no longer a hypothetical. 2025 was a year of shadow IT. bring your own AI to work.
Maybe security won't check. Maybe your chief information security officer won't notice you brought your personal chat GPT. That is increasingly going to be out of bounds, caught and not allowed. And the reason I say that is because these CISOs, information officers have had a year to get their teams in gear to approve a bunch of tools like Cloud Code, like Chad GBT, like lovable.
And so increasingly the tools that are allowed are inside the fences now. And the critical thing that you need to be aware of is that the security focus is now moving into that agent space. And so more and more the real meaningful shifts are going to be done in partnership with your security teams at work.
It's not going to be just the marketing team setting up their individual little tool and hoping and praying nobody notices. more and more that's going to require your partnership with the rest of the IT department and that is something I will absolutely get into but it's it's a skill we need to develop that most of us haven't had to use before because frankly the ability to deploy technical agents to do this work is brand new.
The third major change in 2025 is that enterprises learned where AI agents actually work. This is probably the biggest one. I can't underline this one enough. across hundreds of deployments. The pattern is annoyingly consistent. Agents are reliable and deliver really good ROI on work tasks when they are bounded in scope, when they are objectively verifiable, when they are repetitive, and when they have clearly defined inputs and outputs.
So you can think back office operations, triage operations, claims, lead qualification, document checks, basic compliance, customer support flows. It is not invent our product strategy the AI agent. It is hey can you execute this same process we do 10,000 times a week and please don't get bored.
That's where AI agents are going. So 2025 gave us a lot of clarity and that shapes how we prepare ourselves in our roles for AI agents and yes it will touch all of us. So it gave us clarity on what agents are how they operate at scale when where they're safe where they're useful and where they're dangerous if you're sloppy. This all lays the foundation for what comes next.
If you're looking ahead to 2026, these are the three mental models that you need to survive in your career as we start to have AI agents more and more in the workplace. Number one, AI is a collaborator on structured work. It is not a magic brain. So, I'm going to say it again, LLMs are pattern machines. They're very, very good at transforming text and code.
They they can map messy inputs to structured outputs very well. They follow explicit instructions increasingly well and they can do the same thing a thousand or 10,000 times and never get bored. But they are not inherently good at making high stakes decisions with very ambiguous trade-offs. They don't understand your organization's politics or background well.
They don't know your context unless you give it to them. and they are very very bad at respecting boundaries that you have not defined previously. And so the right question is not can AI do my job although I hear that a lot that that's wrong. That's not the right question to ask given what we know about AI agents today.
Instead it is which parts of my job are repetitive are checkable are describable or verifiable and how do I turn those into workflows that AI can run or assist with? How do I begin to take charge of how AI shapes my job? And if you can't describe the work clearly, that's something that you're going to have to do.
The AI just doesn't have a chance at that. The second major mental model is agents plus orchestration are becoming the new middleware. And if that sounds abstract, the key thing to understand is that middleware has always existed in our software stacks. In between backend and front end, there has always been a piece of the stack that translates.
That part of the stack now got intelligent. It got intelligent because agents are increasingly going to be that middleware. All an agent is is a loop around a model. It has tools. It has some kind of state that it's working with and it has decision logic. That's it. The important part here isn't that we label this middleware.
It's that we understand that this orchestration layer is going to be driving a lot of how we do productivity. And we need to take charge of what that looks like. So what tools does it allowed to use? Under what identity is it secure with what budget? What where are the logs and the metrics stored? What does it do when it doesn't know? This is the part that most people don't see or think about.
But you need to think about it if you want to have a productive relationship with AI agents in your role. You need to at least understand the vocabulary, how models talk to tools and data. Maybe through model context protocol, maybe other ways. What are agentto agent protocols? How do teams of agents coordinate? And how can you talk about that at a high level even if you're not an engineer? Control panes, gateways.
What are the choke points where organizations are going to enforce security policies and observe behavior? How do you ensure that the agents that are built have the right roles and permissions? I am not expecting you to implement this yourself. Most people won't. But if you want to be taken seriously, you do need to be able to talk at a high level about AI workflows in your area in these terms because that makes you translatable.
That makes you accessible to people who will be building this for you and you will want that skill. The third major mental model for 2026 is governance. It's not a bolt-on. It is it's going to be the new operating system, guys. AI is becoming grown up. If your AI adoption story doesn't include security and privacy and auditability and all of that stuff that seems boring, it's not going to be taken seriously.
And so you need to be providing proactive answers to in your domain, where would you allow AI to act autonomously? Where would you allow it to only draft? Where would you require a human approver? How do you shut it down safely? This is no longer just your chief information security officer's problem. It is becoming everyone's problem because AI agents will not roll out successfully if they do not know your local information and data.
So where will AI actually reshape your job keeping all of that in mind? Fundamentally, you need to think of your job as a stack of workflows. Your job is going to be decomposed and you need to take charge of what that looks like. So don't think of it as doing marketing. Think of it as you run campaigns, you create briefs, you analyze performance, you manage stakeholders.
Those are workflows. You don't do product management. Instead, you collect requirements. You prioritize. You write specs. You coordinate launches. Workflows. Again, you don't do finance. Instead, you reconcile. You forecast. You analyze variants. You produce reports. Again, workflows. Each of these can be decomposed into triggers, what starts the work, inputs, what you look at, transformations, what do you do with it? Decisions, outputs, and checks to know if it's correct.
AI slots into a structure like that. AI will handle the boring and repetitive parts of those workflows. It is up to you to figure out how that actually shapes in your role. Across industries, the same categories keep getting automated or heavily assisted. Triage tasks, routing tasks, summarization tasks, synthesis tasks, policy and rule tasks, repetitive document workflows like pulling data from forms, glue work across tools, moving information from Excel into Word or vice versa.
If you look at your job honestly, for most of us, a non-trivial percentage is in one of those buckets. And that is what is going to move first. Now, the parts that stay human for a long time to come are parts around negotiation, around trust building, around politics, around deciding which problems to solve, around setting strategy, around being accountable when things go wrong.
So, I don't want you to hear Nate is proposing that I AI away my job. I want you to hear that AI drains repetitive and checkable work out of your role. You should be in charge of what that looks like or someone else will do it for you. And your value is going to shift toward defining workflows, supervising them, handling exceptions, choosing what to build, touching the work that matters.
And so when you think about what to do in the next few weeks as you head into 2026, if you want to get a running start, number one, map your work as if you were a systems designer. I've given you a cheat code here. Write down your workflows. Write down what triggers them, what inputs there are, what outputs there are, what decisions there are.
Learn to express those workflows to the tools you already have. Try something even if it's prototypy in chat GPT enterprise or in Copilot or in Gemini to get the idea of what that workflow would look like so you can show a workable prototype when it comes along and has an AI agent initiative.
I'm not saying spin up rogue infrastructure. I'm saying try and prototype something so you get a living feel for what AI agents working with you would look like and then be in the driver's seat when you have these conversations with your engineering teams. I would also encourage you to build a relationship with the people championing AI in your org.
Maybe that's you cuz you watch this channel or maybe it is someone else who is responsible for the technical side. But either way, make sure that you are finding the right team who is responsible for AI initiatives in your area. That you are showing them you've done your homework and you're thinking inside the existing organizational guard rails.
You're thinking about workflows. You're thinking about patterns and tools. At that point, you're no longer a random person. You're a valuable champion and an ally who speaks both languages. The messy reality of the business language and the constraints of the platform that the technical teams think about. and you are in a position to be a fluent translator of AI and drive how AI agents work with you in your role.
That is a very valuable position and that is what you need to be able to do to be in the driver's seat. This is what I wish I could tell 95% of people who are not going to switch jobs in the next year for AI roles. This is what you need to know to be in charge of AI in your role. So, I hope this has been helpful. There's a lot more written up on the substack, including some prompts to help you think through this.
My goal is to give you a guide so that you can meaningfully engage with your existing role and prepare for it now before we go into 2026. And AI agents are absolutely everywhere. Good luck.

---

<!-- VIDEO_ID: iGvJpBWWGOU -->

Date: 2025-11-23

## Core Thesis
AI has transitioned from a miraculous novelty to an inevitable utility, fundamentally reshaping not just technology stacks but also organizational structures and competitive dynamics. The non-obvious challenge for leaders is to move beyond superficial experimentation, strategically design multi-model architectures, and proactively adapt organizational design, recognizing that AI adoption is a path-dependent process that redefines value chains and power structures.

## Key Concepts
*   **AI as a Moving Target / "Once it works, we stop calling it AI":** The definition of AI is fluid; successful AI integrates into utility, losing its "AI" label, driven by novelty rather than inherent capability.
*   **Platform Cycle Frame & Non-Deletion of Layers:** New technological platforms (like AI) follow predictable wave patterns of investment and reshaping, but critically, they rarely *delete* previous layers; instead, they augment and integrate, creating fractal complexity.
*   **Model Commoditization (with Frontier Nuance):** While foundational models are increasingly becoming commodity inputs, the *frontier* of AI innovation (e.g., top labs like Anthropic, Google, OpenAI) still holds defensible moats through advanced methodologies and general intelligence, distinguishing cutting-edge from widely available models.
*   **"Alien Intelligence" Mental Model:** Andrej Karpathy's analogy of LLMs as "non-animal alien intelligences" provides a powerful cognitive framework for users to build better mental models, enabling more effective collaboration, prompting, and leveraging their unique capabilities.
*   **AI as "Inevitable Utility" (vs. Optional R&D):** AI has crossed a tipping point from "miracle" to "inevitable infrastructure." The strategic question shifts from *if* it works to *where* value and margin accrue, demanding a proactive, integrated approach rather than treating it as a tunable R&D play, akin to the ubiquity of spreadsheets.
*   **Path-Dependent Adoption & Strategic Beachheads:** AI adoption is not linear or random; the initial workflows chosen ("beachheads") critically determine future compounding benefits or costs by shaping information flow and unlocking subsequent possibilities. Leaders must strategically design these entry points, focusing on high-leverage junctions in organizational information flow.
*   **Multimodel Architecture for Buyer Leverage:** To counter vendor lock-in and gain leverage as models commoditize, organizations should design multi-model architectures from the outset, allowing them to arbitrage different models based on cost, latency, data sensitivity, and jurisdiction, rather than committing to a single vendor.
*   **"AI Eats the Org Chart":** AI's impact extends beyond the tech stack to fundamentally reshape organizational structures, roles (shifting from doing to specifying/checking), power dynamics, and management layers, effectively acting as an "informal chief of staff" for knowledge workers and necessitating rapid adaptation of organizational design.

---

This week, Benedict Evans, a 20-year veteran of A16Z, gave a memorable presentation in Singapore called AI Eats the World. My executive briefing this week is going to be focused on what he's talking about, why we need to pay attention, and what the implications are for all of us who are building and leading AI teams. Let's get right to it.

So, first, who's Benedict? So he he has been a tech strategist for the last 20 years specifically focused on platform shifts which makes him perfectly positioned to think about what AI means strategically. So he's been involved in PCs, the web, smartphones, social, and of course now AI. His job is to think about how these shifts change power margins and industry structures.

So he's not selling you an AI product in this presentation. He's trying to be a macro translator between the hype and the P&L statement. So he's useful as a sanity anchor in a world that loves hype. The setting is Super AI Singapore 2025 and he is talking to senior leaders, right? CTO's, investors who are asking themselves, is AI a bubble? Is AI just the next software cycle? Is this the moment when everything we know about software economics breaks? So what did he talk about? This is 90 slides.

I'm going to get it into just a few minutes for you and then we're going to start to look at strategic takeaways that I pulled out and what I think it means for all of us. First, Ben talked about AI as a moving target. AI used to mean databases. Then it meant search. Then it meant classical machine learning.

Once it works, we stop calling it AI. I love that insight. So today, large language models and generative models are wearing that label. But other stuff, people are forgetting that it's AI because it works. When you think about it that way, you start to realize how deep the roots of this technical transition are and how much of our adoption curve is driven by novelty.

Ben also talked about the platform cycle frame. The idea that we are moving through predictable wave patterns even as AI is a novel technology. But these novel technologies have predictable patterns that AI is following. So, we've moved from mainframes to PCs to web to smartphones and now to AI. Every wave attracts massive investment at first.

It reshapes who are the winners and who are the losers. But this is the critical point. It rarely deletes previous layers. I loved that takeaway because it's fractal. That takeaway works both for the larger insight that like I have a smartphone now and also a laptop but also in the world of AI the newest tools that are coming out in 2025 are rarely deleting the base tools.

We are getting new tools for 3D models. We are getting new tools for vision. We are not deleting chat GPT. So the idea that like you can have massive investment and reshape winners and not delete previous layers seems very powerful to me. On the capex side, Ben pointed out that yes, big tech is spending hundreds of billions, if not trillions, on data centers and GPUs.

And at the same time, more and more labs are grabbing on to proliferating AI technology so that they can train good enough models. The net effect is that the model itself is looking like a commodity input. And we have talked about that a fair bit on this newsletter. You should not be surprised to hear that the model is not a moat.

I will add a caveat that Ben didn't talk about a ton. One of the other papers that came out this week was a deep study on Chinese open-source models. And one of the things it concluded is that the flexible intelligence of these models taken in aggregate across Quen and many others is less clear, less effective, less generally flexible than the intelligence of Americanmade models.

And that may be because it's not quantized effectively or distilled down effectively. But the general conclusion of the paper is that Chinese models are heavily reliant on US frontier models and distilling those down to get to opensource models that they can release to the world. And in a sense, what the paper suggested is that the pace of innovation is still being driven by private models developed by frontier labs in the United States.

and the rest of the world is following suit in pulling distillations out of those models that may be good for some use cases but are not as generally intelligent and are not appropriate for cutting edge uses within that context right Ben's statement needs some nuance because I would argue that the methodologies used by the cutting edge labs are defensible and certainly their edge is defensible and so no one is going to join the table of top model makers which frankly at this point we've even lost folks books in the last year.

Meta is not a top model maker anymore. Grock is trying to be but isn't leading anything right now. The top model makers are Anthropic, Google and OpenAI. That's it. And so in a sense, the model may become a commodity. Intelligence may be in everything and yet we still may have cutting edge modes.

Let's move on to what else Ben talked about. One of the things he called out that we'll talk a fair bit about here is the adoption gap. Lots of people and companies have tried AI, but Ben made the point that far fewer use it daily in core workflows. I keep pounding this drum. The difference between casual chat GPT users and passionate professionals is night and day 10x.

And this is critical for teams because one person on your team, two people on your teams who are an eight or a nine or a 10 in terms of their AI skill sets out of 10, they are going to run circles around everyone else. And so the blockers to adoption, the blockers to moving people that way are really around motivation, the ability to understand what these models can do, and then on the corporate side, how do you get them integrated? How do you handle governance and risk? And how do you roll them out? One of the things that Andre Carpathy talked about this week on X

that Ben didn't mention because it hadn't happened yet is he talked about this idea that we need to be able to imagine LLMs as nonan animal alien intelligences at a high degree of fidelity so that we can understand how to work with them. Effectively, what he's saying is we are as a species having our first contact with a new intelligence.

And the better we can build a mental model of what that intelligence looks like and how it works, the more effectively we can partner together. This is not a like scary doomsday first contact movie. It's more about imagining how the intelligence works helps us to prompt better, work better, collaborate better, all the boring stuff that's really important.

And this is something that Ben didn't get into, but I think is really important. Having that imagination, that aha moment in your teams is critical to enabling outsized leverage, outsized impact for the team. So that was the heart of his message. That's what he talked about. That's 90 slides in just a few minutes. What are the deeper takeaways here? Number one, I think we've quietly crossed from miracle to inevitable utility.

So this is much more subtle than a commoditization argument. I think Evan's talk marks a tipping point. AI is no longer being framed in most settings as will this work, will we get there? Instead, it's being framed as obviously this works. Where does the margin end up? Where do the winners end up? That's especially true and top of mind this week when we saw visual reasoning solved with Nano Banana Pro when we saw Meta SAM 3 model drop and handle semantic search for video.

We have these previously difficult spaces where we're seeing AI just works. And then we have confirmation from Google that Gemini 3 didn't have special tricks up its sleeve. It was classical pre-training and post-training LLMs. There is no wall on training. You can just get bigger and better and train the same way you always have and get a smarter model.

That may sound like a benol observation, but knowing that that's true and seeing the breakthroughs that we've had, we now are just living in a world where this is inevitable. AI is going to be everywhere. AI has already solved enough problems to let us know that the scaling laws hold. And if we assume it's everywhere, we need to ask a different set of questions.

Where do we matter? Where do our companies matter? How do we set up ourselves as competitive players in this space? Those are becoming the relevant questions. And so the strategic risk isn't sort of missing the AI moment. It's really continuing to act as if this is a tunable or optional research and development play instead of this is inevitable infrastructure and if you don't go after it with every tool you've got, you're just not going to make it.

A smarter question to ask in that world is if AI is as inevitable as spreadsheets have become, what parts of our value chain become just a feature in that world and are no longer competitive? That's a tight interesting question to play with. Deep takeaway number two, adoption isn't just slow, it is path dependent and it can trap you. So adoption is lumpy.

Evans pointed that out. Lots of pilots, not a lot of deep usage. Some people use it a lot. Whether and where you choose to adopt shapes what becomes possible later. And he didn't talk about that. But think about spreadsheets. The first teams that adopted them weren't just more efficient. They reorganized how information flowed through the business.

They could model scenarios. They own the numbers. They could self-s serve. LLMs and agents are poised to do the same. So the pattern is going to be you drop AI into one or two workflows. Those workflows shift how information is produced. They shift how it's consumed. And that in turn shifts which other workflows are now possible.

So the non-obvious leadership problem for you is if adoption is path dependent, are we choosing the right beach heads? talk a lot about problem framing about picking the right places to jump in with AI and that's really the question in front of us as we confront an adoption challenge in our teams recent model evolution makes this an even sharper problem agent native models Gemini class etc aren't just better autocomplete right they're they're suited to many kinds of work meaningful work knowledge work triage coordination followup repetitive

decision loops with clear constraints if your first experiments are all summarize this doc You're never going to discover the compounding benefit of agent assisted customer onboarding or agent assisted engineering support. Essentially, the beach head you picked constrains some of your paths forward.

So, where should we try AI is not a random sandbox question for a Friday afternoon. It is a path design question. In other words, you you will get compounding benefits or compounding costs depending on which workflows you choose. So look where there are important junctions in your organization's information flow patterns and jump in there because when you can create a change in that flow, you unlock a lot of downstream benefits.

You unlock a lot of opportunity to use AI agents elsewhere. Non-obvious takeaway number three, AI is going to turn you into a buyer with additional leverage if you design for it. So Evans commoditization story has a second order effect that most people aren't talking about. As models get closer to par and quality, as you get more model options, your power is going to increase as a purchaser of models, as long as you structure for that effectively.

Enterprise AI conversations still turn too often on vendor lock in. I have screamed about this a lot. I'm going to say it again. Don't say we're an Xodel shop. Just be multimodel from the get-go. If you take Evans seriously, if you take me seriously, the long-term equilibrium is going to look like treating models as components and routing your workloads to different models based on the cost, the latency, the data sensitivity, the jurisdiction, etc.

That's not the reality in most of our orgs today. It is something we need to get to. So, the non-obvious implication is don't think about picking a winner model or even a winner lab. Instead, think about building an architecture that lets you be in the driver's seat in buyers conversations and lets you arbitrage models the way you want over time.

Don't settle for lockin. Deeper takeaway number four, AI is eating the org chart, not just the tech stack. And it's not about layoffs. So Evans focuses on tech cycles, but if you extend his logic, spreadsheets didn't just change software, they changed who needed to talk to whom, what roles become bottlenecks, which functions gained political power like finance and operations. Cloud.

Cloud didn't just move servers off premises. It shifted power from central IT to product and engineering. It accelerated the pace at which teams could experiment. AI will do the same for roles that are around coordination, for roles that are around synthesis versus roles that are mostly judgment and constraint setting. So recent agent style capabilities make this more concrete.

A model that can read your emails, Slack, tickets, dashboards, you you name it, right? And propose actions is effectively an informal chief of staff for every knowledge worker. And we should expect that by 2026. That doesn't just increase individual productivity. It changes who needs an assistant, who needs a team, where the bottlenecks in decision-making live.

And so the non-obvious implication for you as a leader is if you only think of AI as a tool roll out, you will miss that you are doing an org design change at the same time. Some roles will shift from doing work to specifying to checking to escalating that work. Other roles will shrink because the coordination overhead they manage gets automated away.

So your span of control assumptions, your management layers, your hiring plans are all going to need to adopt much adapt much faster than in previous cycles. So Evan is giving you the technical story here, but I think we need to extend that out to the org story. So where does this leave us? I want to suggest to you especially at the end of one of the most jaw-dropping weeks I can remember in AI that we need to be taking a step back regularly as leaders and we need to be asking ourselves when we have weeks like this where I can't name the number of

significant developments we had I've attempted to it's like half a dozen or so over the course of the week we need to say does any of this change the strategic operating reality of the business that I am building and I think Evan's matrix Evans talk AI eats the world gives us a good framework for that because it enables us to say okay is there something that is shifting the tech adoption cycle here is there something that is shifting my org chart here is there something about how information flows in my business that is

changing is there something about my vendor relationships and my power with vendors that is shifting because of this unlock and the answer if we ask is often yes but having the right questions to ask helps put us in the driver's seat during times when the news cycle feels relentless on AI and I got to say that's not going to stop.

And so my encouragement to you if you're feeling overwhelmed and you're trying to think about how to sort all of this out is make a regular practice of stepping back and looking at the world like Evans does. Take a day, step back, get a whiteboard out, maybe you get your senior team together or just go for a walk in the woods and figure out what this means for your business.

Distill it down. Take your time because that time to reflect is what is going to enable you to digest, synthesize, and form core conviction that you need to push your teams forward. A lot of what I'm talking about here is really the meat of where leadership and understanding of AI meets the road, where you need to be at with your teams to drive them forward.

And you can't do that if you don't have energy and conviction. And that comes from having the ability to reset, digest, and synthesize all of these updates effectively and then come back with fresh energy. So, take that into the week and uh I'll see you next

---

<!-- VIDEO_ID: KT4v_I9zvH4 -->

Date: 2025-10-20

## Core Thesis
The proliferation of AI has permanently collapsed the cost of information production in the job market, rendering traditional "credentiing" signals (like resumes) meaningless noise. The path forward requires a fundamental shift from optimizing for easily faked outcomes to demonstrating verifiable processes and core capabilities, leveraging AI itself as a tool for structured assessment rather than mere content generation.

## Key Concepts
*   **Shannon Entropy in Labor Markets:** The principle that when the marginal cost of information production approaches zero (due to AI), the signal value of that information (e.g., resumes, cover letters) collapses, leading to a "cacophony of noise" where both job seekers and hirers are overwhelmed.
*   **The "Bad Equilibrium" of the Talent Marketplace:** A state where all stakeholders recognize the system is broken and have an incentive to change it, but individual actions to "yell louder" only exacerbate the problem, preventing a collective shift to a more functional state.
*   **Shift from Credentiing to Verification:** A fundamental paradigm shift where proving skills moves beyond static credentials (resumes, certifications) to dynamic, provable demonstrations of capability, focusing on *how* work is done rather than just the final *outcome*.
*   **Process as Product/Signal:** In an AI-augmented world, the iterative process, debugging steps, and decision-making journey (e.g., effective LLM co-work, handling mistakes) become a more robust and less fakeable signal of competence than polished final outcomes.
*   **LLMs as Verifiers, Not Just Generators:** A counter-intuitive application of LLMs, suggesting their power lies not just in generating text (which creates noise), but in their ability to act as sophisticated evaluators, researchers, and adaptive assessment tools to *create* signal (e.g., cryptographically signed conversations, adaptive competence assessments).
*   **Bilateral Value Creation in Hiring:** A strategy where job seekers proactively help companies clarify their own needs and define fuzzy roles, transforming the interview from a one-sided assessment into a collaborative problem-solving exercise that demonstrates unique value beyond what AI can generate.
*   **Capability Spaces over Job Titles:** A framework for navigating rapidly evolving roles by focusing on underlying, transferable skill sets (e.g., technical communication, system design under uncertainty) rather than ambiguous or quickly outdated job titles, enabling more effective semantic matching in hiring.
*   **"Zigg When the Market is Zagging":** A strategic mental model advocating for counter-cyclical behavior; when the market is collectively moving towards increased noise (zagging), individuals should move towards increased signal and verification (zigging) to gain a disproportionate advantage.
*   **Information is Free, Verification is Priceless:** The core economic principle underpinning the new job market reality, where the abundance of easily generated information makes the ability to genuinely verify skills and competence the most valuable commodity.

---

I Spent Months Studying the AI Job MarketHere are 5 Secrets to Stand Out No One is Talking About - YouTube
https://www.youtube.com/watch?v=KT4v_I9zvH4

We all know that LinkedIn is dead. But the problem is most of the advice that I see online is still optimizing for that dead system. I want to step back. I want to look at the root causes of what's going on with the AI job market collapse. And I want to talk it through step by step and get to a spot by the end of this 10-minute video or so where you actually have a clear perspective on what's going on, a clear sense of your actionables, the tools you have at your disposal that are not just the standard advice. and if you're in the hiring chair, a clear sense of how you can start to differentiate as you hire. So, let's get into it. We don't have a lot of time. Number one, the core issue here is that signals in the hiring market have collapsed to zero because marginal cost of information production is also zero.

In other words, the job market used to work because signals were expensive to produce. So, a resume took time. A good written resume took more time. Cover letters took genuine thought. I used to be able to read a resume and I could read the effort behind it. The cost worked because the cost separated signal from noise. AI has collapsed that cost to zero.

We all know that. We live that every day. When you can write a good resume at zero cost and in fact pump out 10 different custom rsums, there is no information in that signal. The fancy word for this is that this is Shannon entropy and Shannon entropy is playing out in the labor markets, right? The less fancy way of saying it is that because it doesn't cost anything to make information, that information loses signal, value, and hiring and we're all in trouble.

That's what we feel, right? What's interesting is we mostly talk about it from the talent side, but the truth is both sides are drowning. A thousand applications per job sucks for everybody. And the problem is both sides right now tend to give advice that creates more noise to cut the noise. So yell louder, but everyone's being told to yell louder.

Everyone's being told to put a portfolio out there. Everyone's being told to start a social media presence of some sort. Everyone's being told that like you should be like putting more and more job descriptions out there if you're a hiring manager. And it all adds up to this cacophony of noise in the AI job market.

And what I want to suggest to you is that the information equilibrium that we used to have before 2022 is permanently gone. It is not coming back. More noise does not fix this. In the past, strong candidates could afford the effort to raise the noise level and they could break through with signal. And weak candidates faced issues with their ability to actually put the effort in and generate quality work.

And so we started to have a useful signal when people put more effort in. LLMs have destroyed the value of effort from good candidates and they make it equally cheap for everyone to produce infinite signals. And I think we have to start by just admitting the old game that we played before 2022 is over and we don't know how to play the new game yet.

That's what I'm getting to with this video. Every solution we have is adding to that noise. And I want to be honest about that, right? When you optimize your resume, when you optimize your portfolio website, it all adds to the noise. And so what I want to suggest here is that what we need to do is to move from a world where information is cheap to produce for everybody to a world where we start to see verification instead of credentiing.

So credentiing is what we used to do. Credentiing is what a resume is for. Credentiing is when we have certifications. Verification actually shows in a provable way that we have the skill. And I think that we are trying to make our little baby steps that way when we talk about the idea of proving work through a portfolio.

But we can go a lot farther than that if we go back to first principles and actually reason this through. Let's look at what it takes when you see verification as the heart of a new way of thinking about jobs in the postAI era. In the era when information costs nothing to produce. I want to suggest five quick principles for a verification world.

And I want to start to suggest to you how we could start to build those out and game those out even now. One of the hard things, one of the things that has make it made made it difficult to make this video is that a marketplace like the talent marketplace is sometimes stuck in a bad equilibrium where every single stakeholder has an incentive to change it, but none of us can do it by ourselves.

I want to give you tools that work even in a difficult equilibrium like we're in right now. And that's what I've been really wrestling with. So, the five principles that follow are scalable. They work both. You can see elements of them now and they have teeth that let you get into a better equilibrium if we can all work together as a tech ecosystem.

So principle number one, process over outcome. Outcomes are more easily fakeable now. LLMs, as I've been saying, they generated code, they generate writeups, they generate demos. Process patterns are closer to that verification world. Process patterns are hard to fake. We look for them in interviews. The iteration cycles you took to get something done, where you got stuck, how you debugged some Vibe code, what you would do differently.

Effective LLM use, effective LLM building, effective LLM writing has a shape. You can iterate, you can backtrack, you can override, but it's much much easier to distinguish the shape of good LLM co-work versus blind acceptance. And so I think that one of the things that we should start thinking about is making our process the product when it comes to the talent marketplace.

This has concrete implications for your portfolio. If you're looking at your portfolio as an outcome, maybe you want to look at it as a process or a story that you're telling where you include the debugging and the getting stuck and what you do differently. The most effective portfolio site I have ever seen told a full three-year story of a product.

every stage along the way was honest about mistakes, showed failed designs. It was absolutely compelling. The process matters more than the outcome. And you can't fake the process the way you can fake the outcome in the age of AI. It's number one. Number two, we need to make verification easier, not make signals better. Companies don't need better candidates.

Actually, most of them have all the candidates they need sitting in the applicant pool, as the applicants will tell you. it's that they can't tell who's real. So, stop optimizing for better resumes and shinier portfolios in that world because the companies won't be able to tell. Instead, start optimizing for things that are more verifiable.

How can you show work trials where you solved a real problem? How can you and by the way, as a hiring manager, you should be looking at work trials. That is actually a good way to get a sense of how people work in this world and it gives them something they can show. What about live problem-solving videos where you get on with a candidate and you solve a problem together.

That's a great way to sort of make this work as well. And if you're a candidate, you don't have to wait. You can live solve a meaningful problem. And I've seen people do it in videos where they get on and they say, "You know what? I took a look at your onboarding funnel. These are the three things I think I'd change. This is why.

This is how I'd change it. This is how I'd test, etc." You can just start to problem solve. And again, you're showing that process and you're making it you're sort of surfacing verification because one of the things I will tell you on the talent side, companies want to do this, but they by and large don't know how and they are stuck in the existing default circumstance.

The goal of this video is to shake up the status quo a little bit and get people thinking differently because I think that both sides need to think differently to shake this equilibrium loose. Ultimately, the winner in a system like this isn't the one that yells the loudest. It is the one who makes hiring decisions the easiest.

And if I could tell talent one thing, if you're looking for a role, make the hiring decision the easiest thing. That is actually the mindset to be in more than the noise. Principle number three, we can start to use LLM to generate verification, not just to generate text. Now, this starts to get creative, maybe a touch speculative. There might be a product idea here, but I think there's something for both the talent side of the ball and also for the hiring side of the ball here.

The point is this. We are mostly using LLM as noise generators in the talent marketplace. We shouldn't be LLMs are actually really effective judge judges of other people's work. They're effective evaluators. They're effective researchers. They're creative thinkers and they're verifiers. In other words, these are machines that compute with words and we are just using them to produce lots and lots of cheap text instead of thinking more creatively.

What could we do with this capability? As an example, a cryptographically signed LLM conversation shows your prompt quality and your iteration pattern. Now, you may not be able to cryptographically sign it because I'm not sure I know of a startup that does that, but you can still right now show your prompt quality and iteration pattern.

Again, we're going back to that process piece, aren't we? LLM generated adaptive assessment finds your competent ceiling efficiently. What that means is you can actually get the LLM to progressively test you and ask you harder and harder and harder questions. I wrote an AI fluency assessment just a few days ago on the Substack and it had some of that built into it, but you can go farther.

You can actually design an LLM competence assessment that asks harder and harder and harder questions as you go to eventually find where you top out. And I think that that's actually useful not just for hiring managers to find signal. It's also useful again on the process side for talent to show what what you're capable of, right? Like if you can go through and you can take the hardest, most gnarly product management questions that an LLM can throw at you and answer them in a highquality way after going through 15 easy, medium, increasingly difficult

ones, that says something, especially if you can see the whole process. If you can see that you're not gaming the system. So I think that we are overdue for using LLMs to create signal where there just hasn't been any signal whatsoever. Right? It's like we're pouring all of this energy for AI into making noise in a crowded, noisy marketplace, but there are quiet spaces where nobody's talking at all.

Why aren't we using AIs a little bit more creatively beyond just generating resumes, right? Beyond just generating cover letters. All right, principle number four, bilateral value creation. You want to help companies to verify themselves. I know this sounds funny if you're talent like why do the companies need the help? But trust me, most companies do not know what they really need. They don't.

They're posting LLM generated job descriptions for fuzzy roles and they need help to clarify in most cases. You can interview them about the problem space, right? You can write analyses of their challenges. You can offer trials that validate their needs. I know people who are doing this and are sort of taking command of the job process because the company is trying to figure out the answer and it feels really good for them when a talented candidate comes along and says, "Let me help you get clarity on this role.

This is what you actually need." If you want a cheat code for more senior interviews, a lot of your senior interviews for director and up roles look like that because they're all custommade. And so you end up in a place where you are helping the company to figure out for both of you what the company really needs in the role and then secondarily whether you're a fit.

In that situation you're not just proving your capability. You're helping them understand what capability they are looking for. That is the kind of value that an AI resume can't give. That is the kind of value that reminds them that you produce value that can't be gotten from Clo or Chat GPT.

Right? like it's something that is essential in the humanto human connection of work, which by the way, lest we forget, is the whole point of all of this. Principle number five, you need to be looking at capability spaces more than job titles. I saved the best one for last. An AIPM means different things at different companies. We all know that, but we lack a vocabulary for the next level.

So, what I want to encourage you to do is to think about it this way. Job titles are often noise at this stage because the roles are evolving so quickly and it's part of what makes the talent marketplace so noisy. So instead of looking at all AIPM roles, position yourself across capability spaces, look at technical communication.

Maybe that's a strength for you. Look at system design under situations of uncertainty. Look at LLM evaluation. Is that a skill that you have? Look at rapid prototyping. Build a project that works across multiple capabilities. show your process, which is one of the things I've been calling out, match on problem types that they need to have solved.

And so, one of the things that I think is actually really slept on is that we have semantic search available now that will allow you to match on much more than just keywords. And yet, our entire job ecosystem still runs like the on keywords. Why is that? Why can't we have a job semantic search that matches not on keywords but on the capabilities? This it's not that hard.

And there you can actually build one yourself. If you wanted to do a project where you built a rag and you could build out a listing of jobs in a particular job family and you could semantically search to see where the correct role targets are. All of the tech is on the table. That is basically a weekend project.

At this point, you can transcend the title matching game entirely with with work like that. And the larger point, whether you want to build a rag for your personal job search or whether I'm inspiring someone to do that, because I bet I am, the larger point is this. Think in capability spaces. Think in terms of what are the capability sets you can show.

How can you lay out that process really transparently? And then and then you can get into a space where you can start to show what you know in a way that's provable. And that gets all the way back to verification. The larger point is this. As more LLMs create more noise, as the crowd runs to have LLMs generate resume after resume, generate AI answer for interviews after AI answer for interviews, verification is only going to become more valuable, not less valuable.

The tactics I'm laying out here are designed to have increased returns. The more the market breaks, the bigger your advantage for making vetting easier because that is the core problem companies are facing. I don't want to give you principles here that require everyone who is listening to this to yell louder and compete with each other.

Instead, I want to give you things that let you zigg when the market is zagging. And right now, the market is zagging hard toward yelling in a noisy marketplace with AI. So, let's find some creative alternatives, shall we? you are building with these kinds of moves toward a new equilibrium while everyone else is clinging to the old one and that gap is going to widen with time.

The LLM noise crisis is not going away. I said at the top this is a permanently broken system. It's broken permanently not because of anybody's bad intent but because LLMs have permanently reset the cost of this kind of information to zero. So this is not really advice for navigating a broken system. It is positioning you for the future system that will replace it.

And it's setting you up to work well even now in a system that is not quite ready to reach that new equilibrium. It is a principle for bridging. How can we succeed now and zigg while the market is zagging and how can we build toward a better equilibrium. The bottom line is this. Information has become free in the last two years.

Verification has become priceless. The winner makes verification. Think about that. Good luck in your job search. Good luck hiring. is hard to hire to.

---

<!-- VIDEO_ID: l8m9eb1Y-Ao -->

Date: 2025-11-27

## Core Thesis
Effective discourse about AI, particularly with skeptics, requires moving beyond factual debates to empathetically identify and validate the underlying moral intuitions and fears of others. By reframing AI's role through specific mental models that emphasize augmentation, abundance, and its rapid, prototype-phase evolution, one can foster curiosity rather than confrontation.

## Key Concepts
*   **Moral Foundations Framework:** Arguments about AI often stem from deep moral intuitions (e.g., fairness, purity, care) rather than purely rational disagreements. Productive dialogue begins by validating these underlying values.
*   **AI as "Steroids in Sports" Analogy:** A mental model suggesting AI's role in competitive environments might be akin to performance-enhancing drugs  potentially banned for direct competition but valuable for training and augmentation.
*   **AI as "Camera" vs. Human Art as "Oil Painting" Analogy:** A framework for distinguishing AI's capacity for instant generation from the enduring value of human intent, struggle, and authenticity in creative endeavors.
*   **Augmentation vs. Automation Framework:** A critical distinction for reframing AI fears; AI is more effectively viewed as an "Iron Man suit" (enhancing human capabilities) than a "robot doctor" (full human replacement).
*   **Scarcity vs. Abundance Framework:** Challenges common fears by reframing resource consumption (AI as an industry, not a household; water usage often exaggerated) and highlighting AI's potential to make scarce resources (like personalized tutoring) abundant.
*   **Beta Tester Framework:** A counter-intuitive insight that current AI limitations (glitches, hallucinations) should be viewed as temporary "prototype phase" issues, akin to early internet dial-up, rather than permanent characteristics, given its rapid evolutionary pace.
*   **The "Curiosity Nudge" Strategy:** A pragmatic communication goal to foster open-mindedness and curiosity about AI, rather than attempting to convert skeptics or win arguments, by first validating their concerns.
*   **Media's Role in AI Misinformation:** A counter-intuitive finding that much public fear and misunderstanding of AI is propagated by journalists who themselves lack deep understanding and are fearful, leading to inflated or unhelpfully framed narratives.

---
It's Thanksgiving in the US and this is a video all about how to talk about AI around the Thanksgiving table. I'm going to assume that like many Americans, you have people with a wide range of opinions on AI sitting down to have turkey together. Some of them may be very pro AI. Many of them may be suspicious of AI or not happy that AI is coming into the world or even actively hostile.
What do you do? How do you have a positive conversation and not just say, "Let's pass the stuffing and change the subject." I wanted to go beyond just giving you suggested answers to common questions. Look, I know the questions cuz I get them, too. There are questions about water usage, about electricity usage, about cheating in schools, lots of other things.
I wanted to actually go underneath, look at the psychology of persuasion and debate and think about why these questions are surfacing. So, we're going to reframe some of these key questions around a larger set of frameworks that help us to have productive discussions regardless of which question comes up. And if that sounds too abstract, it won't for long.
We're going to start with a smarter framework called the moral foundations framework, which sounds fancy, but I'm not going into professor mode here. All this is is a theory that suggests that we argue about policies, but we're actually defending our deep moral intuitions. That's pretty intuitive. And so instead of defending technology when someone makes comments about AI, identify the moral foundation they're protecting and validate that first.
So, if you catch someone telling you, "I think that kids are going to use Gemini 3 to fake their handwriting and cheat in schools." They're saying they value fairness. You can say, "I agree. I want a world where hard work matters, where kids learn in school, actual stuff that they have to work on with their brains.
And I think we need to treat AI maybe like steroids in sports. It would be banned in competition, but maybe useful for training in specific ways." Let's think about how we can draw that line. That's a sample answer. You're going to have to decide exactly how you want to answer it.
But my point is, if you think about why they're asking that, you're going to get farther around the Thanksgiving table. Let me give you another example. Let's say it's about fake art or fake writing. You know, it's like, this art isn't real art. I don't understand why people are so into mid Journey. Why can't they pay real artists? The value underneath that is purity, authenticity.
I feel that, too, right? There's something sacred about the human wrestling and struggling to make art. The books that are written on my shelves were not written by Chat GPT. And so I look at AI less as an artist and more as a camera. A camera can capture an image instantly, but we still value a painting because of the human intent behind it.
So AI might become a cheap photo, but human writing is still going to be valued like an oil painting. Again, you can decide how you want to tackle that, but my sense is if you let people know that they're valued and the things they care about are something you can agree on, you're going to get farther. Let me give you one more example here in this sort of framework of validating values.
Let's say they fear job loss, right? That may be a care and protection value coming through. And you can say like, look, I I get it. It's scary to think about people getting left behind while the rich get richer, but I want to make sure that AI is actually out there in such a way that everyone can take advantage of it precisely so everyone can leverage it for their careers.
It's not a perfect answer, but the point here is not perfect answers. And if you're having an honest conversation around the table, it opens the door to discussion. We're all in truth deciding on the future of AI together. That's part of why I have this show is I I talk about where we're going and we learn together as we go. In the same way, we can learn together around the Thanksgiving table how to talk.
Let me give you another framework. Maybe this one will help instead. Maybe the values one isn't for you. Augmentation versus automation. So, a lot of fear comes from the idea that AI is a onetoone replacement for a human. If that is at the root of what's going on, there's definitely ways that you can talk about it.
So, let's give you a specific example. Let's say they're worried about a robot doctor. The robot doctor is a classic example of of automation instead of augmentation, right? And you can say, "Look, I I don't want a robot doctor either, but I do want my human doctor to make the best diagnosis possible, and I know that AI can help with that.
I know that AI can actually give suggestions that are useful. We've seen those studies. And so I think of AI more as an Iron Man suit and less as a full automation solution. See how that sort of reframes from automation to augmentation. Another one, it might be a scarcity versus abundance conversation.
A lot of objections are implicitly based in this idea of scarcity. There's not enough to go around. Let's say maybe electricity and water. So, one way you could answer that if someone asks about electricity or water is to say, well, you know, to be honest, AI should be treated like any other industry and perhaps graded harder because of the investment that is being made in it.
It shouldn't get a free pass. But when I looked at it, water, we lose more water, like a lot more water just from the internal dripping of faucets and leaking inside our homes in American houses than we do to AI data centers. Water at golf courses is a much much bigger issue than water in data centers.
And yes, electricity is a major issue and we need to make sure that we plan for it. But that would be true for any industry. It just happens to be true for AI as well. And so I think the most productive thing to do is to hold AI to the standard we would hold any other industry and to stop maybe comparing AI to an individual home's consumption of anything.
Because it turns out that when you think of AI as what it is, an industry, you get a sense of whether the abundance and the value that it provides is worth the cost. And it's difficult to do that if you're just trying to compare it to like one household's electricity consumption and saying it's x thousand households or something.
That's not super practical. Another one that might get it scarcity versus abundance, tutoring, education. Right now, tutoring is really scarce. Only rich kids get tutoring. But if AI works, we could work to a move to a world where personal attention and education isn't scarce anymore. It's abundant. Anyone can have it. And every kid would get a tutor.
And that's messy right now. It's not perfect, but that's the kind of world that we're on the threshold of, and that's what makes me excited. I'll give you one more framework, the beta tester framework. Relatives will often judge AI by the fact that it's currently glitchy or it hallucinates. It's sort of important to remind them that we are in the clunky prototype phase right now.
So, judging AI by where chat GPT was last year is like judging the internet by a dialup modem in 1994. It's annoying. It's slow. It cuts out when you need it. But we're not talking about keeping this version forever. In fact, we're evolving the version all the time and that most of the people who talk about it this way haven't tried the latest model.
They haven't tried Gemini 3. They may not have tried Nano Banana Pro. And so some of the claims that they make based on the idea that it's always going to be this way, that it's just always going to be stuck in beta. And one of the things we can share is that it's it's changing really fast, right? Like it's growing really fast.
If this video has seemed a little bit scattered, I'm going to tell you something. I did that on purpose because Thanksgiving conversations are a little bit scattered, too. They tend to bat around, bat back and forth. I want you to be equipped with a mental backpack, a sense of how to reframe, how to have talk tracks around AI that are more productive.
And the goal, at least my personal goal, I don't need to win converts for AI. I don't need to convince anybody. I don't need to say AI is the best thing since sliced bread. That's fine. I don't I don't need any of that. But I would love it if someone opened up their mind and was willing to approach AI with curiosity. That's it. That's all I'm looking for.
And so when I sit down and I talk and I have relatives who are have feelings about AI, too. I know that might be hard to believe, but it's true. That's my goal. My goal is to have a conversation, use some of these framework approaches to validate what they're looking at and what they're concerned about and then maybe nudge toward curiosity as well as educating with some facts because the truth is if you follow AI, you know AI has progressed really really rapidly since last year.
The things that were true even 6 months ago aren't true now. If you follow AI, you know that the claims around water and electricity are often extremely inflated by the media and not helpfully framed. And I did a little bit of a job framing that for you. And that's similar to the jobs thing. And I think that one of the things that we can be honest about is that the storytelling around AI has been largely by people who don't know AI.
Most of the journalists, and in fact, some of them tell me this privately, they don't understand AI and they're fearful of it. that's not really the right person to tell the public what AI is. And so, if you're trying to figure out how to tell the story around the Thanksgiving table or at least answer with the mashed potatoes, I hope this video is helpful.
If you want to send them to my YouTube channel, you can send them to my YouTube channel. I'm happy to sort of be the voice in the room if that's helpful for you. But really, maybe just pull out Gemini and have some fun. Pull out Chad GPT, have some fun. Pull out Claude, have some fun. Find something that makes the family gathering feel more traditional, more like it should, more authentic.
And if AI can reinforce that, great. Maybe AI can help you with the family trivia game this year. Maybe AI can help you with the thank you cards. Maybe AI can help you with the menu planning. There's a lot of different ways to do it, but don't let don't let those demonstrations feel out of character for Thanksgiving.
do the work to make them feel like they're just part of the holiday because I think that we're at our best in these situations when we just let AI be what it is, which is intelligence that's pretty cool and it has some use. So there you go. That's my two cents. That's how I'm kind of thinking about approaching Thanksgiving.
If you have difficult relatives, difficult neighbors who are really struggling with AI, they probably have good reasons for that. And I think that that's one of the things that is hard to remember. And that's one of the things I wanted to call out with these frameworks is the frameworks are all based on the assumption that these people have really good reasons for thinking the way they do and we should respect those first before we jump in and just talk about what we want to talk about with AI.
So there you go. Listen, pass the mashed potatoes. See if you can get them to be curious about AI just just a little bit. Cheers.

---

<!-- VIDEO_ID: MYK0d5ikeZw -->

Date: 2025-09-17

## Core Thesis
The advent of AI is rapidly shifting the professional and product landscape from a "world of averages" to a "power law world," where AI acts as a potent amplifier, disproportionately rewarding top-tier performance and exacerbating existing skill disparities. Navigating this non-linear reality requires a strategic embrace of focused obsession and the deliberate leveraging of AI as an accelerator, rather than relying on traditional, average-centric approaches.

## Key Concepts
*   **The Power Law World vs. World of Averages:** AI is fundamentally changing the underlying statistical distribution governing success in talent, products, and careers from a normal distribution (where averages matter) to a power law distribution (where the top few receive disproportionate rewards).
*   **AI as a Disparity Amplifier:** AI doesn't just improve performance; it significantly magnifies even slight differences in skill (e.g., prompting ability), leading to exponential gains for those who leverage it effectively.
*   **Cognitive Mismatch:** Human brains are inherently ill-equipped to process and plan for the non-linear, exponential pace of change and disproportionate outcomes characteristic of a power law world, leading to underestimation of its impact.
*   **"Time on Station" & Singular Obsession:** Achieving disproportionate rewards in a power law world is less about inherent genius or luck and more about sustained, focused obsession and dedicated time investment in a specific domain or "power curve."
*   **Non-Linear Reward Scaling:** Even incremental improvements in performance (e.g., moving from 40th to 70th percentile) yield significantly more than linear gains, as rewards become exponentially greater closer to the peak of the power curve.
*   **Fractal Nature of Power Laws:** The power law dynamic applies across all scales  individuals, teams, products, and companies  meaning the strategic implications are universal.
*   **The Future is Steeper:** The power law effect is not static; AI's continuous advancement means the competitive landscape will become even more skewed and challenging year over year, necessitating proactive adaptation.

---

The Power Law of AI: Why Averages Don't Matter Anymore - YouTube
https://www.youtube.com/watch?v=MYK0d5ikeZw

Transcript:
One of the greatest truths of the AI age is that we live in a power law world and most of us don't realize it yet. I'm going to explain what that means. If you are living in any kind of traditional working world, your entire working universe is defined by averages. You are defined relative to your promotability by the 50th percentile performance for your particular job role.
Your software competes in the market place traditionally based on whether or not it's better than average and is a better fit for the customer need than an average or replacement level fit. That is literally what the entire concept of buying committees in B2B software is all about. It's finding the fit that is best given a narrow distribution of use cases.
We don't live in that world anymore. We don't we don't live in it for talent. We don't live in it for building. We don't live in it for distribution. We don't live in it for marketing. We don't live in it for business. And increasingly in our personal lives, we don't live in it. But we don't really think it through. We aren't used to it.
Our brains are not processing nonlinearity. Well, what do I mean by that? I mean that people come up on stage in San Francisco and they say we're going to have a country of geniuses that lives in a data center by late 2027. Most of us have no idea what that means. And if we do have a sense of what that means, we don't know how it changes our world.
And we can't compute the idea that the progress or pace of change might be that great between now and the end of next year or the end of two years from now. It's like 16 months from now. No way. We can't. No, that's not happen. What if it does? Regardless of whether it happens in 2027 or 2028 or 2030, the point is not exactly when the date is.
The point is that we live in an exponential world. We live in a power law world. Let me give you some examples of what that looks like. In a power law world, the top 1% of performers in a job family will get disproportionate rewards. Ridiculously disproportionate. Like Mark Zuckerberg calls you up and offers you a hund00 million rewards.
By the way, that really happened. That's actually happening now, right? Like you can see in the world that we live in today, job families are not getting rewarded according to any kind of normal distribution, according to any kind of law of averages. This is an power law world where the top 1% are reaping disproportionate rewards.
I'm not talking about Wall Street here. I'm talking about talent performers in the workplace, product managers, engineers, even executives who can lead technical teams. They are getting rewarded based on extraordinary ability. People who are sort of the new average is 90th percentile, right? People who are below 90th percentile roughly speaking end up feeling below average because of the disproportionate rewards that acrew to the top 10%.
This is true for builders, too. If you're an individual who is building a project, I hear this all the time. I guarantee you someone is building your project and I don't care. I don't care. The world is a big place. There's like 8 billion of us. Someone is building your special project. I don't care. I care if your project actually works, if I can use it, and if it solves a problem for me.
And if it does, and if you can communicate that in a way that I can understand, well, you have a chance in the marketplace. There's a power law world for product distribution, too. If you are in the 1% of product distribution as a founder or a builder, if you're Peter Levelvels, right, that very famous solo founder and vibe coder, he can vibe code something over a weekend and he can immediately make a lot of money on it.
And it's not that he's cheating at the game. It's that the game reinforces top performers. It's that the game is wired so that AI reinforces tiny disparities in skill sets and exaggerates them. If you prompt just a little bit better than the person sitting next to you, you are going to get significantly more work done because you actually have AI as an accelerator.
And accelerating AI in a slightly more correct direction makes a big big difference. And that difference is growing over time because the models that are powering you are getting better all the time. We are stacking power curves here. People don't think about it that way. I promise you. They don't think about their products that way.
They don't think about the idea that there will be disproportionate value acrewing to a product that leverages AI correctly, that is distributed correctly, that has the right product evolution for where the market is going and where the market's needs are with AI, and that those kinds of products have the chance to be overnight breakout successes in a way that we haven't seen before.
If you want to know why Lovable is the fastest company to reach a hundred, that's why. They figured out the intersection between traditional software and AI in a use case that unlocked a whole new generation of builders. And tada, $100 million later, in just a few months, they're off to the races. The world is a power law world, and AI is making it more and more and more of a power law steep curve.
We are not going back to the old world. We are not going back to the world of averages. If I can give one piece of career advice to you, it is believe that you are in a power law world and act accordingly. Plan your strategy accordingly. And by the way, if you think I'm not in the top 1%, this is so disempowering, you have to realize in a power law world, moving up gives you disproportionate rewards wherever you are on the curve.
And so if you move from a 40th percentile performer to a 70th percentile performer, you get much more than 30 percentage points in gain there. You're actually moving steeper up the curve. And ditto if you move from 90 to 95%. Do the rewards get greater as you get closer and closer to the peak of your profession? Yes.
Is it also true, this is not always popular, but it's true. It is also true that you get rewards for just sticking with a subject and being persistent with it. People think that moving to the top of the skill chart is something that requires tremendous luck and living in San Francisco and everything else. Look, there's some luck.
There's some San Francisco connections, but a lot of it is time on station. It is time focused and obsessed over your subject that you care about, that you want to become one of the best in the world at. and just caring about that. There are plenty of people who work at Anthropic, who work at OpenAI, who will tell you they did not go to school for this. They obsessed over it.
And so, it is actually easier than you think to move along this curve. It is a function of your willingness to put time and passion in. And as you put time and passion in, you are going to discover connections. Those connections are going to start to become networks. They're going to give you options as far as where you want to be in the world.
and you are going to get chances to connect with companies and products that you didn't think were possible. But it's a function of singular obsession with a particular power curve that you want to drive. So do you want to ride the product management power curve? Do you want to ride the engineering power curve? And those are big broad ones like you can get very fine grained.
And I actually think just like I would recommend a startup focus on a specific problem. Focus on a specific power curve you want to drive in your career as a builder. If you're trying to find a product niche, it's the same thing. You are basically looking for a niche in the world where you can compete to be in the top 1%.
It is it is like a universal rule. It is how the world works now. And AI is exacerbating that. AI is pushing that forward. AI is driving it faster. It will be more of a power law world next year than it was this year. So if you think it's hard now, well, one, you can control that to some extent, and two, it'll be harder next year.
So maybe start to take it seriously this year. Does that make sense? Power laws are here to stay. You can't control that. AI is here to stay. That's not going anywhere. But you can decide where on that power curve you want to be to a much greater extent than you might realize. And that is a fractal insight. It's an insight for companies.
It's an insight for individuals. It's an insight for teams. It's an insight for products. Everything is running on a power curve. And if we have in our heads that we live in a world of averages, we're not going to be competing. and we're going to end up way way way below where we should be in that power law world.
We're going to be in the bottom 40% or whatever and just not achieve the results we want. And if you think about the big headlines that happen about why AI didn't get adopted by this company or or why this strategy failed with AI, so often if you dig the mindset and the culture of the organization that was doing this change or the even if you're sitting around, you're getting a drink with someone and they're frustrated because of sort of the way AI is changing their job family.
If if you peel that too, if you if you dig into that insight, what you see is that people aren't getting to grips with the fundamental exponential nature of the problem space now. And if they did, they would think about the problem space differently. Now, I'm not here to tell you that if you're talking with your friend over drinks and they're two drinks in and they're complaining about their job and you say, "It's a power law world.
Nate said so," they're not going to throw a beer in your face. they probably will, which would be justified, but it's still true and there are probably gentler ways to put it. You know, you can say something like, "Hey, I know that we don't have guarantees now. I know the world has changed, but let's take a minute and let's think through what our options are now that we understand how power laws work.
" And they might still throw a beer in your face, but it's still true. So, power laws are a thing. There is insight here for companies. There's insight here for teams. There's insight here for individuals. if you're willing to hear it. It is one of the things that I find people often do throw metaphorical beers at me for, but I gotta be honest with you, that is what is actually happening.
And so, if you want to drive your career, your product, your team, whatever, that's what you need to do. You need to look at the world as a power law world and you need to look at investing in specific curves and writing those curves. Tears.

---

<!-- VIDEO_ID: N8ddmMBJrzo -->

Date: 2025-10-01

## Core Thesis
The speaker critically argues that mainstream AI education, exemplified by OpenAI's generic prompt packs, fosters a dangerous "messy middle" of superficial AI adoption. True proficiency and competitive advantage in the exponentially evolving AI landscape demand a workflow-centric approach, deep engagement with underlying principles, and continuous learning, treating AI as a transformative general-purpose technology rather than a simple, one-and-done software tool.

## Key Concepts
*   **The "Messy Middle" of AI Adoption:** A critical mental model describing the state where individuals and organizations engage with AI superficially (e.g., using generic prompts), leading to a significant competitive disadvantage as AI capabilities rapidly advance.
*   **AI as a "Moving Train" on an "Exponential Curve":** A powerful analogy highlighting the dynamic, accelerating nature of AI development, necessitating continuous, deep learning and adaptation to avoid being "left behind."
*   **Workflow-First AI Integration:** A pragmatic engineering framework advocating for identifying specific organizational pain points and integrating AI solutions directly into existing workflows, thereby making AI tangible and genuinely useful, rather than relying on generic, decontextualized prompts.
*   **Prompting as a Distinct Skillset:** A counter-intuitive finding that effective AI prompting requires a fundamentally different cognitive approach than traditional search engine queries, demanding a deep understanding of context, goals, and underlying principles for optimal results.
*   **Model Agnosticism (Use Case over Model Hype):** The insight that the effectiveness of AI lies more in *how* it's applied to specific workflows and problems than in the specific model used, challenging the common focus on "best" or "latest" models.
*   **The "People Gap" in AI Adoption:** A critical analysis revealing that the primary barrier to effective AI utilization is not technological limitation but human factors: a widespread unwillingness to train deeply and the misconception that minimal training is sufficient.
*   **AI as a General Purpose Technology:** A foundational mental model that frames AI as a transformative technology akin to electricity or the internet, requiring a fundamentally different adoption and learning strategy than typical software.

---

ChatGpt launched an absolutely terrible resource for prompting and I think it deserves more attention because we need to talk about how bad AI education is today and how much is dependent on getting it right. And Chad GPT is a leader in the space. They're seen as an influencer as the first mover. People will look to things like the Chat GPT prompt pack that just got released and say this is something we need to give to all of our teams.
They're terrible prompts, guys. They're like one or twoline prompts that are extremely generic. In fact, I'm going to go ahead and read you one for their most technical team, engineers. Let's say your engineers are asked to come up with GDPR compliance uh responses from a technical perspective. How should we advance GDPR compliance? You might think that you need a fairly complex prompt for that.
It should take account of your data schema. It should look at the countries that you have a footprint in. It should look at data processing, also where data is stored, what your existing stack looks like. None of that. None of that comes out in this prompt. Research best practices for GDPR CCPA compliance. Not even one. It mashes them together.
So we can help kick off our discussions with our legal team. When has engineering kicked off discussions with legal ever? Context. Our app stores sensitive user data in the EU and US. Output a compliance checklist with citations sorted by regulation. Include links to documentation and regulations. No, that's what Google is for.
That is not what intelligence is for. If you're building intelligence that's too cheap to meter, teach us how to use it. Be useful with it. And this worries me because one of the looming fears I have for 2026 is that we are going to get a generation of builders of workers of knowledge workers trapped in the messy middle of AI adoption.
And resources like this encourage that kind of behavior. They encourage the assumption that we only need to pretend that this is regular software we have to adopt. I can go get the prompt pack from OpenAI. I can roll it out as a manager to my sales team or my engineering team or my product team and I'm done and we can move on and it's just it's a oneanddone thing. AI is on an exponential curve.
This is a case of getting onto a moving train. You are either going to lean all the way in and you are going to learn fast and you are going to scale up quickly in your skills and keep leaning in or you're going to get left behind. And if you learn two or three lines in a prompt and you think you've got it, you're in the left behind contingent.
You're going to be surprised when people come along and say, "I one-shotted an entire financial analysis off a screenshot." And here it is in Excel, which by the way, real example, I did that with Sonnet 4.5 last night. Very helpful. I actually tried it with Chat GPT5 as well. Chat GBT5 did not do as good a job, which I thought was really interesting because it's usually very good at image analysis.
But that being said, that's an example of the kind of thing that I tried. I learned something new about image capabilities that wasn't really published very well from Claude and now I know more and now I'm sharing it. There are hundreds of those examples. Part of why I make this channel is so that it is easier to keep up. It is easier to understand.
Part of why I write the posts I do on Substack is so it's easier to find. My response, by the way, to the Open AI choice to release effectively a gigantic packet of lousy prompts. By the way, not just me saying that, Reddit also has been ripping it apart. And I know we don't always like Reddit, but they they have rightly been ripping this prompt packed apart as completely useful for people who are serious about AI.
I am making a prompt pack in response that is actually useful. I'm going to put on Substack. And so if you want something by job family, I'm putting it together. I just this is it's really bad. You can't assume that all you need is basic ability to ask questions of AI. If that were true, one, Chad GPT5 would be easier to prompt, which it is not, and two, you would assume that people would be able to transfer their existing Google skills to AI seamlessly, which it's actually a very different skill set because people have been asking
questions of Google for a very long time. That's not really a new thing. I'm concerned. I'm concerned that our assumptions about what is needed for AI education do not match the pace of development. If I were designing a curriculum for teams, and I get asked this, so I'm going to share right here with you what I would say.
If I were asked to design an upskilling curriculum for teams, I would start by working through use cases with them. Where are the pain points in the team's existing workflow? Engineers, product managers, sales, whatever it is. Where are the pain points where we see lots and lots of manual cycles and not a lot of results? Like you just grind on it.
Great. Thank you. that is a candidate for talking about AI. And then we start to ground the whole day, the whole time we have together in actually talking through how AI can unlock that for you. And that makes it tangible. It immediately goes from the silly two or threeline prompts like I've been tearing apart here into something that is useful for your use case.
Maybe your use case is that your team struggles to get classic strong bulleted technical requirements out of the documents product gives you. Great. That's one we could work on with AI. Maybe your team struggles with getting accurate sales pipeline predictions. Well, thanks to tool use with LLMs, you can start to get that, too.
Maybe you're struggling with the just the pace of the interview pipeline as you try to bring people on board. You can get note-taking. You can get some standardized forms to review. You can get standardized question sets. There's a lot you can do with AI to lift that burden and still put the human at the center of the interview process so you can focus on assessing candidates.
Those are just off the top of my head. Every single department is full of those kinds of opportunities. And the gap is our ability to understand how quickly AI is scaling and how much capability we have on the table. There is meat on the bone here that we are not touching. Most managers have no idea how much AI opportunity there is in their space.
It's like I look at it when I come in and I'm like 80 or 90% of the AI opportunity is untouched. You guys are sitting here talking about you know how Copilot can do this and that or how chat GPT can do this and that. Great. I'm glad you're chatting with Jack GPT. I'm glad you're using Copilot for your emails.
Have you thought in workflows? Have you thought about the impact your team is delivering and worked back from that into your pain points? No. Well, maybe we should start with that and then get into training. And so, yes, when I build prompts, when I think about what teams need, I think about how to build prompts that are going to be supportive of workflows.
So, of course, they are longer and they can be longer because AI can do more. And by the way, if you're listening to this and you're like, "My or uses Copilot, Nate, like Chad GPT, what Claude what?" Well, one, I have news for you. Claude is now in the Office family for Microsoft. It's it's blessed. That is why Sacha Nadella was bragging about having the best Excel model.
He just put Claude in a rapper, right? Like he doesn't have a magical best Excel model that he's been hiding. He put Claude in a rapper. So Claude's going to be there. But two, it is not the AI model that matters. It is the way you use it, which is a very sort of zen thing to say, but it's true. If you have a good idea of what you want to get done with workflows, you can do a ton with Copilot. I wrote a whole guide for that.
You can do a ton with co-pilot to enable your business to actually use AI. It is not just for email. It is a model you can actually employ. Like to sidetrack conversations around my model's terrible or my model isn't as good as like the best thinking models out there. You can still do a lot with it.
We would still be impressed if it was 2022 and that model launched. If co-pilot came out in 2022, everyone would be over the moon. There's a ton you can do with it. The gap is people. The gap is people. one, not being willing to train. And that's part of why Accenture fired 11,000 people is the strong implication was they were not willing to be trained on AI. I don't know if that's true.
It's Accenture's side of the story, but that's what they said. Um, and then two, the gap is people thinking a little bit of training is enough. And that is why I am concerned about what Chad GPT did because they basically said, do you want to get started? A little bit is enough. You can just get started.
Put these two sentences in on GDPR CCPA and you'll be done. You'll be good. And then they did that 200 times. People on Reddit were saying the intern wrote the prompts uh with Chad GPT. And I'm like, "No, I think the intern just wrote it by themselves because Chad GPT would write a better prompt." We owe it to ourselves and people farther in AI, people at ModelMakers owe it to the community to produce better resources.
And I know that we have a gradation of talent and we need on-ramps for everybody to get into AI. Not everybody's going to sit there and listen to Andre Carpathy talk about LLMs and just go, "Wow, this is amazing." Yes, they're stoastic people spirits. It's an illusion to the YC 2025 presentation that he made.
No, like they're not all going to do it. And so, everybody needs to get on at their own pace, but we need to have really clear progression and we need to help people to understand principles that can scale. And so, if you're going to give people simple prompts, maybe that's all right as long as they understand one, this is just the start and you need to do better.
and two, this is how it ties to your workflow and moves things forward. And three, these are the principles that scale with it. Like if they had taken the time to say from their own best practices, if OpenAI had taken the time to say it's really important to establish context for the prompt, having a goal for the prompt is important.
Look how we're doing that even in a simple prompt, right? Like that's helpful. That helps you to internalize these principles. If you don't do that, you're going to be stuck thinking that you understand prompting and AI and you're going to get left behind in 2026. We don't want that. We need better prompt education. We need better AI education.
We need better understanding of where AI opportunities lie in our fields of work so that we retain our curiosity and we learn with AI. And we're just not getting that when we get resources like this. And so I call them like I see them, right? Every model maker has spots they do well on and spots they don't.
In this case, I don't think the new chat GPT prompt pack is moving the ball forward at all. It reads very much like a defensive gesture where they needed people buying chat GPT for enterprise to have a link they could point to to say they offer prompt pack education and then like somebody ticks the box in it and they get the sale. They don't.
That is not what that is. So, I built some prompts, but mostly make sure you understand why you are learning the AI you're learning. Make sure you understand your use cases and make sure you lean in on growing your AI knowledge over time. This is not a typical software adoption story. This is a new general purpose technology and we need to treat it like that if we are going to successfully hang on to the train while it is scaling exponentially. Onet 4.
5 did 30 hours of continuous work and rebuilt Slack. They built their own version of Slack and Sonnet just went and did it and wrote 11 thou th,000 lines of code and it worked. That is what the bar is becoming. I'm not saying any of the dramatic things about that replaces engineers or this and that because if you work in software engineering like you will see the weak spots of AI all over the place, but it's a big big deal.
It is going to change how engineers work. It's going to change how PMs work. It's going to change how product gets built. It's going to change our velocity expectations. And we need to have AI education that keeps that in mind. When we talk about prompting, we need to prompt with that world in mind. And that's why I care so much about this because we deserve better.
So this is my plea. If you were to model maker, please invest in yes AI education for beginners, but really clear on-ramps, really clear scaleups. Help us to be able to teach this well. And in the meantime, I'm doing my best to put content out there everywhere I can think of that is going to be more useful, that is going to be more aligned to where AI is going.
So, if you want the prompts, you know where to get them. In the meantime, have fun, enjoy AI, pick a problem space you care about, and uh yeah, get passionate about

---

<!-- VIDEO_ID: NP-qmffUNHQ -->

Date: 2025-10-29

## Core Thesis
Amazon's recent layoffs are not a direct outcome of AI-driven job automation, but rather a strategic financial maneuver to reallocate capital from operational expenses (salaries) to massive capital expenditures (Nvidia GPUs). This aggressive investment is crucial for AWS to secure its competitive position in the foundational AI cloud market, directly challenging simplistic media narratives about AI's immediate impact on employment and the notion of an AI bubble.

## Key Concepts
*   **Strategic Capital Reallocation for AI Infrastructure:** Major tech companies are undertaking significant financial restructuring, cutting operational expenses (e.g., salaries) to fund massive capital expenditures (e.g., Nvidia GPUs) essential for building foundational AI cloud infrastructure. This is a proactive investment in future AI competitiveness, not a consequence of current AI automation.
*   **The "AI Bubble" Counter-Argument via GPU Demand:** The overwhelming, unmet corporate demand for specialized AI compute hardware (Nvidia GPUs) fundamentally refutes the notion of an "AI bubble." This intense demand signifies real, tangible value and market need for AI capabilities, indicating a foundational shift rather than speculative overvaluation.
*   **Pragmatic AI Adoption vs. Automation Hype:** Enterprise AI integration is often a complex, incremental process, with existing internal workflows described as "duct tape and bailing wire." This pragmatic reality challenges the widespread media narrative of immediate, magical AI automation leading to mass job displacement.
*   **Logical Incoherence of Dual AI Narratives:** The simultaneous media claims of "AI automating all jobs" and "AI is a bubble" are logically contradictory. High demand for AI (implied by automation) would negate a bubble, while a bubble implies a lack of genuine, sustained demand.
*   **AWS's Competitive Imperative:** Declining growth in Amazon's core profit driver, AWS, necessitates aggressive financial strategies to invest heavily in AI infrastructure (GPUs) to maintain market share and competitiveness against rivals like Azure and Google Cloud, who are perceived as leading in AI offerings.

---

Amazon laid off 30,000 people this week in a move that had been widely telegraphed for months. But the real story is not about AI and jobs. The story I'm seeing over and over and over again in the news media is, hey, AI is automating all of these jobs. This is why we're seeing this. We're going to see more of these cuts.

The media seems really excited about that story and they just want to keep telling it. And I got to say it keeps not being in this case. There's a very interesting reason why it's incorrect. And I think it's actually really important for us to understand because it gets at a core narrative that a lot of people have about AI that is wrong.

So what actually happened here? Number one, you need to understand where Amazon's business came from. I spent half a decade at Amazon. I'm very fluent in this. Amazon makes their money on AWS, not on retail. The whole store doesn't do anything, right? that the margins on the store are ridiculously low. They make no money. They would not be a profitable company.

Jeff Bezos would not be wandering around in a yacht. Amazon makes their money on AWS, which means the entire street, all of Wall Street checks AWS's growth numbers every single year obsessively to tell whether or not Amazon is doing well. And the problem is that AWS's growth numbers have been declining for the last few years.

And so they're down to 18% growth year-over-year, which is a deceleration in their last quarterly report. And the street doesn't like it. And part of the reason the street doesn't like it is that in the meantime, AWS's two main rivals, Google Cloud and Microsoft Azure, have been catching up and they have been doing so with AI.

Right? If you think about where to go for AI, you think about Azure and you think about Google Cloud. And I got to be honest, AWS is a distant third. They just aren't there. And so the challenge for AWS is they need to show that they are still serious players in the AI era. To do that, what do you need? You need a specific piece of hardware that Jensen Fuang sells that nobody else has.

It's the Nvidia GPU. I well there's a few others that have GPUs but by and large it's all Jensen's GPUs and you got to buy a bunch of them and they don't come cheap like think of this as a computer chip that is worth a car and you have to buy thousands and thousands and tens and thousands of them to get anywhere with AI especially at scale especially if you're serving corporations that is the dilemma that AWS faces now look at it from a corporate finance perspective you have to do that without damaging your margins in AWS because the whole reason

Amazon is worth anything as a company is because of AWS's margin. So you don't dare damage AWS's margins to get this job done, but you have to buy a whole bunch of GPUs. In finance terms, you have to add a ton to your capital expenditures, your capex. Well, if you're going to do that and you want to keep your margins consistent, you have to cut other places that are in your expenses category.

You have to look at other fixed expenses. And what is the biggest fixed expense category that you have? It is salaries. It's salaries. That is what they're looking at. And so when you have that tradeoff, what you should really be looking at is Amazon did not fire 30,000 people because AI automation was already taking their jobs.

Amazon fired 30,000 people because they needed the money today in order to buy GPUs to desperately try to secure a place in the future of AI cloud. That is the actual story. Now, that story does not sound as nice for Amazon as a future forward story about how we're automating with AI. So, the story Amazon's putting out is, hey, we're automating with AI, so we don't need these jobs anymore. No.

Especially as someone who worked there, the interior workflows at Amazon, and anyone will tell you this, this is not proprietary. It's all duct tape and bailing wire in there. Like, everyone does a lot of manual stuff. And that's been very intentional as Amazon has grown because it helps us keep costs low for customers.

And so, there is not a huge massive automation magical solution that they have invented that allows them to right now cut 30,000 jobs. It just that's not how it works. The people who remain are very stressed. They may have ambitious projects to eventually bring AI into those areas. But if I'm looking at it at a very high level, what I see is not an investment roadmap for AI automation that is already paid off that they're trying to show.

No, what I see is Amazon saying these are areas where we can afford to take a risk on less talent getting less done. In other words, these are areas that we can divest a little bit. And that's interesting because that makes a whole lot of sense when you look at where some of these cuts happened. As an example, MGM got hit, the Hollywood studio that Amazon bought a few years ago.

I got to say, if you're asking yourself, is this an area where we have already invested our best efforts to automate AI right away? MGM would not be at the top of my list. I do not think that is the most strategic place in Amazon where Amazon would have poured vast resources to invest in AI. No. But I do think it makes a ton of sense as a place where Amazon would say we can invest less for a little bit and cut talent for a bit because we desperately need to reallocate cash over to the GPU side of the business. Well, that makes a lot of

sense, doesn't it? And people aren't reporting that. And here's the reason why all of this matters. The narrative out there is very simple. We are in a bubble. Everywhere I look, we are in a bubble. AI is a bubble. AI is a bubble. But step back, take a breath, don't just obsess over the news.

If we are in a bubble, why does Amazon have a 25% essentially overage rate? Right? The the the amount of demand they have for for GPUs right now vastly exceeds the available GPUs. Would that be true if we were in a bubble? Demand is a sign that we are not in a bubble. surging corporate demand that Azure has trouble meeting because Azure's expanded their data center investments this year that Google Cloud has trouble meeting that Amazon has tons of trouble meeting is a sign that we have built something with AI that is valuable enough that

corporations are lining up like crazy to buy it. If you have customers out the door and cannot serve enough chips to all of them, that is a sign that you are not in a bubble. Ipso facto. By definition, it is a sign that you are not in a bubble. And I don't understand why this is so hard for people. I don't understand why journalists are so interested in pedalling the narrative that AI has already automated jobs and that somehow we are simultaneously in a bubble because both of those things cannot be true at once. If we lived in a

world where AI had magically automated away all jobs or whatever they're claiming, well then it wouldn't be a bubble because people would like corporations would have demand for that, right? They they would ask for that to happen so they could expand their footprint. maybe not even to fire people but to add more like c talent and capability right there'd be demand for it if we were you know in a bubble we would not have the kind of surging interest in AI at every level small medium business uh commercial scale

enterprise scale the consumer like all of us as individuals that's not the story we see and yet we are being asked to believe by the media simultaneously that AI is so big and scary it is automating jobs and also though that somehow we are magically in a bubble and it's all fake. It's it's not both people. It's not both.

And in this case, it's not either one of them because we don't have the talent yet to build AI systems that fully automate roles and we like not for a while. Like it's not there. And yet corporations love that narrative because it makes them future focused. It makes Wall Street happy because Wall Street doesn't know what AI is.

And everybody like goes away happy with the story. And nobody pays attention to the contradictions here. The the answer is very simple. They just need to buy GPUs because corporations need to buy cloud compute. And that is what happened. And none of that makes getting fired easier. I don't want to sort of pretend that being able to explain it makes it better.

It it sucks to be fired. I've been fired before. It just it's terrible. And so for those of you who have unwillingly left Amazon, there are Amazon alumni. We're out here. We look out for each other. We're doing our best. And you will find a spot. And I know that none of this makes it better.

But I do hope that we actually can tell the truth about what is going on instead of getting fooled by stories that are not even not even coherent, right? Like you can't have both an AI bubble and AI automating all jobs. It does not work. And yet that is the story we're being sold. So there you go. That's the truth. That's why it matters.

And that's why we need to pay attention when people make layoff announcements like that. Good luck. Don't for

---

<!-- VIDEO_ID: oqgk6fdjnno -->

Date: 2025-09-30

## Core Thesis
OpenAI's launch of Sora 2 as a friend-centric social network is a sophisticated strategic maneuver, signaling its redefinition as an "intelligence company at scale" rather than solely a model developer. This move pragmatically addresses the monetization imperative for a billion-user base through diversified ad surfaces, while simultaneously shaping public perception of responsible AI use and cultivating a controlled, positive cultural ecosystem to safeguard its brand integrity against the "AI slop" narrative.

## Key Concepts
*   **"Intelligence Company at Scale" Reframe:** OpenAI is strategically repositioning itself from a "chat product" or "model maker" to a broader "intelligence company" focused on delivering delightful, AI-driven experiences to a massive user base, which dictates a natural evolution into social and ad-based monetization.
*   **Strategic Brand Integrity through Product Segmentation:** To counter the "AI slop" narrative and preserve the perceived trustworthiness of its core ChatGPT experience, OpenAI is strategically launching new, separate surfaces (like Sora 2 and Pulse) for ad monetization and social interaction. This allows them to experiment with revenue models and user engagement without "rotting the product experience from within" the flagship AI.
*   **Norm-Setting as a Strategic Win:** OpenAI views the successful demonstration of constructive, responsible AI use (e.g., friend-to-friend creative interaction in Sora 2) as a strategic victory, even if the product itself doesn't achieve massive scale. This prioritizes shaping public perception and setting industry norms over immediate product market fit, addressing the pervasive concern about AI's negative impact on social media.
*   **"Centers of Gravity" for Billion-User Platforms:** A fundamental internet business model framework suggesting that companies reaching a billion-user scale inevitably gravitate towards becoming ad networks and social platforms to monetize attention and relationships. OpenAI's moves are seen as a natural, almost unavoidable, consequence of its scale.
*   **Cultivating Ecosystems via Controlled Seeding:** The invite-only launch of Sora 2 is a deliberate strategy to "seed the ecosystem" with specific creators, aiming to cultivate a positive, safe culture from the outset. This is a proactive measure to build brand reputation and counter the negative cultural dynamics often seen in large social networks.

---

Sora 2. What is it and why is OpenAI launching it as a separate standalone social media app? I'm going to break down the entire strategy and why OpenAI specific product choices tell you about where the company is going. Let's get into it. First, it has taken them a long time to get video and images right.
They did not release this back when they did the special splashy Sora 1 launch that had a very nice page showing you potential Sora videos and then much more mixed results a few months later when Sora 1 actually released. They feel better about this product. It's got longer video images. I think it's up to 16 seconds now.
They're figuring out some of the sound pieces and critically they've put clearly a ton of thought into what kind of social network they want this to be. And yes, let's just say it out loud, it is a social network. Chat GPT is launching a social network as a standalone app. This is a direct shot across the bow at Meta and Zuck. And they mean it.
They intend to go after the social network phenomenon for multiple reasons which I'm going to get into. But first, let's talk through the product itself. This product is intended to be old style Facebook, old style Instagram. It is about you and your friends. And that is what they are aiming for. And the reason why is pretty simple.
They are very very sensitive to the accusation that AI is going to produce slop on the internet. They don't want to have their brand associated with the idea of AI slop. They have seen Meta's terrible PR experience with AI girlfriends and boyfriends with AI chat sort of assistance with AI produced. I think there was a nonprofit scam where AI was like bragging about donating and helping others last year.
They've seen all of those stories. They've seen this broad and pervasive impression that people have that AI is wrecking social media. And they want two things out of that. One, they want to make sure that they are perceived as the good guys. And two, they want to make sure that they can show a path for AI being used responsibly.
And so as much as I think they're excited about Sora launching, I'm sure the team worked really hard on it. From a strategic perspective, even if Sora doesn't succeed, but it shows a way toward a more constructive use of AI, that is still a base hit from their perspective. That is still a win because they can show that AI can be part of the solution, which chat, GPT, and OpenAI desperately need right now.
So that is why they have centered this app around you and your friends. That is why the viral feature of this app is you insert yourself as a cameo into the AI video and show your friends and cast yourselves back and forth. It is designed for fun, encouraging, positive interaction between friends that cannot help but be powered by AI.
You can't put yourself into videos and send them back and forth and have fun unless you're using AI. And so it is also inherently a set of training wheels for people who are going to be using AI to be creative in new ways. In some ways, I think the network that we don't talk about that should be most worried is Snap because Snap is designed for those kinds of ephemeral interactions back and forth between friends.
And it's interesting to see that that is the direction that OpenAI has chosen to go rather than more of the content network like Tik Tok. We will see. It is early days yet. I remember when threads released and then the long fallout from threads. This is day one for Sora 2. We will see where it ends up.
Before we go further though, we've talked about sort of their core friend oriented product strategy. I want to talk about the larger product strategy. Why is OpenAI getting into the social network business at all? It is not just because they need AI to be perceived positively. It is also because they have a billion people and they are a consumer application and one of the inevitable forms of business on the internet for consumer applications is attention to ads.
I've been talking a lot about OpenAI monetizing ads and we have seen them do three things that argue for ads maybe four in the last week. One is starting to hire a head of monetization and ads. That's pretty transparent. Two is launching Pulse for pro users where they actually show you cards. You can see sponsored cards so easily there, guys.
It's like falling over. And then three, launching Sora 2. Again, a super easy platform for ads. Now, the fourth thing, if you sort of want to get into the full purchase funnel, they are working with Etsy and Shopify on checkout within chat GPT. And so, now they would own the whole funnel, right? Well, all of this helps to answer the question for all of us about how they are thinking about ads and how they are thinking about maintaining the perceived integrity of the system when you get answers from chat GPT.
Because one of the persistent concerns I've seen in my DMs that I've also had is how do you handle the integrity of the system if someone can buy an answer on chat GPT that feels like it will rot the product experience from within and it feels like something that chat GPT is going to be self-aware enough to avoid.
This may be how they may be choosing to launch new surfaces off their existing install base in order to provide themselves ad paint spots that don't touch the core chat GPT experience. Like you get an ad in pulse, you get an ad on Sora 2, they didn't touch chat GPT. You still think of Chad GPT as trustworthy, right? I'm not saying that's the whole solution.
They may indeed get to a point where they start to put ads in Chad GPT itself. My bet is they're going to use some of these other surfaces initially because they're much much lower stakes. They're newer. But we come back around to the imperative of monetization. We come back around to the idea that this is a billion user company or soon will be.
This would be the only billion user company that is not an ad network if it didn't do ads. This would be the only billion user network that is not a social network if it didn't do social. It is sort of like centers of gravity on the internet. When you get to a certain scale, you head to ads and you head to a social network. It's just what you do.
It's how you make money. You make money off relationships and attention. And so that is why when people started to write me and say, "Why on earth is chat GPT doing this? I thought they were a chat product." I think we need to rethink what they are. They are not a chat product per se.
They're an intelligence company that is in the business of providing delightful experiences at scale driven by intelligence. So think that one through and suddenly the social network makes sense, pulse makes sense, checkout makes sense. These all make sense. Now is there an enterprise arm? Is there an R&D arm? Absolutely.
And those help with profitability and the P&L of the business. But the beating heart of the company is the billion user base. If you don't keep that healthy, you're not a big brand. And if you're not a big brand, you are indirectly damaging your enterprise and your R&D plays as well. Part of why enterprise has chat GPT on the table is because everyone has heard of it.
So you need that popular support even on the B2B side. So I think that's some of what is going on strategically here. That's why I wasn't surprised when Sora 2 launched. I think they also launched preemptively before the holidays. They wanted the attention. They wanted the buildup to Black Friday, Cyber Monday. They wanted the Q4.
They wanted the tiein to monetization. I don't know what that means for ads this fourth quarter. But I think they at least want the telemetry. They want the metrics of how people interact with it. And they want to start to build against it. They also want to steal a march on Google, Snap, and Meta because all of those will now be looking at how they can start to get into the action on the social side too.
Meta is actively building that way and has been roundly criticized by most people. I described that earlier in this video. Google hasn't launched anything in the social and network space for a very long time and they are trying to maintain attention by leaning into Nano Banana everywhere you can look. Like they have ads up everywhere. They have ads on TV.
They have ads on Tik Tok. They have ads on other channels across the internet. They're leaning in with lovable on this build week. They want you thinking of Nano Banana a lot right now because they don't want you distracted by what Sora can do, right? Like for them, this is a little bit of a defensive play where they need to lean in on the cool thing they have so you don't get distracted by the cool thing OpenAI has.
And we'll see what Snap does. My perception is this is a direct threat to Snap, but it's only a direct threat to Snap if people start to adopt it and use it and find it delightful. And I think that is always the question. That is the magic that comes with social networks. You get a particular ecosystem and culture very very rapidly.
And that is why they are making it invite only is because they want to seed the ecosystem with the kind of creators that will help them to build a culture that will be attractive to build a long-term following and long-term positive experience for people. I suspect they especially want to develop a reputation as a safe and positive place versus some of the brand reputation stuff you see with other major social networks. So we will see.
It is day one for Sora 2. But that is why OpenAI is thinking about that. This is the intelligence company and the intelligence company that has a billion users. They are figuring out how to leverage intelligence every way they can to provide delightful experiences for those users. And that's a very different strategic position from every other sort of model maker in the space.
What do you think? What are they up to with Sora 2? Have you tried Sor 2? Do you have invite codes for Sor 2? I'm sure the people in the in the comments will be very excited about that. Cheers.

---

<!-- VIDEO_ID: O_VL5clgN_I -->

Date: 2025-11-10

## Core Thesis
The speaker argues that as AI makes raw intelligence and analysis abundant and cheap, the critical human skill of *judgment*defined by its context-sensitive, constrained, and accountable applicationbecomes the new, irreplaceable bottleneck and most valuable asset in any organization, challenging the notion that AI will diminish human strategic roles.

## Key Concepts
*   **Judgment as the New Bottleneck:** In an era of abundant, cheap AI-driven intelligence, human judgment (the ability to apply intelligence wisely, contextually, and accountably) becomes the scarce and most valuable resource.
*   **Value Migration to Scarcity:** As one resource (intelligence) becomes abundant, value shifts to the next scarce resource (judgment, context, sequencing, human attention), highlighting where human effort yields the highest returns.
*   **Context-Sensitive Synthesis:** Good judgment is not just pattern recognition but the ability to synthesize patterns with a deep understanding of unique situational context, distinguishing it from generic, overgeneralized AI outputs.
*   **Pragmatic Constraint Thinking:** Effective judgment involves moving beyond paralyzing analysis to identify and act upon what is *possible* to build and execute within current constraints, prioritizing actionable solutions.
*   **Strategic Deprioritization:** A critical, counter-intuitive skill where good judgment is demonstrated by explicitly defining and defending what *not* to do (non-goals), countering AI's inherent tendency to expand scope.
*   **Thin Slicing Value & Sequencing:** Building trust and momentum in AI initiatives by carefully ordering bets and delivering small, verifiable pieces of value (MVPs) to overcome skepticism and earn buy-in.
*   **Accountability and Transparency:** Good judgment is characterized by taking ownership of outcomes (even AI-generated ones) and transparently articulating reasoning, assumptions, and trade-offs, rather than relying on opaque, polished presentations.
*   **Judgment Compounding through Systems:** The highest leverage of individual judgment is achieved when it's encoded into scalable playbooks and automations, transforming personal heroics into sustainable organizational capability.

---

We're all becoming judgment merchants. I know that's a new word. I'm coining it. The reason I'm saying that is that whether you work as an external consultant in AI around AI or whether you're working inside a company and you are building in AI systems, wanting to build an AI systems, leaning in on Chad GPT, everyone is going to have to start practicing judgment.
This is not a taught skill. This is not something that you can just go and say, "Oh, I know judgment now. I'm good at judgment." In fact, for a long time, the ability to exercise good business judgment was the bar for principal product manager, the bar for a senior engineering leader. Now, what I'm suggesting to you is that we need to find ways to cultivate that for every level, for every job family.
Why is that? Because intelligence is becoming too cheap to meter. Sam Alman was saying that intelligence is falling x a year in cost. X a year for the same intelligence level. If that is even remotely close to true, we have to double down on being good judgment merchants. And if that sounds too abstract, it's not going to be by the time you're done with this video.
I'm going to go through 10 principles. As far as I know, no one has really put together a miniourse on how to have good judgment in the age of AI, even though that's an irreplaceable skill and we talk about it a lot. So, let's get into it. What are 10 ways we can show we exercise good judgment? This applies regardless of job family.
And this should be specific enough for you to actually jump in and wrap your head around. That's my goal. Number one, the scarcity principle. So intelligence is abundant, right? If it's coming down at a cost of 40x a year, everybody's going to have intelligence. Value then migrates to the next bottleneck. Basically, one of the ways you show value, whether you are inside a company like Walmart or Netflix or Amazon or a tiny company like a series A or a Seed or whether you're outside as a consultant, everywhere you look on an AI
project, you will see places where intelligence unlocks an enormous amount of volume and you will see places where that volume bottlenecks. Part of your value is finding the bottlenecks. It's the scarcity principle. Find what is still scarce in a world where intelligence is abundant. Get eyeballs, get get binoculars, get a microscope, whatever metaphor you want, but find the scarcity. The scarcity is there.
Maybe it's scarcity of selection and sort of finding what to choose. Maybe it's scarcity of sequencing. It's really hard to know how to sequence. Maybe it's scarcity of implementation. Maybe it's scarcity of human resources in other areas. There is always going to be something scarce. Maybe customer attention is scarce, right? Find the scarcity.
The fastest signal of good judgment is how precisely you can define the true current bottleneck. And that is true whether you're internal or external. Principle number two, context. Judgment is contextsensitive synthesis. In other words, you are reusing patterns with an awareness of what makes this situation unique. Good judgment is excellent pattern recognition crossed with excellent context discrimination.
You know the current context and you have enough of a pattern recognition that you can actually put them together. Poor judgment is overgeneralizing on a past success or failure or just relying on an AI generated best practice. The implication is that if you are hungry to show you have good judgment, you can surface the nontransferable elements of your recommendation and put those in center stage.
So when you're talking internally about a project you want done, when you are externally and you're pitching something as a consultant, what is unique about this moment, this org, this system? Let's say you're a product manager and you're trying to get something built and it's an AI native architecture. What is it about this org that makes that solution specifically correct? If you are looking to show you have good judgment, look to show you understand context deeply.
That is your advantage. Get passionate about a particular corner of context. Principle number three is the constraint principle. You are analyzing what's possible and you are judging what's possible. Now, analysis by itself is paralyzing. analysis will tell you all of your options. But a great business builder will think in terms of constraints, will think in terms of what is the possible build that we can execute today.
And so an excellent judge, someone who shows good judgment in the middle of the AI age, understands intuitively within a given context what is possible. Now, do you see how these principles build on each other? I've sequenced them carefully. These are not randomly allocated principles. They build on each other so that you actually develop a cleaner and cleaner understanding of judgment as we go forward.
Number four is the sequencing principle. Most insights fail because timing is bad or sequencing is bad. Judgment shows up in ordering your bets to create momentum and proof before resistance starts to mount. In other words, if you are internally trying to champion an AI system and there is some skepticism about how this system will work, order your bets carefully.
Show good judgment by showing you know how to sequence bets and thin slice your value so that you can deliver something that people can believe in. Thin slicing value sounds abstract until you basically have to look at an MVP of a buildable system and say this little piece that's what I'm going to deliver because that earns trust.
Maybe initially we're only going to deliver a chatbot on this particular page on the website. Maybe initially we're only going to convert this part of the wiki into a rag system internally. Maybe initially I'm only going to offer this particular piece of value in my consultancy as an AI consultant because I know I can deliver it and I know I can deliver it fast.
That will earn trust and then I can deliver more. Sequencing matters and good judgment shows up in knowing what is possible. Now, principle five is dep prioritization. Everyone's favorite word. Know what not to do. Have non- goals. Explicitly list and defend ideas that you are not going to go after and not going to pursue.
Have a rationale for that. That is something I will guarantee you that AI is not so great at. AI loves to expand scope. One of the signals of good judgment in 2025 of someone who is accountable for their work is they will be honest with you at a moment's notice about what they are saying no to what they are deprioritizing what they cannot do.
That is irreplaceable. And that is something honestly that is an excellent piece of career advice in an age littered with do more, do more, do more. What are you doing that is too much? What are you doing that you need to let go of so you can focus more effectively? And if you really commit to that, if you choose what you're going to depprioritize when you look at a project, you open the door to disproportionate leverage.
And so when you see projects that are runaway successes, part of why they're runaway successes is someone in that project said, "This is the goal. Not this, not this, not this. We are building a chatbot for the customer. It will not have the ability to use images. It will not have voice initially. It will not give you the ability to upload files.
it will be extremely good at talking about the products on our website. That is all focus have non- goals and that's true whether you're trying to define what to build or whether you're trying to define what you're doing next. It is it is a universal truism. It's absolutely essential.
You you have to say no in order to be able to say yes. And it's something that I I keep beating the drum on it because AI is so bad at it. And I want to call out like the there are things AI is terrible at and this is one of them. like specialize in this, right? Dep prioritization. Number six, the calibration principle. Judgment compounds through feedback on accuracy.
In other words, your judgment gets better as you get feedback on what goes right and what does not go right. You start to anticipate more correctly as you get feedback on what works and doesn't work. In a sense, this principle is fractal. It works for your career in the sense that you try things on and you say that worked, that didn't work.
But that takes period of years, right? That takes time. It also works on projects. It works on AI projects especially well because AI projects are typically run fast and they're run hard and you get feedback quickly. And so if you set up an AI agent and you see nobody is using it internally, you get feedback on how you scope that agent.
Maybe you prioritize incorrectly. Maybe you depp prioritize the one thing that the business actually wanted. You can learn from that. You can calibrate. You can get better. Principle number seven. This is another key element of good judgment. It's a coalition principle. Good judgment includes mapping the social graph of decision makers and sequencing their buyin correctly.
You need to be planning conviction moments where stakeholders experience early wins and shift their stance from merely permitting something to happen to active ownership. And if that sounds too abstract, if you're trying to get something over the line internally as a project, it only happens if you get your director and then your director's peer and then your senior vice president and then finally the seauite on board.
That's exactly what I just described. I just used abstract terms for it so everybody could understand it. It's the same thing. If you're a consultant, you run through the same process. Do you see that? One of my larger thesis is that we are overblowing the death of consultants. It is it is actually I think more correct to say that a lot of the busy work in producing decks that consultants have done for a long time is going to go away.
But the idea of someone who offers good judgment is becoming more and more important because AI is bottlenecked on good judgment. And so excellent consultants that offer good judgment are priceless. But so are excellent internal teammates who offer good judgment. In a sense, we are all becoming consultants.
So the coalition principle matters. Figure out who needs to go from permission to excitement to ownership to enthusiasm and sequence your conviction moments, your aha moments to get them there. That's good judgment. Again, AI can't help you there. Principle eight is the responsibility principle. Judgment carries ownership.
The clearest tell of good judgment is a willingness to say, "If I'm wrong, here's how we'll know it, and here's how what we'll do." You don't have to be right all the time to have good judgment. People sometimes think you do, but you have to have accountability and you have to be willing to say if I made a mistake, here's what we're going to do about it.
This is where I often tell people the fastest way to fix AI slop at work is to tell everyone, you are accountable for every word you write. It can be with AI, but you're still accountable. That accountability is a sign of good judgment. Own the consequences. Own the consequences. Again, not something AI is good at.
Number nine, the transparency principle. In the old model, assess opacity, signal value. Trust the deck. That's a fancy way of saying just trust our deliverables. A shiny report. Now, people actually trust transparent reasoning more. This is a trend that's opening up because intelligence is so available. When intelligence was was expensive, people trusted that a wellp polished deck was a sign that you'd put a lot of good thought into it.
Now, a wellp polished deck is just, you know, a chat with Kimmy K2 away, but a thoughtful deck is not. A thoughtful deck still requires good old-fashioned brain power. And so, being more transparent with your reasoning is coming into vogue. You want to be honest about the options. You want to be honest about your depp prioritization logic, about your assumptions, how you think it through, what trade-offs you're negotiating.
That is something that wasn't true 5 years, 10 years ago in the same way that it is today. that is somewhat new and it's a response to AI. It is a way of showing you know how to think clearly, logically in an age where AI just wants to spit tokens because the difference between intelligence that just says everything is on the table.
We won't have non- goals. We're going to be super aggressive. An intelligence that can lay out hard trade-offs really clearly is real. If you've seen good thinking, you know the difference between really good quality thinking and the first draft out of chat GPT. Don't just ship the first draft out of Ched GPT.
Good judgment is showing transparently how you are actually wrestling with the problem. Principle 10 is the compounding principle. Judgment creates leverage when it's encoded into systems that last. And so a lot of good judgment is figuring out how to solve a problem by scaling out a playbook or an automation that others can run with.
So your judgment shifts then from personal heroics to true organizational capability. I want to give you some encouragement. The 10 principles I've outlined are all things that AI is not getting better at all that fast. So if you're sitting there wondering why is Nate talking about this soft skill stuff? Isn't AI going to be good at all this stuff? The answer is these are the soft skills that distinguish what humans are good at in the age of AI.
I want you to know it because I think it's not something that is taught well because we didn't have to learn it except by osmosis before. Principal engineers learn this from other principal engineers for their discipline. Now someone has to start teaching it for all of us because suddenly judgment is all of our job.
Good judgment is something that AI can't take away. So we'd better get good at it. So we'd better learn it. So we better learn how to teach it. And that's where this is coming from is I want everyone to start to get into the idea of having good judgment. And I want you to start to break down the mental wall between being inside the company and outside the company because I think a lot of the traditional roles that consultants had are now being occupied by people inside the company as well.
Those lines are starting to blur. And similarly, you see consultants doing software work that in the old days would have been done by internal employees. It's another example of AI pushing the lines. And that's all happening because intelligence is becoming cheaper and cheaper and judgment is becoming more valuable.
Judgment is the new bottleneck. Judgment is what is becoming scarce when analysis is free. I hope you enjoyed this. I hope you feel like you have a better sense of what actually goes into the idea of good judgment. And I hope you're able to apply it at work because that's the whole point. I wrote this up more on the blog.
I have a prompt to help you think through where you need to learn and grow on judgment. Dig in and have fun.

---

<!-- VIDEO_ID: qVufzX_8bqE -->

Date: 2025-09-22

## Core Thesis
The current AI-driven hiring landscape is fundamentally broken because both candidates and companies misuse AI, leading to a degradation of genuine human signal. The path forward requires a counter-intuitive shift: embracing AI transparency and strategically leveraging AI to *amplify* unique human judgment, critical thinking, and the ability to navigate complex constraints, rather than attempting to replace or merely detect these qualities.

## Key Concepts
*   **Human Signal amidst AI Noise:** The core challenge in modern hiring is distinguishing authentic human judgment and unique insights from generic, AI-generated responses.
*   **AI as an Augmentation Tool:** AI's true value in hiring (and work generally) lies in enhancing human capabilities and clarifying judgment, not in replacing it.
*   **Artifact Strategy:** A framework for candidates to proactively demonstrate their thinking process, constraints, trade-offs, and iterative work (including "ugly artifacts" like failed experiments) as proof of genuine skill and authenticity.
*   **STAR-C Method:** An extension of the STAR interview technique that explicitly incorporates "Constraints" to highlight a candidate's ability to make informed trade-offs and exercise judgment in complex situations, a key differentiator from AI-generated answers.
*   **Shift from AI Detection to AI Assessment:** Hiring managers should stop trying to detect AI use and instead design interview processes that assess a candidate's ability to effectively collaborate with, verify, and strategically leverage AI.
*   **AI Fluency Framework (Literacy, Integration, Leadership):** A structured model for evaluating candidates' AI capabilities across three levels:
    *   **Literacy:** Understanding tools, verifying outputs, awareness of limitations.
    *   **Integration:** Designing AI-powered workflows, handling errors, systematic collaboration.
    *   **Leadership:** Strategic adoption, governance, team development, architecting AI systems.
*   **The Value of "Messy Problems":** Counter-intuitively, providing candidates with complex problems featuring conflicting requirements and ambiguity is a superior method for assessing human judgment and critical thinking, as these are areas where current AI models struggle.
*   **Transparency as a Green Flag:** Openly discussing how AI was used, including verification processes, disagreements, and limitations, is a counter-intuitive indicator of authenticity, critical thinking, and a valuable skill set for both candidates and hiring managers.
*   **AI's Context Window Limitation as a "Tell":** A counter-intuitive observation that current AI models tend to "anchor" to a specific angle in a conversation due to context window limitations, making them less adaptable to rapid shifts in problem framing  a potential way to identify over-reliance on AI in interviews.

---

If you are hiring or if you are interviewing, this is your interview guide. I'm making it for both because both sides are responsible for using AI better. And I want to talk about it because everybody's using AI and most of us are using it badly. That includes hiring folks and candidates. 83% of companies admit to screening with AI.

I bet the others do anyway. 65% of candidates admit to applying with AI. I bet the others do anyway. Everyone sounds the same. Let's say you get past the application process. Now you have candidates using tools to interview. And you know what? Interviewers catch them. And candidates have a terrible experience because they're not even talking to humans anymore.

I know a senior engineer who has over a decade of experience who recently got rejected because the AI interviewer talking to him talked over him wouldn't let him finish his sentence asked him confusing questions and it's not even clear it recorded it correctly and this is passing for efficiency. I'm seeing case studies here where companies are celebrating the efficiencies they get with AI hiring when people are all over Reddit and all over X talking about how terrible the experience is for candidates. This is not how you get your

next champion if you are hiring. It doesn't work that way. We need an interview process that prioritizes human signal amidst the AI noise. And I want to give you specific strategies both if you're an applicant, which we'll do first, and also if you are hiring, which we'll do second. And I want to go into both because I think both sides have a responsibility to get better here.

So number one, if you're an applicant, these are my top tips for how you interview better. Number one, fix your tool strategy. There are a lot of very expensive tools out there. Final Round AI runs over a hundred bucks a month. I think it's 148 or something. ridiculously expensive, but they're preying on the fact that you need a job.

You don't have to use the most expensive tool. In fact, there are reports from the employer side of detecting final round AI interview responses because they sound so generic. Whereas, candidates are saying that a much cheaper alternative like Bay YZ AI is working better because the answers are fast and feel fluent and natural.

The point is not to pick a cheating AI that helps you cheat undetectably. The point is to find something that you can partner with that helps you to structure your thinking. I actually think the most useful tool may be free. Google interview warm-up lets you practice and get better with AI answers.

It helps you understand what you did right and what you did wrong. It helps you to go back and forth and spar in a way that's low stakes. You don't have to pay a lot to get help. Before we go further, I want to underline something like three or four times. The right tool will not get you the job.

And the right tool will not get you the career. And the tool people are selling you lies if they say so. That is not what gets you a sustainable career. Figuring out how to showcase who you are, your passion, your genuine skills, your insights, that's what helps you win. And AI is only there to help you do that well.

And the prompts that I'm writing for candidates in this piece are prompts that I am designing so that you can prepare better than anyone else prior to the interview with the help of AI. Let's get into your artifact strategy next. Almost no one has an artifact strategy. So tip number one, get an artifact strategy. What's an artifact? An artifact is a proof of work. It's it's a packet.

It helps you to show your thinking around real problems. And by the way, that is going to help you prepare for interviews. As an example, you would want to look at a project you've done and not just do what so many people do, which is throw up a nice little website, put up a bar chart, say you made it go up and to the right.

Instead, you want to build a proofof work packet that shows how you actually think, the constraints that you faced, the decisions you made, the trade-offs that you considered. Traditionally in product management, we've been doing this for a while because we were always told you have to show your thinking as a PM.

I would now say looking at how people are actually interviewing, that is more and more the case for every role in tech. If you're in design, you're going to need to do this. If you're in engineering, you'll need to do this in your own way on the technical side. Even in roles like customer service and sales, you are increasingly going to be asked to show solid evidence of human judgment.

Especially as you get into more senior roles, you want to be in a place where you can show that thinking clearly. And it doesn't necessarily mean that you just email this packet off and hope that that works well. I'm not saying that. It's in the interview. You have the option to pull it up if it's interesting and moves the conversation forward.

It acts as an after interview additional packet of information if the interviewer is interested. And it helps you most of all to get prepared without sounding like a parrot. And so many of the issues with these AIs that assist you in interviews is that they make you sound like a parrot and you are so desperate to answer the question right, you don't realize you sound like everybody else.

You should also include ugly artifacts, not just the pretty ones. I actually look when I get resumes, when I look at the websites people send me, I want to see is everything super polished or are you courageous enough to show things you've worked on, scratch notes, iteration history, failed experiments. The most compelling story I have ever seen on a personal website for a job was this lengthy single page post.

And it showed a 5-year history in a role. And it went through meticulously what the person had done to add value at each stage in that role. And it had pictures and visuals and designed elements. And it read really fluently. And you could see how the person had negotiated setbacks and obstacles along the way to get the company to where it was.

It was incredibly compelling. It showed iteration. It unquestionably proved authenticity. The last thing I want to call out is that the artifact strategy extends into how you interview. I I call it the star C method. If you've ever done STAR, you know it's situation task and then you go from there into the assignment and your response.

And I'm adding constraints. And so star C is all about showing that you can work within constraints because AI answers classically are not very constraintheavy. And so what I recommend that you do is that you take your star situation and you want to make sure that you layer in the constraints that enabled you to make hard tradeoffs along the way because good constraints, if properly told in the STAR format so people can follow along, help you show good judgment.

Good constraints help you show good judgment. And I think that that's increasingly important because if you're just giving a standard response and the interviewer has heard star before and all the AIs have heard star and you tell star, it feels very stale. You need something that helps you to add that human element.

And if you remember star C, it can help. So situation, task, action, results, and make sure you layer in those constraints. That's the C. I want to go beyond just the toolkit and the artifacts and interview strategy for a minute with candidates. If you are using AI, please be transparent in 2025. It actually increases your authenticity.

Let me give you an example of some talk tracks that would impress me. I use Claude for research. I went back to primary sources. I looked through what actually worked and what didn't work. The analysis that I'm putting in front of you is mine and I made sure that I can own it and stand behind it. Fantastic.

Show the AI stack that you're using in the verification process you use. This is going to be true in technical roles and non-technical roles, too. Make sure you mention places where you disagreed with AI. May make sure you mention where you caught AI in hallucination. Make sure you mentioned what tools you wanted AI to use versus not. That conversation is important.

And that actually is a nice segue brings me to the second part of this video where we're going to talk to hiring managers. Hiring managers, you need to evaluate better. And it starts with not penalizing people for exactly what I described. If your candidate talks about using AI, don't you dare penalize them. Especially if they're being transparent.

That is the kind of culture you want to have in your company. You want AI champions who can talk about their successes with AI and also their failures with AI. Make sure that you don't penalize candidates who are showing that behavior. And so this brings me to the next piece. If you actually want candidates who work with AI, you need to stop running interview processes that are designed to have zero AI.

So, I'm suggesting that you stop with your AI detection practices and start with AI assessment practices. Give candidates AI tools during interviews. Meta actually does with this with their engineers. They give them a llama install and they tell them to work with AI and assess their ability to do so. Evaluate how the candidate actually collaborates with AI.

evaluate not just if they use it, but how they use it to add value, whether they just do what AI says or whether they're actually able to exercise some agency over the AI and direct it in ways that are useful to get the overall job done. Make sure that you also test how they handle really messy problems that require conflicting requirements, high thinking quality, and the ability to negotiate multiple constraints.

Those are classical areas where AI breaks down. I just advised candidates who are interviewing to call out constraints with the star C method. I am suggesting to hiring managers that you fish for those constraints. Look for messy problems because the candidates will have to show they are good at what they're doing with their human brains to answer messier data problems.

Don't just give a candidate a really clean data problem as a take-home exercise and expect to get useful value. In fact, take-home exercises are on the decline precisely because AI can get them done. What I'm advocating is that you give them exercises that are kind of a mess because you're testing their ability to use human judgment.

And candidates, if you're still listening, I'm sorry, you're going to get some exercises that are a bit of a mess. But on the plus side, it gives you the chance to show your human skill sets. And that's what we're here to demonstrate. I want to give you also a framework as a hiring manager to assess candidates for AI fluency.

It's one of the hottest topics in 2025. I'll probably do more on it soon, but as a quick rule of thumb, you want to be checking for three levels. One is AI literacy. I guess zero is no AI, but one is AI literacy where you are able to see that the candidate can choose between different tools intelligently. The candidate can verify outputs for hallucinations.

The candidate has awareness of AI limitations. The candidate can tell you the difference between claude and chat GPT and why. Number two is AI integration, which can be technical or non-technical depending on the role. But you're looking for the candidate who can talk about their workflow design or how they would design workflows in your role with AI at the heart of those workflows, what tools they would select, why, how they would handle data.

You want to check for error handling and have the candidate bring that up proactively. Talk about their evaluation and metrics philosophy. Talk about systematic collaboration. If you want someone who can actually help you be the 5% in the MIT study, that's someone who can help you with workflows. That infamous study with execs that said only 5% of projects deliver ROI.

The key was good integration. Level two candidates on AI are going to be able to talk integration fluently. And yes, you want to be asking interview questions that test for that. You don't want to just ask your traditional role interview questions. Level three AI leadership. This is going to be for senior roles.

You need someone who can do one and two. So they can do tool selection, output verification with their eyes closed. They can walk through workflow design, error handling, but they can do more. They can talk to you about strategic adoption. They can talk to you about AI governance. They can talk to you about team development with AI very fluently.

They can architect systems that allow others to design workflows and ensure and be accountable for outputs against multiple workflows that are designed. These are the kinds of people that you're looking for in leadership roles where they understand the domain, but they also have a very high level understanding of AI that allows them to truly lead their team.

Because these days, most people hiring for leadership roles need a leader coming into the space that doesn't need their handheld on AI. They need to be a champion for AI from day one and potentially be a champion in a room full of people where some of them are deep domain experts but may not be deeper on AI.

And so every hire you make as a hiring manager needs to move the ball forward on your AI transformation strategy and that includes senior leadership roles. Expect your senior leaders to know how to develop their teams on AI from day one. Don't tolerate ramp time. Ask the questions you need to ask to ensure that they can do so. As an example of a good question, why don't you give them the actual stack you have? Not the ideal stack, the actual stack that you have.

give them an example of the kinds of resistance you're seeing across your organization with AI and then say how would you solve this? How would you bring your team along? Let's say we needed the team to get to strong integration fluency within 2 months. What would you do? Why? Cuz you're then testing multiple things, right? You're testing domain expertise.

You're testing their uh fluency with AI and how they would handle that. And you're also assessing leadership and change management. And you can break down that answer and see where they stumble, see where they're weak, see where they're strong. There's other questions, but you get the idea. When you are interviewing, and by the way, if you're still listening, this is like free intel for the candidates.

There are red flags and green flags. And we've always had that, right? The AI red flags look a little different. And the AI green flags look a little different. And I want to spend some time for AI. A red flag looks like not just generic responses which I talked about at the top where you're overrelying on tools and like you're just reading the response which yes people can read body language they can read when you're like sliding your eyes to the side they can read the weird pauses people notice.

Okay, candidates, if your candidate can't explain AI limitations, if your candidate can't go off script, if your candidate doesn't have the ability to break down a problem from a different angle on fairly short notice, that is a big red flag. It's also a great tell because AI tends to take some time to break down problems from different angles and is actually even the most cutting edge models are not super good at that right now.

they tend to get stuck in the middle of a chat on a certain angle and anchor to that because of the context window. And so if you are suspecting that your candidate is just reading answers, if you shift the angle of the problem quickly, their AI may not catch up. It's a way to push them off script. On the other hand, in the AI world, we have new kinds of green flags.

If your candidate volunteers, how they're catching AI errors, if your candidate volunteers to talk about how they do a systematic verification process for work they get done so that they take ownership and accountability for it and they're not just paring what AI says. If your candidate can quantify AI impact and talk both at the individual level and the team level and the organizational level about what AI can do for the business, what AI has done in their role, that's a huge green flag.

If your if your candidate has a philosophy of the role that says this is how this role is evolving in the age of AI that they can explain coherently they can act as a peer champion for AI for their role. It's really compelling. So there's lots of green flags too. It's not just red flags here for both parties.

Right? We're bringing this to a close here for candidates for hiring managers. I want to give you three principles to stick with that I think will help you. Three principles to unlock what feels like a stuck market right now. Number one, enhancement beats replacement. AI is there to make human judgment clearer, not to substitute for it.

Candidates, that means if you're reading answers and not using your brains, you're losing. Hiring managers, that means if you're using AI to evaluate interview transcripts and you're not actually thinking about what the candidate is saying and taking the candidate seriously as a person, if you're just using AI to interview, you are also losing. You're also losing.

You're contributing to the problem. Both sides win with transparency. That's number two. Candidates need to show better judgment when they admit to using AI. And managers must find better talent when they have to talk about how they actually use AI at work. So bring AI to the table. Don't hide it. Candidates don't hide it.

Hiring managers don't pretend it's not there. Both sides need to talk about their AI strategy to actually move the ball forward. Third, last but not least, make sure that you know how to use your prompts well. I've included a bunch of I think nine s prompts that like dig deep on interview prep. But you need to have prompts that actually help you move the ball forward.

If you're assessing rsums with prompts as an aid, not as a substitute, you need to have prompts that actually help you to do that. If you are preparing as a candidate, you need to have prompts that help you to research a JD and go way beyond the surface level in order to stand out as a candidate.

I they have written prompts where you can get three or four pages of really strong interview prep material out of one job description because you're telling the AI very specifically what to look for that helps you to prepare. It's all about intent. It's not like my words are not magic. It's about telling the AI how to effectively assess what is in front of it, what is between the lines, and what you need as an interview prepper to get ready for a big conversation.

Hiring is broken, kind of broken. I want it to get better. And I think the only way it can get better is if we admit that AI is at the table now. If we're transparent about it, and if we use AI to support human judgment rather than to replace it. Best of luck out there, and let me know how you're doing. Cheers.

---

<!-- VIDEO_ID: vDtwS1w16K4 -->

Date: 2025-11-12

## Core Thesis
The widespread, uncritical adoption of AI tools is actively detrimental, introducing significant system complexity, hidden integration costs, and unmitigated failure modes that often outweigh perceived benefits. A disciplined, first-principles-based evaluation framework is essential to identify the rare tools that genuinely solve measurable problems, can be sustainably integrated, and whose worst-case failure modes are understood and acceptable.

## Key Concepts
*   **AI Tooling as a Source of Active Harm:** Unlike traditional software, AI tools introduce unique complexities and failure modes (e.g., prompt injection, memory leakage) that can actively degrade system performance, increase operational overhead, and erode organizational trust, rather than merely being useless.
*   **The "No Tools Tool" Culture / Bias to Not Buying:** A counter-intuitive default stance of skepticism towards new AI tools, advocating for rigorous, multi-faceted evaluation before adoption, challenging the prevailing hype-driven acquisition trend.
*   **The 3-Question AI Tool Evaluation Framework:** A structured decision-making process for assessing AI tools, acting as a decision funnel:
    1.  **Measurable Pain Point ("Heat-Seeking Missile"):** Does the tool address a specific, quantifiable problem, moving beyond vague "hopes and dreams" to target a crystal-clear, articulable pain?
    2.  **Integrability & Sustainability ("Effort of Change"):** Can the tool be integrated and maintained long-term, considering the full spectrum of technical, behavioral, and organizational costs, including the "edges" of interaction with other systems and teams?
    3.  **Worst Failure Mode & Mitigation ("Stomach Test"):** Are the potential catastrophic failure modes understood, acceptable, and explicitly mitigated, especially given the inherent non-determinism and emergent risks of generative AI?
*   **Hidden Costs of AI Integration:** Each AI tool exponentially increases system complexity by adding new handoff and integration points, leading to significant, often overlooked, costs in terms of maintenance overhead, training, IT support, and behavioral shifts.
*   **Organizational Readiness as a Prerequisite:** The success of an AI tool is critically dependent not just on its technical capabilities, but on the organization's capacity and willingness to adapt, sustain, and manage its implications, including understanding edge cases and supporting its operation.
*   **Documentation as a Proxy for Quality:** Good documentation serves as a pragmatic indicator of a well-engineered and sustainable tool, offering a more reliable assessment of quality and long-term viability than marketing claims.

---
You know, there are more than a 100,000 AI tools out there and most of them are going to be useless. And in fact, they're going to be actively harmful. Let me explain why. If you add any AI tool to your system, you are adding at least two new handoff and integration points, not to mention a whole host of failure modes.
Because generative AI products, they're more complex. They solve things that are harder to solve. And so, there are more ways they can fail. And so unless you are bought in and ready to sustain the product, you are buying yourself a load of failure. And that is why we see study after study coming out showing companies investing in AI tools and being disappointed by what they buy.
Sometimes it's not even just the tool itself. It's the fact that the organization isn't ready. And so today I want to walk you through the three critical questions that I ask when I'm looking at AI tooling so that you have a framework. Then I want to show you a couple of tools that I think are worth thinking about for specific pain points that you should ask yourself those questions for.
And if you want to go deeper, I've got a whole load of tools to review over on the substack 45 or so that you can dig into that I've started to ask these questions for. Frankly, I think we should be asking these questions whenever we evaluate a tool. Question number one, does it kill a pain that we can measure? So when you are trying to find an AI tool, so often you think about hopes, you think about dreams, you think about how far you can go with the tool, the vendor sells you a lot of cool stuff.
Do you have a specific painoint? Do you have something that is absolutely crystal clear? So as an example, Lera Guard, which I'm going to show here in a second, it cuts down prompt attacks. That is its purpose. It stops prompt injection attacks. Maybe not perfectly, but a lot of them. If you have a production AI system, you may want to think about a tool like Laragard because of that.
And I'm going to show these tools at the end of the video. So, we're going to stay with the principles here. We'll get to the tools at the end. The point is simple though, right? I I can name a painoint. I can say I have prompt injection risk. Therefore, I need a tool to address that. Therefore, I need to review some vendors to go after it.
I rarely see that level of discipline from people who are shopping for tools, whether they're individuals or whether they are larger companies. Either way, that I'll give you another example that's individual sort of focused. I want to keep track of my chats when I have them with Claude, when I have them with chat GPT, when I have them with Perplexity.
But I I don't have one place to do that. Well, there is a startup that's addressing that now. It's called Nessie Labs. and they have a product out for Mac called Nessie and that's exactly what it does. It imports your chat GPT chats. If you use Chrome, it's going to work with you to keep track of your chats.
It is laser focused on that specific painoint. But again, it's not perfect. It does not work if you use the clawed app builtin. It doesn't automatically keep track of your chat GPT chats that are not in Chrome. So there are weaknesses, but you can't assess whether it's right for you or not if you don't know very specifically what the pain is that you're trying to solve.
If you really care about getting all your chats in one place and organizing them and you can name that pain, then maybe it's worth it. Maybe that's the right one to go after. Question number two, can we integrate and sustain this tool? So you need to map out the effort of change. If it's an individual tool like Nessie, the effort is a change in behavior.
Maybe you're using another browser, not Chrome. Maybe you're used to using the desktop apps for AI. Maybe you're not ready to do the work of exporting a zip file of old chat GPT chats to get the organization started inside Nessie Labs to have your memory layer. But you have to decide if that's worth it.
You have to decide if the cost of sustaining that over time of changing your behavior over time is worth it. If you are installing an enterprise tool, it's of course much more complicated. Your teams will need training. They will need to understand edge cases where the tool doesn't work. Your IT department is going to have to support it.
It's exponentially more complex. And every single tool you add adds edges that you have to sustain. It touches other tools in your ecosystem. and it touches other teams. Have you mapped that out? Are you ready to sustain the tool? Good tools are going to make it as easy as possible to own setup, to tune your alerts, to figure out what ongoing maintenance looks like in a way that is sustainable for your business.
Tools that are poorly constructed assume most of the work for figuring that out falls on you. That's why I'm a big fan of looking at documentation when you want to figure out what tools work. Number three, when you're asking yourself about tools, ask yourself, what is the worst failure mode here? And can we stomach it? Now, individuals sort of get away with one here.
The worst failure mode for individuals is usually not too bad. You can actually just look at a particular tool set and say, "Yeah, you know what? I'm going to try Nessu Labs. It's going to be fine. The worst thing that could happen is that I end up with a tool with some memory that I didn't end up using. That's not too bad.
" or I forget to use Chrome for a chat. That's not too bad. Companies have a much higher bar to meet. Let's say you're using mem zero. It's a memory layer for customer success agents so that the customer success AI agent can remember your customer and interact with them more personally. Great idea. What if there's a catastrophic failure and there's a memory leakage of some sort? Can you stomach the lack of trust that comes with that? Do you have asurances? You have architecture in place to make sure that that's mitigated. What if you are
using Lera Guard and a prompt injection attack does succeed? What do you do then? And so a lot of what we're doing when we look at tools is we're essentially trying to reference like do we understand the pain? Does this actually act like a heat-seeking missile and just go after that particular pain point? If it does, can we integrate and sustain it? And if we can integrate and sustain it, do we understand the downside and have we mitigated for it? If we ask ourselves those questions, we are going to be so much farther along on
unreged tool purchases. There are, look, there are billions of dollars being thrown around here. Part of how the VC industry is sustaining itself right now is that people are throwing money at AI tools and not asking themselves, is it useful? So, without ado, I'm going to show you just a peek at Memzero, at Laragard, and at Nessu Labs because I think those are all ones that I've referenced here.
And if you're curious, I'd love you to dive in. There's, as I'm saying, a bunch more tools that I'll have up on the substack as well. So, this is LERA. The idea is it's a layer in between your generative AI applications and bad actor. And this is a tool that enables you to proactively understand what is going on. You get visibility.
It's going to protect you from prompt injection attacks. You can control and configure. Think of it as like it's the classic security play. It's a shield, right? Like, and you can decide how you configure your shield, etc. Nothing is perfect, but it's an example of a tool that's aimed at a particular risk that companies tend to articulate is very painful. This is Nessie.
Nessie is exactly what I talked about, an individual AI knowledge base for the mind. You can download it for Mac. The idea is that it's able to capture your overall chats and get them into summaries. You can organize, you can play with, you can use laser focused on a particular painoint I hear from a lot of people.
And last but not least, this is me zero. It's focused specifically on how you can help AI agents to remember customer success use cases. And so this is a travel example, but you can imagine this for a lot of other examples, too. If you have generative AI applications that you're focused on, this becomes a really powerful way to connect with your customers.
So, I don't pick these because they paid me or anything. They don't know they're being talked about. I'm mentioning them because I think that they do a good job talking about a particular painoint and I think that they are enabling us to talk about how you choose tools. Well, I don't care if you choose these tools or not. I want you to understand how to pick tools that work for you.
If you'd like to dig in further and understand how I think about tools, I have the same set of questions that I just outlined around sustainment, around picking tools, well, around finding the pain point, around worst case scenario planning. And I have that for like 45 tools because I think that we need to have honest conversations about this and we have to start somewhere and we have to start with harder questions than we're asking.
So, this is the no tools tool episode. It's it's I want you to bias to not buying the tool unless it says yes to these questions because I think too often we're too easy like we open the wallet too quickly. We need to be sort of hard-nosed about what tools really matter. So there you go. That's my take. It's how you build a no tools tool culture and pick AI tools that actually matter.

---

<!-- VIDEO_ID: Vm_v70xhas8 -->

Date: 2025-10-11

## Core Thesis
Managers are critically failing to adapt organizational budgeting and procurement processes for advanced AI tools, mistakenly applying traditional software cost models to capabilities that function as fundamental productivity multipliers. This cognitive inertia, driven by outdated mental models and misaligned incentives, risks both significant competitive disadvantage and the loss of top talent who will gravitate towards organizations that recognize and invest in AI as a strategic imperative.

## Key Concepts
*   **The "Mechanical Horse Problem" for AI:** Organizations are applying outdated mental models and budgeting frameworks (designed for traditional software) to fundamentally new AI capabilities. This leads to underestimation of AI's transformative potential and economic value, treating it as an incremental improvement rather than a paradigm shift.
*   **AI Tooling as a Human Compensation Baseline:** The true cost-benefit analysis of advanced AI tools should not be compared to traditional software costs, but rather to the cost of human labor they augment or replace. This reveals AI tools, despite higher per-seat costs, as a vastly cheaper alternative for achieving significant productivity gains (2-10x).
*   **Incentive Misalignment in AI Adoption:** Organizational structures and budgeting processes often suffer from incentive misalignment. This includes prioritizing new hires over empowering existing employees and a "problem of the commons" where no individual manager is incentivized to be bold in advocating for necessary, non-traditional AI investments.
*   **The Evolving Definition of "AI":** The rapid advancement of AI means that tools labeled "AI" (e.g., a 2023 chatbot vs. a 2025 agent capable of hours of work) are not interchangeable. Their vastly different capabilities and economic implications require distinct budgeting and strategic consideration, challenging the notion that "AI is AI."
*   **Talent Exodus as a Consequence of Underinvestment:** Top-performing employees, particularly those adept with AI, will "vote with their feet" and leave organizations that fail to provide access to cutting-edge AI tools and training. This creates a critical competitive disadvantage in talent acquisition and retention.

---
managers are screwing the AI revolution. And I say that as someone who has managed teams and I have great sympathy for the challenges that we're all experiencing right now, but it's true. It's true. And I'm going to I'm going to be really honest. I have talked to a lot of individual contributors and the universal complaint I hear near universal 99% 98% is that they want access to AI tools that will help them do their job better in a really significant way.
Like we're talking in some cases for your best performers double or triple their productivity and they can't get it. And they can't get it because the managers are not budgeting for it. And managers will tell me well we can't go to it. It is giving difficulties about security going back up to department heads. Like my boss is just not going to like go for 400 bucks a month per employee.
They've never seen that. Managers, senior managers, directors, you guys carry the flag on this. You have to be the ones that explain clearly to your bosses, to the IT department, why we don't live in a traditional software world anymore. We do not live in a world where getting access to a regular sort of software product is a minor help and a minor inconvenience if you don't get it right.
It's not a big deal either way. Maybe it costs 25 bucks a seed for the year. Maybe it costs a h 100red bucks a seed for the year and you forget about it. No, we live in a world where people who are extraordinary at AI can effectively double or triple their productivity if they have access to the right tools.
You can literally buy the productivity off the shelf. And yes, it is more expensive than traditional software. It is two or three hund bucks a month. And you may be thinking to yourself as a manager, I'll kick this down the road. I'll get it next fiscal year. I've got news for you. It's not going to get cheaper.
And the reason why is we are now baselining against people compensation. So you think 300 400 bucks a month is expensive. Sure it's expensive. Is it as expensive as a person doing that job? No. No it is not. It is vastly cheaper. There is a lot of headroom there for model makers to build more effective tools that will go from two or 3xing your employees productivity to 5 or 10xing it and they will charge more.
Soon you're going to be looking at software costs maybe by 2026, by 2027 that are running a,000, $2,000 a month per employee. And IT departments aren't ready for that. Leadership isn't ready for that. But it is critical that we have the conversations now because otherwise there are going to be a few companies that figure that out.
Figure out that their own aged legacy processes are getting in the way of actually helping their employees to do better at their work. and they'll kick the processes aside, which is what you should do, and they will figure out a way to get it done. This is the same incentive set problem that frustrates existing employees when they see new hires coming in at a higher comp band, which is a notorious problem across the industry.
People care more about getting the new hire in the door than about keeping the legacy hire because the incentive is to make sure that that new hire in that moment walks in and fills that role. Whereas the incentive for the existing employee is just can they come in tomorrow. It just isn't the same. We have the same problem with software.
The incentive is can we just keep our existing process? Can we just have them come in tomorrow? Can we just have them do the work? And people are thinking of it like a mechanical horse. So in the old days when we had the car, we did not know what cars were capable of, right? We thought that cars were capable of things that horses were capable of.
It was the mechanical horse. It was literally called that. That was our mental model. We have the same problem now with software. It is the mechanical horse problem. And we think of AI software as if it is software. It is not. We need to have a new category for it. And I realize that this makes me sound annoyingly like an onhype on trend AI agent. I don't know influencer.
It's not. The point is not to say agent for the sake of agent. The point is not to invent a category because we need a category because we need to hype things. The point is just that the old category doesn't fit anymore. Traditional software just isn't priced like this. Traditional software doesn't deliver productivity gains like this.
And so if you were going through traditional software budgeting and planning processes, it's not going to work, is it? You're going to go through and like every other department head is going to sit there and be like, "Yeah, yeah, yeah. We're doing our regular basic like 100 bucks a month or 200 bucks a month total for our employees for software and professional development.
" And then we're done, right? Moving on. And you're going to sit there as the forward thinking guy where wherever you are, right? Or gal or whatever in whatever department you happen to be. Maybe you're in product, maybe you're in engineering, and you're going to be like, "Well, we're going to be budgeting 2,000 bucks a month per employee.
" No, that doesn't include professional development. We actually going to budget another 2,000 bucks a month to make sure they get the AI courses they need to supercharge them. And that's what we're going to do. Doesn't that sound great? And you're going to get laughed out of the room because nobody else at your level in your business is doing that.
And so this becomes a problem of the commons. Basically, everyone is incentivized to look in the budgeting process for leadership. And so, nobody is incentivized to be bold and say, "I could deliver extraordinary value if you can shift my budgeting." But that is what we need. And employees are going to vote with their feet here because the really good ones are going to move to roles and to businesses that get this and they will not stay with you.
They won't they won't stay with you. best employees at AI will go to the companies that understand this. This is not just a matter of Mark Zuckerberg broke the market with $100 million compensation. No, this is actually I mean for one, people left Meta after that. People walked away from the $und00 million compensation, right? They went back to their old jobs cuz they didn't like Meta. So no, it's not about the money.
It's about your ability as a business to generate a culture change that enables your best people to feel like they can thrive with AI. Because if you look at it from the point of view of an individual contributor, it has never been more challenging to figure out a career path. You have to figure out how are you going to supercharge your AI skills? How are you going to deliver in your current role? How do you set up your resume so it looks good in a world where roles are changing and mixing and merging all the time? And to do that,
you desperately need access to AI tooling so that you can show your next role or your existing role when you get a promotion that you know how to use it. And by and large, our businesses are not setting employees up for success on that. And I do think that comes back to senior managers and directors.
We have to have more advocacy for our teams. They need that for their own careers. They need that to deliver for the business. And if you look at it from a managerial perspective, you need that in order to grow your own career. Your teams need to be that successful. We are going to live in a world more and more where leadership at the best companies is going to expect you not to grow your headcount.
Instead, expect you to prove that you have done everything with AI that you possibly can to expand the impact of your team as it is today. And what we need is not just that expectation, but a corresponding investment in the team, a corresponding willingness to say if we want to grow that impact, it's not just a 20 buck a month chat GPT subscription anymore.
That was 2023. Sorry, they now do so much more. And I think that is one of the hardest things to convey to leadership is that even though the name is still chat GPT, we have gone in two years from a little chatbot to a agent capable of doing hours of work. And that's not the same thing at all. It's not remotely the same thing.
It is a weird world that we live in that we name it the same thing. We call it AI. Do you does your team have AI? Right? It should be 20 bucks a month. It's AI, right? Get the co-pilot. It's AI. AI is not AI. These are not onetoone comparisons. This does not work this way. And so it is on senior managers and directors to sit down with teams to explain in detail what they need.
And so teams need to explain and voice to managers. These are the tools I need. And managers need to explain and voice really clearly and advocate for their teams for this use case, for product, for engineering, for marketing, for customer success. This is what my teams need to be extraordinarily productive in 2025. It will cost.
It will cost more than traditional software, but you'd rather pay it than a lose your best people or b flo bloat your headcount budget because increasingly it is looking less in most cases like firing people unless they don't want to do AI and it's looking more like we're just not going to grow the team as fast because we have so much leverage from AI.
Well, if your team is wanting to lean in, that's fantastic. They have the domain knowledge. They want to lean in. Let's get them the budget so that they can actually deliver for the business. And overwhelmingly I hear from people that that is not happening. I see that that is not happening. Very few businesses are sitting there and saying our software procurement has to change.
Our budgeting has to change. But it does. It has to shift. It is not acceptable to expect your AI employees to do 2025 AI work on a 2023 budget. It doesn't work. Traditional software budgets are broken. So, if you are in that spot, send this video to your manager. Tell your manager, "Watch this. This is a problem for us.
I can't get the budgeting I need to do my work." And if you're in leadership, this has got to be a call to action for you. You cannot look at traditional software budgeting the same way. This is not traditional software. I don't care if you call it agentic software. I don't care what the hype term is, but you've got to shift the way you budget for work and you've got to invest in your best employees so they can deliver this 2, three, four, 5x impact over the next couple years.
So that is my challenge to you. We are we are not in old software land anymore. We are in new software land and there are new rules and there are new budgets and you have to invest in your team at a different level. Good luck.

---

<!-- VIDEO_ID: X7PWBlxJV1Q -->

Date: 2025-11-03

## Core Thesis
The widespread confusion regarding AI project success rates (e.g., MIT's 95% failure vs. Wharton's 75% success) is a symptom of misaligned measurement criteria and a superficial understanding of AI's organizational impact. True enterprise AI success hinges on cultivating "institutional fluency" through a deliberate, three-pronged strategy: fostering deep, team-level context awareness, inverting traditional ownership and skill distribution, and democratizing strategic "taste" across the organization.

## Key Concepts
*   **Measurement Bias in AI Success Rates:** The perceived success or failure of AI projects is heavily dependent on the chosen metrics (e.g., immediate bottom-line impact vs. broader productivity gains), leading to wildly divergent conclusions and "headline nausea." This highlights the importance of defining success criteria rigorously and transparently, rather than accepting headline numbers at face value.
*   **Institutional Fluency Framework:** A structured approach to building successful AI-native organizations, comprising three interconnected pillars:
    1.  **Context Awareness (Team-Level):** The ability of teams (the atomic unit) to deeply understand their domain, workflows, and uncertainties, and to articulate this "context" effectively to AI systems. This is "everyone's job" and is critical for local utility and multiplied value, effectively reducing the entropy of the input space for LLMs.
    2.  **The Ownership Flip (Individual Ownership, Team Skills):** A counter-intuitive shift in organizational dynamics where individual contributors must take high ownership for AI output quality and assessment, while technical AI skills (like prompt engineering, understanding LLMs) become commoditized and shared at the team level. This is driven by AI's "superpower" effect, demanding more from individuals and challenging traditional management structures by distributing agency.
    3.  **Democratization of Taste:** The necessity of pushing "founder-level obsession with quality" and strategic discernment ("taste in problems," "knowing what good looks like") down to the team level. This moves beyond a "priesthood" of taste-makers to enable autonomous, high-quality decision-making and problem selection at scale, crucial for speed and impact in an AI-driven environment.
*   **AI as a 10x Cost Multiplier:** AI is significantly more expensive per employee than previous software, necessitating a re-evaluation of traditional ROI metrics and a higher bar for demonstrating value. This pragmatic engineering concern underscores the need for the "institutional fluency" framework to ensure investments yield tangible returns.

---
You know, I don't blame people when they are confused about AI because the studies that are coming out are also confused. This week, October 28th, Wharton came out with a study on generative AI return on investment and implementation at very large companies. If that sounds like a familiar subject, it should because MIT studied the same group of companies just a few months ago. The kicker is this.
MIT study had a 95% failure rate on AI projects and Wharton came back with a 75% success rate. Now, it does not take a lot of mathematical skill to figure out that these are not compatible numbers. You cannot be both correct. And so I want to spend time unpacking what is really going on at the enterprise, how we put these two numbers together and what is a reasonable path forward that cuts through frankly the headline nausea that I get from all of this just top lines that don't make sense and that keep changing all the time. Business needs consistency. Business needs clarity and business needs to be able to actually build in a way that makes sense. So, my goal is to ground you by the end of this so that you don't get spun and confused when people are saying, "Is it 75%, is it 95%." Here's what's really going on. 95% came out of the extremely tight screen that MIT put on Project Success.
That is one of the ways they effectively engineered a headline that would go viral. And yes, I'm just going to say it. I think they engineered the headline because the screen is tighter than almost any other internal software measure I have ever seen. In this case, what MIT was saying is every project is by default a failure unless you can measure a dollar and cents impact on the bottom line, not the top line of the business within just a few months.
It was like 6 or 12 months or whatever it was. If you can't do that, then it's useless. That is no other software that I have seen. If you're buying software is measured that way. You always measure it on internal metrics that you think will map to larger business value. And that brings us to the Wharton study and the 75% success because Wharton took more of that approach.
Wharton's approach was to talk to executives and let executives tell them how they're measuring ROI. And what executives said overwhelmingly is that they're using other metrics. They're not just using dollars and cents on the bottom line. They're looking at productivity. They're looking at time saved.
They're looking at throughput. And when you look at all of those, execs feel like you get a very clear measure of success. And that's where that 75% number comes from. So really, if you want to know first what the heck is going on and why they're different, it's apples and oranges. You have a very hard profit measure from MIT and you have a looser, more conventional software ROI picture from Wharton.
Here's what both of them are not getting right and where I have sympathy. I think MIT is correct that we need to hold AI to a pretty high bar. This is a transformative technology. It's also an expensive technology. It is on the verge of being 10x or more more expensive per employee than any software was before.
Yeah, we're going to have different ROI measures. So, I think that MIT is getting at something when they're challenging leadership to think differently about software purchase ROI. But I think Wharton is doing a great job actually analyzing the the reality on the ground and the way execs by definition and by convention and the way they usually act really measure stuff.
So, my ask to you, if I were to like take all of this away, my ask is that you not pay too much attention to these kinds of headlines, I get inbounds, right? I get emails, I get messages coming in. I get it. It is confusing when the news media loves to report contradictory information. But the reality at organizations that are succeeding with AI is a lot more steady.
And that's the piece I want to leave you with from a grounding perspective. When we build with AI systems and they actually work, there are a few things that do align really well with both of these studies and I'll sort of explain that, but they don't like the studies don't get at them, right? They don't get at how to positively build and that's you know me, right? Like that's what I love to do.
The first piece that I want to lay out for you, think of these as sort of building blocks that you can build institutional fluency with. So, I talked about individual fluency a couple of weeks ago. I want to talk about institutional fluency today. I think that is one of the missing pieces that connects these two studies. And I think that understanding how it works will help you to not get swept and pushed around when the next whatever study comes out with whatever number.
The biggest piece of institutional fluency, if you want to set up a sort of whole companywide fluency on AI, your company has to get good at understanding and shaping context awareness for teams and individuals. And I think teams are really the atomic unit here. Individuals come and go, but teams are steady.
Teams have a particular vertical they take care of. Teams have a particular domain ownership. and institutions that are fluent in AI understand that the value of the team is the context they inhabit and specifically the context they're able to articulate to AI systems. So when we talk about context engineering typically that's a job.
I'm suggesting that we think of it less as a job and more as everybody's job. Context is something that we all bring to the table. Context is something that teams need to deliberately maintain. What do I mean by that? Right? Like if you understand at a very deep level, this is the way my domain actually works. This is the way I actually drive value for the business.
These are the unique processes and workflows that I can use. These are the areas of uncertainty and the areas I need to explore in my domain to get better. And if you can articulate that intentionally to an LLM as a team, you are going to be in a position to deliver multiplied value to the business relative to individuals working on AI alone or relative to the work that we've done before 2022 sort of pregenerative AI. Context is king here.
Context helps us to feed an AI with what's needed to be useful at a local level within the business. If you can't figure out how to help your team to articulate context to the AI, you're going to have trouble with everything else. And this is one of those things where like if you look at the Wharton study and the success, part of what's going on here is that leaders are saying that they are seeing accountable acceleration amongst teams.
Like the way I read that is that leaders are starting to see teams pick up and use context in their disciplines to drive value and the executive just kind of gets to measure it, take credit for it potentially and count it as a success. So context is the first piece I want to call out.
Institutionally fluent organizations in AI understand that context is local that context operates at the team level not the individual level and they are deliberately fostering team level context fluency. The second piece that I want to share with you that institutionally fluent organizations have is problem solving skills.
And this sounds really obvious because we've been talking about problem solving skills as an element in sort of employee training and upskilling for for decades, right? Way before generative AI. But socializing those problemolving skills is something that managers, directors and above are coming to me privately and saying this is really hard. This is not easy.
And I think that part of why we see the discrepancy with the Wharton and the MIT measures is that the MIT measure, the 95% fail rate measure demands that a entire organization be so good at problem solving that it meaningfully upshift the bottom line. That is an extremely high bar. You can get a whole bunch of teams who are good at problem solving and if you have two or three bad apples, you will bottleneck somewhere in your process and have trouble delivering value to the bottom line.
And so what we need is we need to treat problem-solving skills as a critical patch on team fluency that we cannot live without that we must have on every team and that we will hire for if needed to get done. In other words, AI problem solving is becoming all of our problems today now and it doesn't get better until we actually fix it.
So what does AI problem solving look like in practice? We can say it, but what makes it something that a team can reasonably learn? Because keep in mind, if teams know context, teams are going to know problems and teams are going to be able to sort of learn to solve problems. I want to suggest that problem solving is really, if you peel the onion back and you think about it deeply, a function of understanding how AI thinks about and processes information.
Because if you think about problem solving conventionally before AI, we're really manipulating information in order to unlock ambiguous problem spaces. And so traditionally, it would be like, I'm going to write my product requirements document or I'm going to do this data analysis. And we're manipulating information in order to get closer to unlocking a complicated customer experience or a painoint in operations.
And all of the stuff we talk about like critical thinking skills, good writing skills, those were all ways that we could scale up manually so that we could successfully manipulate information as an individual and as a team to solve these problems. And in that world, individual skills mattered a lot because individuals pushed information fluency forward.
Right? If the individual could write well, they might write well enough that the whole team was elevated, right? And then ownership resided at the team level. And so a team manager would be responsible for would own solving the problem, driving around obstacles, all of that stuff you want good managers to do. That is starting to flip.
And I have never shared this before. I think this is really interesting. I think that what we are starting to see in the age of AI problem solving is instead the individual needs to index really highly on ownership and the manager or the team needs to index highly on skills and that's sort of a reverse of the usual. So the problem solving skills, the ability to understand how LLM works, those actually can reside at the level of the team, but the ownership piece has to rest with the individual if we're going to make progress. And I'll explain why that flip has happened. When you think about solving a problem in the age of AI, what you really are doing is you are understanding enough about AI to feed the AI the problem in a way that it could understand and work with. And I've talked about this part before where you're sort of chopping up the problem, decomposing it so that the robot AI can pick it up and manipulate the problem and help you get through the problem space faster, which is the whole goal.
It is easier to solve problems if the robot intelligence is working on that problem with us. Here's what I haven't talked about before in practice with real teams building real AI systems. What I'm seeing is that ownership is irreplaceable at the level of the individual working with AI. If you don't have a very strong sense as an individual, as an individual contributor of ownership and quality and assessing the bar that AI is using to solve and insisting that the AI isn't doing good enough when it really isn't, you're not going to be able to add any value at all. Whereas in the past, you could have that bar set at the team level and the manager would be able to sort of manage the informationational standard and it would be okay because all of the humans were working together and information was moving slowly enough and we were exploring the problem slowly enough that the manager could act as a quality bar.
In this day and age, that's not true. AI is giving everyone so much superpower that you have to devolve ownership down to the level of the individual contributor. And I think that at root is one of the reasons why organizations are struggling so much with the AI transformation. It demands more of our individual contributors than it ever has before.
And we're not used to a world where the individual contributor is the atomic unit of the corporation as opposed to the manager. Corporations are founded on management theory. The idea is that the manager is accountable to for the domain for the department. They are the representative of the business. They work with the individual contributor.
That's how how we've done it for hundreds of years. I am beginning to think that that is not how AI native organizations are actually going to be configured. The power you have with AI resides so heavily with the individual. I don't think you can do it any other way. I think you have to put ownership at the level of the individual contributor.
And that has profound implications for how we train people. Because really what we need to train people to do is you need to start by taking ownership of your domain and your situation, of your problems, of the way you work with AI, of the bar you use it, everything flows from that. And ironically, what we previously had at the individual level, this sort of skill, hey, this is a really skilled writer, right? This amazing writer, uh, and we couldn't do it without him.
And he lifts up the whole team. That kind of thing can now reside at the team level. Look at how teams are sharing prompts with one another. Sharing clawed skills with one another. How teams are sharing custom GPTs with one another. AI is enabling the commoditization of a lot of those skills. And when it comes to AI problem solving, you can encode a lot of the technical skills and understanding of AI in sharable format.
And so let's say someone isn't super familiar with how transformer architectures work and how you want to chunk problems so that the AI can read the problem coherently. That's okay. You write a prompt for them. You share the prompt with the team. You have a brown bag where you talk about what it does, but they can just immediately run the prompt and the skill translates and they can gain skill over time as they socialize with the rest of the team.
But what you can't do is give them the skill and they don't have the sense of ownership. That breaks. That does not work. So we've talked about context. We've talked about problem solving and how it sort of inverts traditional team and managerial norms. There's one more piece that I want to talk about today that I think underlies this concept of institutional fluency that that isn't talked about very often.
I think that previously the concept of taste, the concept of is this excellent, is this extraordinary, is this something that is an incredible offer for the customer, we could delegate that to a small, call it a priesthood within the company. The Steve Jobs of our company is over here. He has taste.
He's an extraordinary builder. He's an amazing inventor. We'll run this by him and that will be fine. I think in the age of AI, taste is something that doesn't work that way anymore if you really want to move quickly. And so, one of the things that you want to do is actually give and socialize a sense of taste down to the team level so that teams are empowered to move autonomously without sacrificing extraordinary quality.
And I think that that quality tradeoff is one of the pieces I really have been sitting with in the Wharton and MIT studies. I feel like MIT essentially had an extremely high quality bar and Wharton had a more relaxed traditional software quality bar. And if you want to thrive and build an AI native company that actually works, you have to figure out how you can socialize that insane almost founder level obsession with quality and taste to the point where the team has it built into their DNA because they have so much power with AI agents with uh AI tooling to launch their own products to drive their own corner of the business. This might look different at different companies. Maybe you say it's at the department level, not the team level. But the point stands, right? Taste is something that shows up at a much more democratized level than it did in the pre-AI age.
And what's interesting is it's not just is this product good taste. It's taste in problems. Which problems are spicy that we should choose to solve? It's taste in problemsolving skills. Taste in learning methods. What I'm saying is you have to develop a sense of where the juice is in the profitability matrix of the organization.
Maybe the most effective thing your team can do is to scale up for the next 3 months and other teams don't need that but yours does. Maybe the most effective thing you can do is double down on problem space discovery and other teams are building product or maybe it's a more traditional definition of taste and you're working on an excellent product.
But the reason that matters is because the team has to have taste or the tooling they're using with problem solving with high um high ownership is wasted. Taste is effectively a fancy way of saying pick the right thing to work on and make sure that you are really really good at knowing what good looks like. That's taste.
When we talk about someone with high taste in fashion, they pick the right thing to wear and they know how to wear it so it looks good. very very similar idea and I think that that's something that we could previously delegate to just a handful of people to a tiny sort of collection of folks when IBM was at its height IBM had taste makers they were a group of 10 or 15 people who were licensed to break all the norms of the organization and they were licensed to do that by the organization so that they could introduce creative thinking well their their taste has to be democratized that idea does not work anymore more. We need to build institutions that socialize a sense of taste. And I do want to suggest this is not universal. Right? The way LLMs work is universal. The ability to learn to solve problems with LLMs is also a universal skill.
The sense of ownership is a universal skill. Taste is not. Taste is specific to your vertical. Taste is specific to your situation. Taste is more like context, which I mentioned at the beginning of this video. taste requires you to know your local domain very very well and have an excellent taste in problems. So there you go.
I think what we're really talking about between Wharton and MIT is institutional fluency. And I think the three keys are context and then the ability of teams to start to flip the traditional relationship between ownership and skills. Ownership residing now at the individual level, skills at the team level and then finally taste. I think that taste is something that we have to push down into our organizations and that's also new.
What do you think you're missing or I'm missing on AI fluency in institutions? This is an evolving field. I'm learning and seeing this in real time. What are you saying?

---

<!-- VIDEO_ID: XBG5tqrz3Rc -->

Date: 2025-10-27

## Core Thesis
The speaker argues that certain AI models, particularly Codex, possess an under-recognized and superior capability for high-level strategic planning and problem decomposition, offering significantly higher leverage than their advertised coding functions. This "transformative intelligence" for strategic thinking is often overlooked due to marketing narratives and interface barriers, despite its potential to democratize complex problem-solving.

## Key Concepts
*   **AI as a Strategic Planning Partner:** The highest leverage application of AI for complex problems lies in the planning and strategic decision-making phase, not merely in code generation or execution. This challenges the common perception and marketing of "coding AI" tools.
*   **The "Senior Engineer" Mental Model for AI:** An effective AI strategic partner operates at a high level of abstraction, providing well-structured options and identifying critical strategic questions before delving into tactical specifics, mirroring the approach of an experienced human expert.
*   **Counter-Productive "Bias for Action" in Strategic AI:** While often valued, an AI's tendency to jump quickly to specific solutions or tactical details (a "bias for action") is detrimental during the strategic planning phase, where broad options, foundational questions, and high-level considerations are paramount.
*   **Abstraction Layer Management:** A crucial differentiator for strategic AI is its ability to effectively manage abstraction layers  presenting high-level strategic options, laddering up to core leverage questions, and translating complex technical concepts into accessible language for diverse, non-technical audiences.
*   **Legibility and Intelligibility as Core Metrics for AI Partnership:** The true value of an AI strategic partner is measured by its capacity to facilitate collaborative thinking, producing outputs that are easy to understand, legible, intelligible, and shareable, thereby enabling effective human-AI cognitive partnership.
*   **The "Command Line Barrier" to Transformative Intelligence:** Powerful AI capabilities for strategic thinking are often underutilized because they are hidden behind less accessible interfaces (e.g., the command line), highlighting a significant user experience challenge for broader adoption of advanced AI.

---
Codex is hands down the best strategic thinking partner that I have found for technically adjacent problems. What I mean by that is that if I am facing a problem and it's not code, I want to emphasize that people hear codeex and they hear clawed code and they think that means I'm just going to talk about coding stuff. I'm not.
This is not for coding and engineering only. It is for anyone who wants to think intentionally about software systems or things you want to build. It doesn't even commit you to building in codecs. And we're sleeping on this because everyone has advertised all of these models that are built for coding as if they just do coding and they don't.
And I really wish that we had more people talking about the other stuff you can do. You can use clawed code for lots of things beyond code. Anthropic does that by the way. They use it for legal. They use it for marketing. They use it for HR etc. Well, guess what? You can do some really cool stuff with codec that is very distinct from claude code.
And what I'm going to show you in this video is a breakdown side by side of claude code and codeex and how they compare answering a tough technically challenging strategy question not a coding question because I find that that is actually a higher leverage use of AI than just coding per se. if if can help me to make smarter decisions that is going to pay off so much more down the road than just having it do the code and it's also way more accessible because most of the people in the world don't code and so if we just paint codeex as a coding model we're
missing out so without further ado let me show you what I mean I'm going to show you codeex and cloud code side by side okay here we are codeex is on the left cloud code is on the right on my screen. They each have been given the exact same prompt. If you're wondering how I got this installed, it's super easy.
I can give you a quick tutorial another time, but essentially, it's a two-minute install. You just open up a little terminal and you ask to install Cloud Code or Codeex with a special command, and that's it. It's super easy. So, if you're a non-coder, I know it seems scary to install things on the command line, but trust me, as someone who lived in the 1980s and did a lot of stuff with Windows and DOSs, it's not that scary. It's totally okay.
Uh here is the prompt that I gave them once I got in. Right, I'm in codeex. I ask, I'd like to think through a complicated problem. Can you help me lay out options and technical pros and cons, please? Specifically, I'm looking to figure out a multi- aent AI deployment. I want the system to one, triage incoming tickets in Jira filed by customer success.
Two, correctly assess whether or not the reported issue is a bug. Three, trigger initial code review if the bug is confirmed. and four begin drafting a pull request to address that bug. There are also going to be fail and degradation paths to consider. As you look at this, what strategic questions emerge that I need to answer to design the system effectively.
Exact same prompt over to claude code over here. I literally pasted it so it'd be identical, right? Both say they'll help me think through it. Here's the thing. Codeex is already winning because much more reasonable. It is headlined properly. I see possible approaches and there's three options that are really easy to scan.
I can take a tool augmented approach, an event-driven workflow or an agentic pipeline. Great. If I look over here, it says single agent versus multi- aent. It doesn't really give me as many choices. It gets into confidence thresholds really fast. Whereas codec stays really focused at the strategic layer, right? It says, "Okay, what are some component considerations you want to think through?" I feel like I'm talking to a more senior member of the engineering team when I'm looking at codeex over here.
Whereas when I'm looking at claude code, it just jumps right into this specific failure table and tries to explain what what the heck it means by these specific failure choices. I am not ready to get into specific failure modes. That is why I asked the system to think strategically. Only codeex figured that out and Codeex is here having an earnest conversation with me about how I should think about the components.
It's giving me ideas for degradation paths that are concise and easy to understand. Classification uncertainty is one. Model hallucination is one. This is so easy to follow. It is so easy to think through. It is so clear. And then it gives me a series of strategic questions around data, human in the loop, tooling, scalability, etc.
Fantastic. I already like this codec summary better. By the way, Claude just keeps going. Right now, Claude is drawing a entire sequential pipeline. Where is the order here? It's not that anything that Claude is proposing is on its face obviously incorrect. It is that Claude the agent in Claude code seems to jump to specificity really really fast.
It's eager. It has bias for action. And that is really unhelpful if I'm trying to think through a problem before making big choices. And by the way, the leverage in engineering and also non-engineering tasks in AI right now is in planning. That is why cursor launch planning mode. That is why so many people get frustrated with vibe coding tools because they jump into action too quickly.
Planning matters and codeex is extraordinary at planning. And I want you to look at this especially if you are not an engineer. This is super readable. What labeled history exists to train or triage accuracy? Okay, like that's plain English. They're asking, "Do you have a a history of tickets with some labels on them so we can understand what good looks like?" And in fact, if I want to, I can literally ask Codeex to restate that in less technical terms.
And it will watch down here on the command line. Could you please summarize this for a non-technical 12th grade reading level? present options and strategic questions clearly and concisely and it's just going to go off and do it. In the meantime, I want to call out that it successfully laded up the highest leverage questions when I asked it to.
So, I asked it to ladder up the highest leverage questions. Super readable. It asked about automation boundaries, quality and risk metrics, governance, operational resilience, and investment. These are all correct questions if you're designing an agentic system. Super helpful again. But I asked the same thing to Claude.
See, ladder this up into three to five highest leverage questions and what I got this is like a a dock, right? And it's not even this like how much are you willing to spend on false positives and false negatives? That is not a strategic level question. That is a tactical level question. Claude is missing the point here.
And it just goes on and on and on. Meanwhile, Codeex answers in a few lines and then gives you the non-technical summary. It describes an agent as a central coordinator that hands each ticket to specialist bots. That's correct. It describes an event-driven setup as another option where bots react to ticket status.
This is basically a way of translating code to non-technical people and codeex lets you do it. People are sleeping on this. I really wish people would understand how much you can do with these systems, but they're just locked in a terminal and they get scared and they don't do it. Like if I asked this to to Claude Code, I would be curious to see what happens.
I suspect Claude Code is going to be much much more worthy, but but we'll see. In the meantime, if you look over at the codec side, this is going to give you what automation boundaries mean. Which step steps stay automatic? Where should humans get involved? Again, this is something you could explain to a non-technical CEO and they would understand what you mean.
Codex is able to efficiently lad up strategic decisions and strategic thinking in a way, yeah, see Claude Co just did that. But Codex is able to lad this up in a way that's really accessible. And Claude Co comes back and it's gosh it's so long, right? I asked it to write concisely and it did lad it up non-technically. It's harder to read. It's longer.
It gives you its opinion on risks in a way that's not super clear. This is so much better. I feel like we are missing the value that codeex is bringing because it is buried in the command line. And look, there are moments when I like claude code. I have said before that claude code is useful as an agent on a loop where it comes back and it gives you more options and you can work with it over time.
Codeex has some of those loop-like qualities, but it tends to like more structure. So far so good. That is a helpful distinction for coding tasks. For people who like to load context windows up and code, using codecs is great because they can give it a goal and give it a context window and off it goes, especially if they're solving really hard problems.
For people who are more iterative um who want more of a conversational approach, cloud code can be very helpful. This is not about that comparison. What you are looking at is intellectual capacity. Can the system think with me on a hard technical problem in a way that is easy for me to understand that is legible that is intelligible that I can share with others.
The answer is that Codeex could absolutely keep up and I love it and Claude Code is just not there right now. And it's it's not close, guys. It's not close. I got to be really honest. I did not start this video assuming that we would have only one winner. It was when I started to put the terminals together and say, "Wow, there's a huge difference.
" That I realized I needed to share this. Codeex has a secret talent here that we are sleeping on. And so if you have not tried Codeex for strategic thinking, I encourage you to do so. It is not close. It is the best strategic thinking partner out there for technical tasks. Don't sleep on it. And don't be scared by the command line.
If you're new, I'll include a whole guide to installing codecs off the command line so it's not scary over on the Substack. And I'll include some other sort of starter questions, ways that you can think about using codec strategically for non-coding purposes. I want people to access the intelligence they can use to do their jobs better.
And this is transformative intelligence. It's just hiding in the terminal. Let's free it up. Let's use it. That's my sideby-side comparison of Codeex and Claude Code as strategic thinking partners. Hope you enjoyed it.

---

<!-- VIDEO_ID: zw39KBZkPeA -->

Date: 2025-09-10

## Core Thesis
The widespread perception of AI project failure stems from a fundamental disconnect between executive-level ROI metrics and the pragmatic engineering realities faced by builders. True success lies in adopting a builder-centric approach that prioritizes iterative learning systems, intelligent friction, and robust technical instrumentation, effectively formalizing organic "shadow AI" use cases into hybrid architectures that deliver tangible, measurable value.

## Key Concepts
*   **The "Executive-Builder Disconnect":** The primary reason for perceived AI project failure is the gap between executive understanding (high-level, ROI-focused, binary choices) and builder realities (technical patterns, iterative development, nuanced solutions).
*   **AI as a Learning System, Not a Static Deployment:** Successful AI integration requires building continuous feedback loops, persistent context, and retraining pipelines, treating AI as an evolving entity that adapts to enterprise workflows rather than a one-time installation. This challenges the "set it and forget it" mentality.
*   **Intelligent Friction as a Design Principle:** Counter-intuitively, embedding "smart friction" (e.g., confidence thresholds, human review gates with adjustable parameters) enhances system robustness, builds trust through transparency, and reinforces learning, moving beyond the simplistic goal of maximum ease-of-use.
*   **Leading Technical Indicators for ROI:** Builders should focus on actionable, technical leading indicators (accuracy, latency, error rates, override metrics) rather than solely relying on lagging, executive-defined ROI or vanity metrics like adoption, enabling proactive problem-solving and clearer communication of value.
*   **Shadow AI Mining as a Source of Innovation:** Identifying and formalizing existing "guerrilla AI" use cases within an organization (unofficial tools, shared prompts) reveals high-value, user-driven needs that can be systematized into robust workflows, bridging the gap between informal hacks and enterprise solutions.
*   **Hybrid Architectures over Binary Buy/Build:** Successful AI solutions rarely fit into a simple "buy" or "build" dichotomy; they typically involve hybrid architectures that combine best-in-class models with custom workflow logic, acknowledging that even "buying" an AI solution still entails significant integration and adaptation work.
*   **Context Management as a Core Engineering Skill:** For non-deterministic AI models, effective context management (e.g., clean, relevant data input vs. "kitchen sink" approaches) is critical for differentiating between bad and good work results, requiring a new blend of engineering and data science skills for instrumentation and optimization.

---
In the next few minutes, I'm going to save you at least six hours of work. I'm going to help you turn your prompt mastery, let's say you've been following my videos, you feel like you know prompting into a recipe for organizational AI success. What does it take to go from being a prompt ninja perfecting chat GPT prompts, Claude workflows, cloud artifacts, whatever it is, chasing every new feature announcement.
Claude code interpreter came out this week. At the same time, you're in a world where your company or your team around you are not on the same page. Even in AI first businesses, there is a wide range of adoption patterns of AI. There are some people who are still using those manual workflows. And so, you see that your company's AI pilot stalls out.
Budgets, they might get slashed. Executives will say AI is a fad or say that things aren't working or that they aren't seeing return on investment. Here's the secret that isn't getting told enough. The individual prompt mastery practice you're doing doesn't scale. But the secret is not just get your leaders on board with AI. To be in the 5% who succeed, you need to figure out as a builder, as someone who may not even be a director or a VP, how to level up your personal AI hacks into something that is a system of learning, something that can help your team
deliver business value. And that's what we're about today. Why am I talking about this now? Because the number one study circulating on the internet right now is the infamous 95% fail study. It is a study published in August 2025 by MIT reporting that 95% of enterprise AI initiatives deliver zero measurable ROI within six months.
95% that's based on 150 plus executive interviews and 30 to40 billion dollars in represented AI spending. It sparked global headlines. I can tell you that the Google first search page is like all disaster headlines. It's just all everything is bad. LinkedIn doom loops people sharing the headlines, not reading it.
Very few people have actually read this study. This is part of how I'm saving you time. All of this surface level narrative overlooks some of the key nuances that separated the winners and the losers. And I did the digging so that you don't have to. Number one, nobody is talking about this. The frame for this study is mostly incorrect.
This study is asking executives what builders are doing. And anyone who has worked in a business will tell you executive pictures of AI adoption and AI fluency differ dramatically are not the same as what builders on the ground are doing. And that's why I am talking in this video to you. If you are building, if you are prompting, if you are excited about prompting, if you're a founder, a solo builder, whatever it is, you have a chance to change this narrative.
And I'm going to give you specific principles that popped out from hours of study looking at the MIT study, the people talking about it, everything else. First, what did the internet get wrong about this reaction? Then we'll get into how we dig in further. Number one, the executive panic is incorrect. We got executives saying, "Is AI a bubble?" We got stock crashes.
We got boardroom jitters. It's just it's not the right focus. It misses the nuance and the detail. We got methodology debunking. I don't want to go into it. There are entire subreddits dedicated to debunking this particular study and saying you can't draw big conclusions because it's such a small study and interviews and this and that.
Look, I understand statistics. I could go there. You don't have time for it. I don't have time for it. We're going to save the time. Let's just say maybe the study is flawed, but there's a lot we can learn and we don't have to worry about it. There's a lot of copypaste journalism. We're not going to waste time on that.
And there's a lot of really binary conclusions. One of the things that they came out with is a opinion on the binary buy versus build where the MIT study basically said you were much more likely to succeed in your AI pilot if you bought versus if you built. It's kind of convenient that they're saying that because the people running the study are selling something.
So yeah, I'm shocked that they came out with the buy. But but wave that aside, let's say they have good intentions and they got that and they're honestly sort of presenting what they see. It's still oversimplified advice that misses the realities I see diving into organizations daily and so it's still wrong.
So let's dive in and let's figure out what the real takeaways are so you don't have to spend 6 hours staring at all this stuff. Number one, the MIT study actually measures only profit and loss focus over a 12 to 18month period. That is it. It is a very narrow measure of success. It's the first thing to be aware of.
Number two, as I said, it only talks to execs. Number three, it only gives execs a buy or build. It's sort of a very binary conversation. We talked about that. Number four, it talks a lot about workflows, but is too high up because they're talking with execs to have specific guidance on how to build those workflows in ways that actually work.
This is where we're going to start to close the gap and give you takeaways that matter. So, what actually drives success? Let's turn around and say, what if you want to be in the 5%. and you don't have the power of a vice president and you're you're a prompt expert, you're a team influencer for AI, maybe you're uh a leader on your team, how do you actually start to drive success? I want to suggest to you that we builders know technical patterns that come up again and again and again as success indicators that did not show up in the MIT conversations because execs aren't
aware of them. So, let me name them here and let me share them because so many of us are discovering them by running into them. We're like, we've got our eyes closed and we're feeling around in the dark and we're discovering these principles. Let me just name them so that we all are talking off the same page. Hybrid architectures matter.
We didn't have that choice in the MIT conversation. They didn't write it up. Every single time I have talked to businesses who are actually implementing AI, they have hybrid architectures. They are combining best-in-class models with custom workflow logic. They're not just doing a roll your own versus a buy. They're actually taking the best of both worlds and they are recognizing the work it takes to do that.
That there is no such thing as a free lunch. One of the things that the MIT study missed is that even if you buy, you are buying work. And again, executives don't see that. Number two, learning systems is how you should think about installing AI. You want to build feedback loops. You want to retrain your pipelines.
You want to have context persistence. It's builders that understand this and that are stumbling into it. When I wrote my guide on rag, when I wrote my guide on chunking data, it's all about how you start to take the data you have in the business and surface it and make it available so that AI workflows can actually use it in feedback loops that allow the business to learn and get better.
next time at completing tasks that matter. This is one of the key findings from the study, but it was never pulled through. They never figured out how to pull it through because again, they were talking to execs. So, the generic conclusion of the study was, "Oh, yeah, it would be good if AI was able to adapt to enterprise workflow realities.
" Yeah, I mean, you know, sliced bread is cool, too. Isn't that great? The the reality is the only way you get that done is by actually building feedback loops with persistent context and being willing to retrain your pipelines until it works. This is hard work. When I talk about even at an individual level, let alone a team level, what it takes to have an agentic workflow, people get this big heavy site.
They're like, "That's a lot of work." And I'm like, "Yeah, that's why you should pick problems that matter because you're going to have a lot of work. you're going to have to put your shoulder to it and you're going to harvest the value disproportionate disproportionate ratio if you have a better goal. And so if you have a goal that's big that's audacious and you want to learn a lot and you want to break through for the business, let's say that your goal your your choice is between a rag system for an HR policy manual and a rag system that allows you to maintain
context across deals for the sales team. Pick the sales team option. You put in the same amount of work either way and you get so much more value off the sales one. Build learning systems that matter and that are aligned to your goals. Number three, this one almost didn't get talked about, but the study did mention it.
It said that building intelligent friction mattered for successful organizations. People think of these systems as like you want to make AI as easeful as possible. That's not true. You want to embed smart friction. And I'm going to give you specifics and the study didn't. You want to embed confidence thresholds. What if you were able to show in a printed out response from an LLM in red or green or yellow, what is the confidence the LLM has in the token it's presenting? So you can see low confidence tokens that might indicate hallucinations. What if you had
human review gates where humans could go back and retune and they could say, you know what, I want a more aggressive LLM pass. I want a less aggressive LM pass. I have sliders to tell it how to adjust. I don't just have a yes or no. Is it more friction? Yes. Is it something that's actually going to help you build a more useful system long term because that friction is smart and reinforces your learning system? Also, yes.
Again, not something the MIT study jumped into because they did not talk to builders. Instrumentation. You want to be looking really carefully at your accuracy, at your latency, at your error rates, at your override metrics. The reason I am saying this is not because I want you to invest in metrics.
If you haven't built a system, I want you to invest in making sure that you know whether the model is solving a meaningful problem and what is the quality of the solution it is proposing. And the reason I think this matters is that if you let execs determine ROI as a measure, that's fine. It's down the road and you have no leading indicators.
Instrumentation is a way to get actionable leading indicators that let builders actually drive success for AI projects in a way that you guys can then report up to leadership. And if we don't talk about that a lot, if we don't talk about instrumenting these projects, we're going to let leadership dictate what success looks like and we actually have a chance to influence that.
Now, if you're wondering what does instrumentation look like besides like the technical ones, I will say it is more useful to be able to agree with leadership on a general goal and problem you want to solve. Show the problem is being solved well and then show the direction for how to extend the solution. than it is to talk about vanity metrics.
And I think one of the most persistent vanity metrics is adoption and time saved. You'll notice I didn't mention those ar those aren't technical metrics. And what I find is when execs see that, they think that builders like you and me are trying to position success outside of ROI. Whereas if they start to see technical metrics, you have to explain them so they understand what they are.
But once you explain them, no one mistakes them for the end goal. And that matters. Another principle that really matters is that this one doesn't get talked about at all. MIT didn't get to it. I think it's really important to actually be in that successful AI category. Shadow AI mining. You need to be the one that formalizes the gorilla AI use cases your team depends on.
Is there like a GPT you're passing around that works? Is there a use of perplexity that works? Whatever it is, if you can mine the shadow AI for behaviors that work, and by the way, product managers, this is a hint for you. If you're in the B2B space, your customers have shadow AI use cases. If you can get that out of them and build for it, that is gold.
That is gold. So, do some shadow AI. Formalize those use cases and see if you can build them into actual workflows that work. You're going to make it happy. you're going to make if if you're building a product for B2B, you're going to make the business happy because it's going to drive value.
Look for the AI that's happening in the shadows that again execs aren't going to be aware of. So, you've you've seen some of these things. You've seen some of the principles, the technical elements that drive success. I want to transition now to skills. If you want to translate individual contributor skill sets, the things that make you successful as an AI builder with AI into influence, these skills actually map back to some of the principles I just talked about. They're par.
This is not a whole new set of things to remember. You can do the shadow AI detective work and then you become someone who's known for systematizing workflows in a way that brings AI out of the shadows and into the business. That's influence. You can be known as someone who can engineer guard rails that build trust through transparency.
Doesn't that sound good? Well, guess what you're doing? That's friction design, right? We just talked about that. You can become known as someone that designs AI products that improve with each interaction. That's learning system architecture. Again, a way to develop influence from those same sets of principles I just gave you.
You can learn how to show the connection between engineering KPIs like accuracy and business ROI. Suddenly, you're known for technical health monitoring of AI systems. That sounds like influence, too. You can learn to develop prompt libraries and templates that are tailored to diverse team needs and architect those libraries in ways that enable other people to jump onto them.
That's an example of context translation and it is part of the hybrid architecture that shapes that shapes good AI systems. Let me give you just some examples of how you can go forward. How you can be the one that takes this 95% study, turns it on on its head and says, you know what, this is a study for builders and no one said so and we can actually be much more influential.
Product managers, you can run shadow AI surveys. You can build features that are better with guerilla workarounds. You can figure out where to put intelligent friction in your products. You can figure out how to communicate instrumentation to execs in ways that they can understand that give you leading edge indicators so they won't just go to an MIT study and say, "Well, there's no ROI.
" Solo founders, entrepreneurs, you can focus on narrow workflows that allow you to customize to the business. If there's anything you're hearing here, hybrid architectures work. Businesses that buy AI solution are taking a tremendous amount of burden. If you focus narrowly on workflows you can tailor, you can deliver deep value and you can help lift the load for those businesses and you will earn trust and that will enable you to go after adjacencies over time.
Engineers, you can get good at instrumenting. And I realize by the way that this is a new skill set. There is a degree to which instrumenting in AI is different than any other kind of instrumentation. Arguably one of the biggest skill sets engineers need to pick up is data science. Like how can they start to bring more data science into their instrumentation because these are non-deterministic models and it's complicated to measure them.
You can learn, you can scale up, you can do your best to instrument, you can do your best to automate. And in particular, this is an area where I think engineers can lead the way in the company. You can lead the way on context management and what it means for the business. You can lead the way on explaining why the difference between a bad work result and a good work result can be as simple as clean context.
Is not putting the kitchen sink of the wiki into the prompt and wondering why it doesn't work. UX designers, there is so much here for you if you're anywhere in the design space. surfacing confidence scores, figuring out how to help people who are doing QA loops do so intelligently, offering people great override options, figuring out how to take guerilla workflows and formalize them without losing the value.
Security and compliance, like if you're working in that space, you definitely want to be auditing shadow usage. And you can only do that if you earn trust. You definitely want to be on the forefront of explaining how hybrid architectures can actually be more secure by speeding value and like pushing the shadow IT footprint to the edges and speeding the value of the business getting the install done.
You can figure out where to embed friction for sensitive approvals. There's so much here. I'm just giving you a few examples. You can fill in the rest. Your competitive advantage is that you know the prompts. You may be digging into the APIs as a builder. you know the hidden workflows on your team that work that really work the exacts don't this playbook I'm giving you here this this open door into this MIT study what went wrong with it where we need to actually build this is going to help you bridge the gap between your personal mastery
your sense of achievement and how businesses actually get done I guarantee you if you start to think about how to build hybrid architectures how to build systems that learn how to have intelligent friction how to think about buy versus build not as a binary tradeoff. How to think about instrumentation that leads to good ROI outcomes without just measuring the dollar and without just cheapening out and measuring adoption.
You are going to be so far ahead of most of the individual contributors like 99% of them and it's going to give you options to drive good outcomes for the business. I don't know what your career goals are. Maybe you're looking for a promotion. This sure seems like a pathway there. Maybe you're looking to just extend your influence and you want to avoid a promotion.
You can probably dictate your terms if you do something like this. This is a skill set. What the MIT study missed is that the people with the skill sets to do this are developing it on their own and reinventing the wheel time after time after time. And I see this and I want to lay it out in the open. These are principles I see being redeveloped over time that the MIT study missed. They are reasons for success.
So learn from these principles. Don't reinvent the wheel. Know that other people are struggling with you and that if you see a headline like that, you have a surprising amount of influence if you are not in leadership to change the outcome of the business that you work with. You are not powerless. You can build in ways that avoid that 95% headline failure outcome, which by the way, I don't even know that I believe 95% is correct. It's another story.
I hope this has been helpful. I hope you see some of the pathway forward to go from individual prompt mastery to something more to something that enables you to influence the business. Let me know what you think. Pop it in the comments.

---

<!-- VIDEO_ID: _Z-YppWti1E -->

Date: 2025-11-19

## Core Thesis
The emergence of highly capable multimodal models like Gemini 3 fundamentally redefines AI strategy, shifting from a monolithic "best model" approach to a specialized, workflow-driven ecosystem. This necessitates a re-evaluation of organizational structures, skill sets focused on high-level specification and review, and a strategic reallocation of human cognitive effort towards tasks involving complex visual and large-context data previously inaccessible to AI.

## Key Concepts
*   **Workflow-Specific Model Routing:** The strategic unit is no longer a single "best" model, but a dynamic routing layer that selects the optimal AI based on task type (e.g., "see/do" for Gemini 3, "write/talk" for Claude/ChatGPT, "cheap bulk" for smaller models). This challenges the common "best model" narrative.
*   **Expansion of AI-Native Territory:** Multimodal AI transforms previously "dark" or "silent" data zones (e.g., raw UIs, video footage, large codebases) into legible inputs, unlocking entirely new categories of AI-driven workflows like UI debugging, design QA, and video research.
*   **Shift in Cognitive Bottleneck to Specification & Review:** As AI handles execution, human value shifts from "keystrokes" to high-level intent articulation, critical evaluation of AI artifacts, and iterative refinement, akin to collaborating with a skilled colleague rather than mere prompt engineering.
*   **Reallocation of Cognitive Taxes with Context Abundance:** Large context windows don't eliminate cognitive load but reallocate it from meticulous data preparation to sophisticated "query design" and structured output definition, emphasizing the quality of the question over the cleanliness of the input.
*   **Visible Safety as a Design Principle:** AI safety is moving beyond policy documents to become an integral, user-facing part of product design, emphasizing human-in-the-loop review and explicit approval flows (e.g., draft for approval, diff review) to ensure human oversight.
*   **AI Operations as a Dedicated Organizational Function:** Managing a multi-model AI ecosystem (routing, prompt maintenance, internal education) requires a dedicated "AI platform group" with a specific charter, rather than being a side project or a task for a single "champion."
*   **Multimodal vs. Textual Model Ergonomics:** A key heuristic for model selection is distinguishing between "eyes and patients" tasks (visual, complex, large-context processing, where Gemini 3 excels) and "voice and keyboard" tasks (conversational, persuasive writing, where models like Claude or ChatGPT may still be superior).
*   **Counter-Intuitive Model Evaluation:** Judging advanced multimodal models solely on chat-based interactions leads to an incorrect assessment of their true capabilities, which lie in accelerating previously "off-limits" complex, visual, or large-context work.

---
Gemini 3 came out and it is the number one model in the world. What does that mean for all of us and what does that mean for particular jobs like product manager, engineer, marketer? I'm going to get into both of those in this video and we're going to start with the overall takeaways. Number one, the unit of strategy is no longer the model.
You should not be asking which frontier model is best. And I realize that's ironic because we're talking about Gemini 3 as the number one model, but really what you should take away is that Gemini 3 makes it unavoidable to ask which model is best for which workflow because it is clearly a lot better at some things like video screens, handling huge context, and it's not as obviously better at others like persuasive writing or everyday chat.
So the implication is if you're still arguing and saying we're an open AI shop, that's all we do. Or we're an anthropic shop, that's all we do. You're kind of missing the plot. Someone in your org needs to own the routing layer. And I want to suggest a very, very cheap, easy, usefully incorrect abstraction for you. Every abstraction is incorrect.
Some of them are useful. I think this one is useful. If it is a see or do task, think about Gemini 3. If it is a write or talk task, think about claude and chat GPT. If it is a cheap bulk task, you got to go with some small flash models. Is that going to work for every single thing? Absolutely not. Is it a nice handy abstraction that you can work with? Yeah, it fits on a flash card.
Takeaway number two, Gemini 3 turns AI silent zones into AI native territory. There are places where AI has been silent in the past. That's no longer true. Let me give you a few examples. Before Gemini 3, a lot of high-value surfaces that we computed with were effectively dark to AI, right? Raw user interfaces and dashboards.
We didn't always get great results coding them. We didn't always get great great results designing them. We didn't always get great results figuring out what they said, right? The being able to analyze them. Long messy video definitely was dark to LLMs. Giant piles of code with docs and screenshots. We are making progress there.
There's definitely examples that I've seen with cloud code and codecs, but it's not necessarily a super easy space for most AIs to operate. You needed humans to try and digest some of that long and messy context and summarize it before an AI could do anything useful. So, Gemini 3's real unlock is that those surfaces are starting to become legible.
Gemini 3 can read the UI directly instead of guessing from the logs. Gemini 3 can watch footage instead of just reading transcripts. Gemini 3 can digest much bigger chunks of everything related to this system at once. So the most interesting new new workflows won't be better chat. There'll be new places you can use AI that you couldn't before like UI debugging, like design QA, like maybe admin panel automation of some sort, maybe figuring out how to do video research or user testing.
So a good question to ask each of your teams right now or ask yourself is where do I have a lot of eyes on the glass work today? Gemini is probably more relevant there. Takeaway number three, the hard skill now is specification and review, not figuring out the keystrokes. So models are getting better and better at doing and the bottleneck is starting to shift toward telling them what to do and deciding whether that's an acceptable choice.
I think that Gemini 3 plus the new anti-gravity code editor makes this very literal because in anti-gravity agents propose terminal commands, they propose code diffs, they have browser actions, and you approve or reject their artifacts, their plans, their patches, their refactor proposals. That's not really prompt engineering in the sense that it gets made fun of.
It's much closer to working with a colleague to write a runbook, to design a spec, to do fast and highquality code. I'm not here to tell you that this is the only way to develop. One thing I know having worked with engineers for a couple of decades is every engineer has a stack that feels ergonomic to them.
Some are finding anti-gravity really compelling and easy. Others are preferring to stick with cursor, are preferring to stick with codeex or preferring to stick with claw code. All viable AI options. The thing I want you to know, regardless of which you prefer, is that anti-gravity is shifting our sense of how we pay attention in coding in ways that we all need to understand, even if you're not a coder.
Because what anti-gravity does is it dares you to focus on where you need to intervene with an agent that's building something rather than to focus you on the code side of things. And we have seen glimpses of this in the direction that cursor is evolving. But anti-gravity really really leans in. And I think that this implies that a lot of the great work that we do going forward is going to look weirdly similar for great product managers and great tech leads because it's going to to be work that is done by people who can describe what they want built really really
clearly and who can smell a bad artifact really really quickly. That is absolutely a vibe thing but anyone who has worked around code will tell you it's true. And so really, you should evaluate how you want to work with Gemini less in terms of its ability to purely write code and more in terms of your ability to articulate intent, see useful results, and your ability to quickly refine and review.
Increasingly the models will get there on the code that needs to be done but you need to be the one who is given space to review, refine, pay attention and decide what's acceptable. The models and the interfaces that make it easier for you to get your hands on the work and decide what's acceptable are the ones that are going to win.
And so I think anti-gravity is an interesting development in the AI landscape for exactly that reason because that's where Google is focusing you. Takeaway number four, context abundance is just going to change where you pay your cognitive taxes. So a million token context window and very strong retrieval does not mean hey dump in your knowledge base and go to sleep.
It does shift where you spend your effort. So you spend a lot less time curating perfect little packets of context, but you're going to spend a lot more time deciding what is the shape of the question that is worth asking. How do I want this answer structured? Gemini is now good enough that the marginal return on another hour of cleaning the context window is often lower than the marginal return on a better question and a better output format.
And the implication is pretty stark. you need to start thinking in terms of query design and not just data preparation. So as an example and I know not every repo is this small but given that we can throw in a chunk of the repo and docs what is the most valuable question to ask as an engineer or what structured artifact do we want back here? Do we want a diff? Do we want a table? Do we want a synthesis of the data in some fashion? Do we want a solid six pager? What is the output? Teams that are excellent at asking sharp
questions and at defining outputs are going to start to run ahead of teams that obsess over shaving a little bit of noise out of the context window. Takeaway number five is that safety is becoming a visible part of the user experience. This is not a policy PDF anymore. Anti-gravity is designed around the idea that safety guard rails need to be visible.
So the whole idea of draft for approval flows, the clear separation between suggestion and execution, the ability to review the plans of the agents, the ability to view diffs really cleanly in anti-gravity. Essentially, Google is putting their money where their mouth is and saying that they want design of our surfaces to reflect the need for humans to be deeply engaged with what models should and shouldn't do.
And I appreciate that because I think we need a lot more work in that direction. We need more user interfaces that help us to put our hands on what the models are doing. Takeaway number six is actually for us and for our teams. AI operations is becoming a fullfledged headcount function. It is not a hobby job. And so once you start to accept the idea that some tasks go to Gemini, some tasks go to Claude, some tasks go to chat GPT, who maintains that? Who maintains the prompts? who maintains the tools and the artifacts, who teaches teams how to work with these
different layers. This is part software engineering, part product management, part platform team. We're still evolving what the role means. But fundamentally, if you think one staff engineer who's a champion on AI can just do this, you're probably underinvested. One very reasonable 2025 move is to explicitly charter an AI platform group and give them a mandate around how they handle routing, how they handle internal education, how they handle shared prompts.
give them a charter that is big enough that they can evolve the impact of AI across the organization because these models are going to keep getting better in specific areas and you need a team that champions moving workflows where it makes sense. And I'm going to get into the job functions in a second and start to give you a few hints as to where I see that happening with Gemini 3.
Takeaway number seven, your intuitions about this model. And I will go so far as to say almost any model from here on out are almost certainly incorrect if you only test chat stuff. So if your lived experience with these models is biased toward writing emails or just answer me this question or very light coding or just summarize this doc quickly, these are exactly the areas where Gemini 3's advantage is the least visible.
So if you poke around in chat for an hour and conclude it's not that different, you're not wrong. you're just looking in the wrong place. So, I would suggest to you if that's you, don't judge Gemini 3 on your first 10 prompts. Instead, ask yourself, does this give me the ability to imagine accelerating a piece of work that used to be off limits? And I'm trying to go through these takeaways in such a way that you can open your imagination and see some possibilities.
Okay, now it's time to get into takeaways for job families. We're going to go job family by job family and I'm going to lay out where I think Gemini 3 has an opportunity to help uh and maybe where there's some nuance and maybe where Claude or Chad GPT should still be on the list. For product managers, you can now treat UX and video artifacts as first class inputs and not homework you have to watch to get into the AI.
This is a big deal because it simplifies a lot of early discovery and user experience. You can ask Gemini 3 directly for opinions on these artifacts in a way you couldn't before. You can ask Gemini 3 for competitive analysis across raw input data on an app video recording. Now, I'm not here to say that Gemini 3 is the only thing you should be using narrative prder documents emails where you want maximal clarity.
I would still stick with claude particularly set 4.5. I don't find that Gemini 3's persuasive writing is there yet. For marketers, you have a lot of really interesting workflows that open up as well. Similarly, in the video and visual space, you could ask things like, "What patterns do you see in our winning Tik Toks? What's visually different between our high click-through rate and our low click-through rate ads?" And you're going to get really structured takes that you just would not get from AI before. And so, post hawk creative
analysis is really interesting. you have the chance to do some creative asset audits that you didn't have before. But again, I'm going to say I don't think it's going to be as easy to get brand voice, especially punchy brand voice out of Gemini 3. On the customer support and op side, think about tickets with screenshots.
Now, not just tickets as strings of text, right? You can actually look at cluster these issues by what's broken on the screen, right? You can take a screenshot. you can look at tickets and Gemini 3 can put that together. Again, AI couldn't do that before. And so if you want to do something around an automated triage workflow, you want to tag parts of the UI to places that are broken in your customers supports, if you want to draft actions on admin panels and like play around with aic workflows, all things that Gemini 3 would be interesting to
explore for. What stays on Claude or chat GPT is going to be that text piece. Again, I would actually lean on Claude for that. Chat GPT even after 5.1 is not as easy to work with. Sales, you want to think about call reviews here. How can you think about slides, faces, body language in a more structured way and not just feed AI transcripts? How do you start to think about really heavy lifting with Gemini 3 on RFP compliance or on contract comparisons or on video call analytics? You can do things like say summarize this 60-minute discovery
call and white paper for my next meeting. Stuff like that is becoming possible in a way that it just wasn't before. What stays in claw or chat GPT? Cold outreach, follow-ups, LinkedIn messages. The conversational style layer is again not really there. Are you seeing a pattern? Executives and leadership, there's some really interesting takeaways here.
You can ask where is there a difference between what the deck is telling me and what the raw KPI tables are telling me. I know a lot of execs who would love that one. Uh where is and by the way that is something that if you are presenting you should assume your exec will now be asking. You can ask how do I digest a large mixed packet like a board deck with annexes with screenshots with uh a whole set of data tables? How can I make this digestible as a single object with really good synthesis? Gemini 3 is good at that. Gemini 3 also makes
presentations. I find that the visual style is quite creative. The narrative piece again is not quite where Claude is. front-end engineers. Look, UI state and visual bugs are now something the model can see. It's a massive breakthrough. The model is also much easier to push out of that blue purple convergence that it's been stuck in.
And so visual debugging is easier, design QA is easier, accessibility QA is easier. Now, if you're doing some of the simple bug fixes, some of the simple tweaks, it doesn't really matter what model you're using. And if you're looking at what is your overall day-to-day model on front end, I think you are going to have to start to code up parallel projects in Gemini 3 codeex and clog code and see where the model feels ergonomic for you.
I'll say it again with engineers, the fit of the model is a personal thing. And so as much as I can say, look, the model is better at seeing bugs and you should use it for QA, your daily coding driver is something that in part depends on your degree of comfort with how autonomous the model is and how often it checks in and how much code it burns and how many tokens it burns along the way.
And so you need to test it and decide if it's worth switching from codeex or clock code. All I will tell you is you are probably incorrect if you're unwilling to test. I think it is worth a shot. Backend and platform engineers, you can now productively ask, here's the whole service, the code, the configs, the runbooks, the diagrams.
Help me reason and think about this. And you don't have to elaborately shard the context window unless it's very very large. And so terminal terminal agents and sort of the way you engage with assistants are beginning to evolve for you. You can start to actually have a assistant that you supervise. And I felt that when I started to play with anti-gravity and we are starting to get that a bit with codeex as well and with cloud code.
So this is very much something where we should expect the model makers to continue to push. The thing that I will call out is that it is handy to have the large context window and it is worth it to ask yourself if that large context window is something you need for a particular debugging task. I have less determined opinions here on debugging on backend.
It may well be that codeex is still very very strong at debugging complex code bases. It just for lack of a better term, there's a special smell to it and it's solid. Just as for lack of a better term, there's a special smell to claude code and the way it can work within an ecosystem of skills and MCP and write good code.
Those are both strengths and that's why I keep coming back to its ergonomics. You'd be wrong not to test it. It's going to be a matter of fit for you on the coding side. For designers, this is absolutely revolutionary. The model can critique, it can compare, it can spot inconsistencies in UIs, it can see, right? You can feed it screens.
And so, if you are not using Gemini 3, you are absolutely missing out as a designer. It's big. This model is also going to help you to translate visual intent into code ready descriptions for engineers. And so being able to say this layout is whatever it is technically is something that Gemini 3 can really help you with because it can see the design.
For data analysts, the boundary between data in your dashboard and data in your documents keeps getting thinner because you can treat screenshots and PDFs and CSVs as one blob of evidence and ask for conclusions and it can be one big conversation. Right? A quarterly or multi-report analysis might stay inside one context window and not be spread across a dozen chats.
So having that exploratory analysis is really helpful. I think I would not use it to substitute for SQL. I feel like I have to say that. I hope you know that that's obvious. If you want it to start to draft SQL for you, it and Chad GPT and Claude are all going to do that. Well, if you want it to write Pandas code for you, it will do that, but so will Chad GPT and so will Claude.
Really at that point it's just about tight code feedback loops and it's very table stakes. If you are in the video space it is required like you have to start working with this model. This model can be helpful in suggesting how long footage can be turned into candidate timelines that you can then refine in a cut. It can help with pacing. It can help with rough cuts.
It can help with show me the good hooks in this recording. There's all kinds of things it can help with and we are just scratching the surface on this. Video is one of the places I'm most bullish on for Gemini 3. AI enthusiasts and vibe coders, you get to play with agents that use an editor, a terminal, and a browser together without building a specific harness to do that.
That is by itself a big deal. And so that means that we are going to start to see small admin tasks and small personal desktop automation tasks get interesting. And we're going to start to see frameworks for that. And there's going to be a whole lot of build around that. And so Gemini 3 fits in a world where you're tinkering with environments like anti-gravity.
It fits in a world where you are building proof of concept workflows. If you are still looking for the polished website that you can launch quickly with a minimum of fuss, lovable.dev is great. If you are still looking to do a comprehensive review of an ecosystem with markdown files and touching all the files on your computer and you have your cloud code all set up to do that, Gemini 3 is going to have a high bar to climb, right? It may be more intelligence, but it's it's a brain in a box and you have the hooks from MCP and you have the tools that you
need with cloud code and you don't want to touch it. Fair. I would say try it and see what you think. If you're using codecs, codecs may have the power that you want from a debugging perspective, and you may not feel that you miss the planning and review and agentic thinking that anti-gravity lets you do. Try it.
You'll see. I'm not saying you'll like it. I'm not saying you'll hate it. I think it's worth a try. This gets back to the engineering side where people get comfortable with cloud code. We get comfortable with codecs. And that comfort in and of itself drives productivity. And so I want to be careful, but I want to suggest that you should at least give Gemini F3 a try, a fair shake, and see how it does.
If we zoom out across all of these job families, I think we see some pretty consistent patterns. Gemini 3 is for the work that you do with your eyes and your patients. Claude or chat GPT tend to be for work that you do with your voice and your keyboard. And so one of the simpler questions I would encourage you to ask is where am I stuck watching, scrolling, clicking, and reading for hours and I I just need to understand what's going on.
Those are great Gemini 3 candidates. So summing it all up, Gemini 3 is beyond the benchmarks a fascinating push for all of us to start to think intentionally about where our workflows are focused on seeing and doing versus where our workflows are focused on talking, where our workflows are focused on writing.
I think that we're going to see a ton of really interesting use cases explode out. I think anti-gravity is super exciting. I think the video application is exciting. We're just at the beginning of seeing what this model can do.

---

<!-- VIDEO_ID: 1R73pf5Taco -->

Date: 2025-08-15

## Core Thesis
The speaker argues that Apple's deeply ingrained cultural DNA, characterized by an obsession with deterministic perfection, secret development, and a walled-garden approach, is fundamentally misaligned and counterproductive to the probabilistic, messy, and rapidly evolving nature of AI, risking its relevance in the new technological paradigm.

## Key Concepts
*   **The Priesthood of Irrelevance:** Apple's historical "priesthood" of controlled, perfected computing experiences, once a driver of adoption, is now becoming irrelevant as AI's inherent messiness and obvious utility demand a different approach.
*   **Probabilistic vs. Deterministic Systems:** AI, with its token architectures, operates on probabilistic system behavior, making deterministic perfection impossible and requiring a shift from pre-launch polishing to continuous quality assurance in production.
*   **Value Proposition Inversion:** Unlike early computing where value was obscure and needed to be revealed through polished interfaces, AI's utility is immediately and obviously apparent, making raw intelligence delivery more critical than interface perfection.
*   **Core Competency as Core Rigidity:** Apple's historical strengthsobsessive polish, secret development, and a walled-garden ecosystemare now its greatest liabilities, hindering its ability to adapt to a multi-model, open, and rapidly iterating AI landscape.
*   **"Wallpaper in the AI Revolution":** A mental model suggesting that without a fundamental cultural shift, Apple risks becoming a foundational but ultimately non-innovative or value-driving entity, akin to a background service rather than a primary innovator.
*   **Ship Messy, Iterate Fast:** The counter-intuitive finding that rapid, even imperfect, releases (like early ChatGPT) can achieve massive adoption due to obvious utility, challenging Apple's slow, perfection-driven release cadence.
*   **Multi-Model Ecosystem:** The future of AI is characterized by users leveraging multiple models (OpenAI, Claude, Gemini, etc.) simultaneously, rendering single-vendor loyalty and walled-garden LLMs unsustainable.

---

Steve Jobs built a priesthood for computing and that priesthood is becoming irrelevant in the age of AI. That's my central thesis. I want to introduce to you the idea that Steve Jobs's entire vision for Apple was something that was uniquely suited to driving rapid adoption of computing by the world.
And that the attributes that made Apple successful, the DNA that Steve Jobs inculcated into Apple that is alive and well under Tim Cook, it doesn't work in the age of AI. In fact, it's counterproductive in the age of AI. And I'm going to explain why. And this matters because this is one of the most valuable companies in the world.
A lot of people's retirements accounts, the entire stock market, Apple is so big, it's almost too big to fail. If Apple doesn't get AI right, all of us will feel the difference. And I want to explain why Apple's at risk of doing that despite the recent news that they are competing heavily in the AI space. I'm aware of it. I've read it.
We'll get into it. Here is why they are almost inevitably programmed not to win. Culture is so powerful as a shaping force for companies. It is part of why Apple is struggling. Let's dive into why. First, what worked about Apple back in the '9s? Steve Jobs had a central insight. He believed that part of what made computers complicated for people was that they were uncontrolled.
They were a nerdy, techy, configurable, complicated experience. He believed controlling the entire experience of computing would liberate users from complexity. Apple's designers would perfect everything in secret. They would ship polished products that users didn't know they needed, but they were so beautiful. They were so simple to use.
They solved problems so perfectly that everyone would immediately want one. I am old enough to remember when a lot of the Apple products first came out, including the ones that are in the museums now. Yes, there are Apple products in museums. They have all had that aspect under Steve Jobs. And frankly, Tim Cook has tried to continue that with the Apple Watch and more recently with the Vision Pro.
polished products, what users don't know they need, perfected endlessly in secret until every aspect shines like a jewel. Now, you can rightfully complain that it hasn't been the same since Steve left. I think that's kind of reasonable. Steve was one of the most prolific inventors that we have ever seen.
And so, it would be a hard act to follow. And I think Tim knows that. But the DNA that Steve left the company, what I've described remains true. Apple obsesses over perfecting experiences in secret. The good engineers at Apple never ever leave. So, how does that work in the age of AI? What about the age of AI doesn't connect with Apple's culture? I'm going to give you a few, but I bet you can think of some of them as well.
I bet this is not too surprising. Number one, the age of AI is a messy age. This is not a world where you can be deterministic and polished. Inherently with token architectures you have probabilistic system behavior. You have an unpredictability in the response. You cannot nail it down so it is perfect every time.
One of the things I like to emphasize with quality assurance with QA is that we are in the midst of a profound shift where before most of QA energy was around polishing software before it launched very Applelike. Now, most of QA energy is around making sure that we can sustain the quality of software in production. Or at least it should be.
We're not there yet, but I think that's where it's going. Why? Because production software is living now. It's messy. It's complicated. Look no further than the release of Chat GPT5. Let me ask you a very simple question. Would Apple have ever let that out the door? Would Apple have ever let that kind of chart mishap happen on a live stream? Would Apple have ever let out a product that immediately had to be kind of halfway rolled back and had the old product renewed and had obvious server outages in the first day.
No, Apple would never have done that, but OpenAI did. And OpenAI in just a decade has gone to become a $300 billion company. Now, granted, that's not as valuable as Apple yet, but the point is the trajectory. The point is that the fundamental incentives and levers that Steve correctly identified in the age of computing do not set Apple up with the culture to compete in the age of AI.
Users in the age of computers found computers not obviously useful. They were nerdy. They were complicated. You didn't obviously need them. I know for folks who are younger listening and watching this, that seems insane. I recognize that my kids feel the same way, but it was true. I remember growing up when nobody had a computer except a couple of nerds and it wasn't considered necessary.
I remember when people gradually decided that computers were necessary and Best Buy became a big thing. I remember the same transition with cell phones. Apple was able to leverage both of those big transitions, the transition to the PC and the transition to the cell phone, because both of them needed Apple polish and perfection to become obviously desirable status symbols that solved problems in ways that were so simple and obvious anyone could do it.
It's not just that the iPhone has become a status symbol. Arguably, it's less of a status symbol because it's so ubiquitous now. it's that it solves problems by creating a walled garden and a polished experience. In a messy AI world, you can't do that. And I will go farther. I will say the reason the messy AI world is working well and gaining adoption and the reason that LLMs as a whole have gone to a billion users already in just two or three years, much faster than the adoption of the iPhone, is because unlike with computers, AI is obviously
useful. AI is not hm I wonder if it's interesting or useful. It is a generalpurpose technology that is incredibly and obviously useful and people don't have to wonder. And it's incredibly and obviously useful to an eighth of the world's population. Now, now we don't all use it the same way. Some of us use it with really fancy prompts and agents and automation workflows.
Some of us use it to tell our kids bedtime stories. Some of it use it some of us use it as an AI girlfriend. Some of us use it for just chatting about the day and coming up with recipes. We have a wide range of uses which makes sense if it's a general purpose technology. But the point is that it's useful in a way that is clear and clean enough on the surface and that is immediately validating enough that we don't have to have a perfect interface or perfect product to make it work.
I would argue with you and I actually I don't know that you'd argue back. The chatbot is not a perfect product. I've had the head of chat GPT in an interview say so. It's what took off. It's what went viral. It's not particularly a great interface, but the value of the intelligence was so incredibly high it didn't matter.
That all of that is contra the DNA of Apple. Apple was built for a world where the the obvious value wasn't obvious. Where the value of computing was hard and they had to make it make it possible, make it visible. Not anymore. Not anymore. The value is right there. And Apple doesn't know what to do with that.
And that is why we see the news that Apple is going back to the well. They're going back to Steve Jobs. They're going back to their DNA and they are building a device. That's what Apple does. They do hardware. They build devices. They're going to build a tabletop device, right? It's going to be an AI device in 2027. Do you know how fast AI is going? You've got to if you listen to this this podcast, right? Like like you you all know like it's incredibly fast.
We may be at chat GPT7 by the time this device comes out. OpenAI will have had time to work with Joonyi Iive and come out with their own hardware device. They will not be blocked by the same obsessive DNA. They will be able to release. And so what Apple is doing is cultivating an old habit and failing to recognize that the world has changed around them.
They are having trouble attracting AI talent because the world has changed around them. The world is okay with messiness now because the utility is obvious. The world is okay with an open ecosystem with competitive AI models with using multiple models at once. Apple was built for a world where you picked your computer and you stuck with it. It was Windows versus Mac.
I got to tell you, I use Open AI and I use Claude and I use Gemini and I will use Grock. I use a lot of different models. I'm not loyal to one. And you know, for a lot of people, chat GPT is the default. I don't want to gloss that over. But just because it's the default doesn't mean it's impossible to use other things and doesn't mean that a substantial portion of power users aren't using a bunch of different models.
We live in a multimodel world. And in particular, the models are close to parody and it's become very obvious that the future is going to be multimodel. Apple's not built for a world where that technology is so ubiquitous and spreads. so easily that we can have phenomenal open- source models and there just is no point in having a walled garden LLM.
It just it doesn't work well. It's not going to sustain, especially given Apple's obsession with polish and release cadence. Siri is still terrible and Apple's on this very long time horizon to make Siri better. And it won't matter because everyone is transitioning to talking with chat GPT in the meantime. The voice experience is moving on without Apple.
And Apple just cannot get past the obsession with perfection that worked in the age of computing. That is the core insight that Tim needs to shift to move Apple forward. And I care. I like Apple. Look, I'm recording this on a Mac. I have an iPhone. I think that Apple's simplification vision has been powerful for computing.
And frankly, a lot of the development work that is building the age of AI is happening through engineers who prefer to work on Mac. Mac still has a place in the hearts of many people working on the AI revolution. But because of this perfection with polish and perfection, because of this obsession with getting it right, we have a situation where Apple is at risk of becoming the wallpaper.
Apple is at risk of becoming IBM, Windows, the very thing Steve Jobs didn't want to see happen to his company. Apple is going to become irrelevant. Not necessarily unprofitable, not necessarily tiny, but largely irrelevant from a value perspective because value is moving from do you have an incredible computer that helps you do things to do you have the intelligence at your fingertips to get where you want to go? And for some people that's the recipes and for some people that's building code.
And you'll come up with your own uses. And there will be a whole portfolio of trillion dollar companies built off of that idea. But Apple is not setting itself up to be one of those trillion dollar AI companies. They're setting themselves up to be the wallpaper in the AI revolution. And the more their plans leak, the more it reinforces that they have not made the cultural change that they need to.
Apple intelligence remains in limited beta. They just they they cannot get past the polish piece and they're not going to because large language models are imperfect and the labs recognize that and they're public about how they're fixing it and public about how they're addressing it and they release quickly and they iterate. That is the future in the age of intelligence and Apple is just constitutionally unable to adjust to that.
I hope it changes. I have seen cultural change at companies, but a culture change this big at a company this influential, we have not seen that. If Apple really changes and embraces an AI first mindset, that's a big deal. That's a really, really big deal. So, this is my love letter to Apple. This is my appeal to the priesthood of computing.
Please recognize that the world has changed. recognize that the the obsession with detail, the obsession with perfection, the quality of computing and simple solutions, the things that got you adoption and love for the Mac and for the iPhone, those are not things that work in the same way. Now, we have a technology that everyone is adopting even though it's messy.
You will not win by waiting years to polish it. You have got to ship. You've got to ship. And I know that's not the same way Steve Jobs taught the company, but you've got to ship. Otherwise, you're you're going to risk leaving yourself behind the most important revolution we've seen in our lifetimes.
Otherwise, you're going to miss the biggest general purpose technology wave that we're going to see. And that's what Apple's looking like they're doing right now. I hope it changes, but everything I see out of Apple suggests that it's not. And that's why it's it's built into the DNA. Well, best of luck. I'll keep using my iPhone in the meantime.

---

<!-- VIDEO_ID: 2qHxfwvIx-I -->

Date: 2025-08-19

## Core Thesis
Anthropic is strategically using software development as a "Trojan horse" to build and refine a robust, general-purpose agent, leveraging code's verifiability and high leverage to develop stateful, persistent, and collaborative capabilities that are transferable across all workplace tasks, positioning Claude as the future "work surface of choice" for enterprises, a pragmatic approach that challenges consumer-first AI strategies.

## Key Concepts
*   **"Code as a Trojan Horse" Strategy:** Anthropic's deliberate use of software development as a strategic entry point and training ground for a general-purpose agent, rather than an end in itself.
*   **Verifiability and High Leverage of Code:** The unique properties of code (immediate feedback loops, objective validation, high impact on early adopters) make it an ideal environment for rapidly developing and refining agentic AI capabilities.
*   **General Purpose Agent Disguised as a Specialized Tool:** The counter-intuitive insight that the advanced capabilities developed for coding (statefulness, persistence, nuanced reasoning, workflow orchestration, explanation, teaching) are universally applicable to any complex, human-assisted task, making "Claude Code" a misnomer for its broader ambition.
*   **Pragmatic Agentic Development:** Focus on practical, real-world agentic features like usable large context windows (acknowledging imperfect recall), on-demand memory for selective context, persistent state management, and multi-agent collaboration, prioritizing robustness and reliability over raw scale or hype.
*   **Human-AI Collaboration Frameworks:** Integration of learning modes (explanatory, active educational) and sub-agent systems that foster human skill development and collaborative workflows, moving beyond simple automation to true partnership.
*   **Enterprise-First Flywheel:** A strategic business model where attracting early-adopting tech companies with a specialized coding agent creates a virtuous cycle of feedback and broader adoption across diverse enterprise functions, challenging the assumption that consumer AI dominance translates to enterprise success.
*   **"Work Surface of Choice" Ambition:** The long-term goal of becoming the primary interface and operating environment for all workplace tasks, achieved through incremental, polished, and strategically aligned product releases rather than large, disruptive launches.

---

Anthropic is showing us their strategy for Claude in broad daylight and everyone's obsessed with the Chad GPT launch. But look at what they've released in the last few weeks. They released Claude Opus 4.1. It's a 0.1 release. No one's going to pay attention, right? But it delivered meaningful improvements that I can feel every day in Agentic Tasks.
It gets better at realworld coding. And keep in mind, and this will be a through line throughout this, anthropic is really good at code. And we'll get into why and why they picked that later on here. Now, it tests well, right? It gets 74 and a half% on Sweetbench, which is the bench for software engineering tasks.
And it's especially good at large codebase navigation, finding the right corrections, not making unnecessary changes, the things that are making agents more useful. Essentially, they roll it out. Unlike with the GPT5 rollout, which was very rocky, Anthropic's roll out is pretty chill. But we're not done yet.
August 12th, just a few days later, they roll out a million token context window for Sonnet. Huge. And Opus 4.1 will support it now, too. It's more than double the previous sort of flagship AI token window. Now, I grant you there are some token windows that are even bigger than that that have kind of fallen by the wayside.
So we've heard mentions of 2 million token windows for example out of llama hasn't gone very far. This is a and this is what I emphasize. This is a usable 1 million token window. Now is it perfect? Is the recall perfect? No. There is no AI system that has perfect recall in a million token window. But it is usable and it enables you to put more of the codebase into consideration for sonnet and opus which matters with complex code bases.
So it's like, you know, a 75,000line codebase. It can fit inside the context window for a conversation now. And so you can put all of that in front of Opus 4.1 and ask it to think through and solve the problem. You see how these are starting to build. They release an Aentic agent. They immediately upgrade the context window to make that agent more useful.
But we're not done yet. Now they're going to keep building the capability. Also, August 12th, they release an ondemand memory system because you may want Claude to selectively remember from past conversations. So, you can search through past conversations. I've talked about this before. And you can generate a piece of context from those past conversations.
That's like a wedge of context to add to your current conversation. It's not like chat GPT. It doesn't keep static memory. You have to use your prompt to angle the context in the memory. Again, it underlines how important prompt engineering is. I never expected prompt engineering to be such a durable skill set two years ago, but it keeps getting more and more and more important. But we keep going.
We're not done yet. You know what else has been happening in the background? Claude code has been getting better. Claude code can now run servers and manage longunning tasks in the background. It can start a dev server. It can run persistent test suites. It can perform builds on its own. And you can just check in with it.
Now claude code also has learning modes. It basically is going to give you different output options depending on what you want from cla code. You can have explanatory mode where claude narrates its choices, explains what and why as it edits, it commits, it runs tools. This makes debugging and code review easier.
It also helps you to learn if you're new to development. Claude also has learning mode. It's a more active educational style where Claude prompts you to code pieces yourself and guides you by asking questions rather than prescribing. So, it builds human skills alongside automation. These are now rolled out to claude code users and we're still not done. I know quietly.
While GPT5 rolled out, Enthropic has released all of this stuff. We're going to find the through line. They've released hooks and event system management. So you can be cloud can be configured with custom hooks like shell commands or scripts that run before or after a tooling event. They released sub aent systems which was a big deal at least in my corner of the world where you can add support for model personalities or roles inside claude and mentions so you can have multi- aent collaborative workflows in the same project. And they've even
released a micro compact mode which didn't get a lot of attention but it's super interesting. It lets users clear old tool calls to manage an extended session life so that you don't have to sort of clear the entire work surface. It's like you can organize your tools as you go. Users can also now connect Claude code to live services like Apollo's MCP servers.
Claude is aware of persistent context from these servers. So that can handle stuff like registration, health checks, using them with encoding workflows, things that you need for persistent state. and updated API capabilities mean Claude can persist, cache, and resume complex workflows. In other words, all of this stuff put together makes Claude a much more dependable agentic partner in development.
And in addition, and this is the piece that a lot of people aren't really quite seeing, this this is the thin end of the wedge for Claude code and Claude itself to become your work surface of choice because that's of course the holy grail. That is actually what chat GPT5 was aiming to do was to become a work surface of choice and to lay the foundations for that with office workers everywhere.
They want to replace Windows and the Windows suite. Very simply, very Claude is actually making arguably a smarter play for that exact same prize by obsessively focusing on code first. Now, you might wonder why code? What is it about code that makes this really, really interesting? Code works because it's verifiable and it's a high leverage environment.
So, code provides immediate feedback loops, tests, errors, builds. It provides objective validation of the agent output. Anthropic can push the boundaries of agentic autonomy knowing that mistakes are detectable and correctable. And code is also extremely high leverage from a work perspective. The companies that adopt claude code and the clawed coding agents are companies that you want to have as logos when you are driving broader adoption of claude.
These are forward-thinking early adoption companies with big logos that people are going to find sexy and attractive. These are tech companies, right? Tech companies have a lot of engineers. Focusing on coding gives them a lot of leverage with these tech companies and they in turn get a lot of feedback from looking at loops that are run by code bases other than their own.
And so one of the really interesting things is even if it's anonymized, even if nobody's stealing anybody's code, Claude is still getting feedback from thousands of tech companies across Silicon Valley and using that to make their coding agent even better. This is a case where winners keep winning. Software development is also extremely iterative and requires nuanced reasoning and persistence to work well.
If they can tackle those challenges early, anthropics agents are going to be more robust, more contextaware, and have workflow orchestration skills that will be applicable beyond a programming. Again, this is part of the deliberate play on Enthropic's part. Finally, once agents excel at code, they can quickly run to adjacent tasks that go with code like documentation, parsing, project management, automating workflows, even non-technical problem solving like ad optimization.
And what's interesting is that internally that is exactly what is happening at Enthropic. Enthropic talks about this. Their marketing team uses claude code. Their legal department uses cla code. They named it Claude code. It was just a Trojan horse for Claude agent. This is a general purpose agent and each of the releases over the last few weeks have been building up that Agentic approach.
Look at the way they focused on Agentic capabilities with Opus 4.1. You need that for everything else. Look at the way they focused on a usable 1 million token context window. You need that for everything else. Look at the way they did memory on demand. that enables you to cultivate more accurate tool calls without loading up the context window with a bunch of extraneous information.
Look at the way they focused heavily on the ability of clawed code to manage persistent states and take independent action. If you want to ladder up all the technical stuff like running servers, being able to explain what you're doing, being able to teach you, expanding hooks and event system management for tool calls.
This is not just stuff that you need to know how to do for development. Although that is true, it is a quality of stateful work that you need to do any kind of agentic assistance for humans. If humans want a useful assistant, it would look a lot like this. It just might not do coding.
And that is the secret of Claude code. You might want a useful assistant that explains things, that teaches you things, that helps you to learn. You may want a useful assistant that helps you to take action autonomously on your ad network. You may want a useful assistant that can process an entire piece of long context about a contract you're reviewing and can give you really useful feedback.
You may want a stateful assistant that remembers the last conversation you had or that remembers a live feed to an MCP server for something you want to keep track of like add updates and can process that in real time and take action. All of these things are on the horizon thanks to today's updates. Even though today's updates, the last few weeks updates are framed in terms of code.
Code is the Trojan horse. Code is what Anthropic is choosing to use as the wedge into the workplace. And what's beautiful about that flywheel is because they're attracting tech companies with code. They are attracting early adopters who will also be willing to try clawed code elsewhere more quickly. Early adopters by nature are more fluid.
They are more willing to try new things. They're more willing to try clawed code with marketer seats, with product management seats, with customer success seats, and see if that general purpose agent is useful. And all of that feeds the virtuous flywheel for Anthropic. And so, while Chad GPT5 was having a rocky week, Anthropic had frankly a power week.
They were able to release a bunch of pinpoint updates that underline that provide dotted line connects to their longterm strategy of capturing the workplace. And if you were to handicap the race for the workplace right now, I would say Anthropic is clearly in the lead. Anthropic is more likely to be in the workplace of the future than OpenAI.
Despite OpenAI's ubiquity, despite the fact that OpenAI has an edge in raw user count, we already know that Anthropic punches above its weight on enterprise contracts. There are already anecdotes postGPT5 of companies letting go of their GPT5 contracts because they like what they get with Cloud Code.
None of this suggests that Chat GPT will not remain the most iconic global brand for AI out there. I think they have handily won the consumer race, but that doesn't mean they automatically get the enterprise race and Anthropic has figured that out. Keep looking at future releases with clawed code.
Keep watching how Anthropic ships. They ship frequently. They don't necessarily do a big fanfare about it, but every single ship lines up strategically toward that larger goal of capturing the workplace. And Claude Code is the agent they're using to get that job done. I'm very impressed with what Anthropic has been shipping lately and I am enjoying what I'm enjoying the polish.
I'm enjoying the fact that they launch and there's not a big blowback. It's quiet. It's consistent. They just launch it and it works. It's great. I can see why companies are saying we're just going to pick Claude. There's less drama. It's just easier. It codes. And hey, by the way, we can also let our other teams use it.
That's a really good play for the workplace. So, hats off to the anthropic team. Well done, guys. Looking forward to what you ship

---

<!-- VIDEO_ID: gXbTh70m_q0 -->

Date: 2025-08-26

## Core Thesis
AI fundamentally elevates the engineering discipline by transforming the role of engineers from mere coders to critical architects of robust, reliable, and accountable systems. The speaker argues that AI's probabilistic nature and ability to generate code exponentially increase the complexity and "blast radius" of potential failures, thereby intensifying the need for human engineering rigor in imposing deterministic guarantees, managing risk, and ensuring operational stability.

## Key Concepts
*   **AI as a Multiplier, not a Democratizer:** AI tools act as "rocket fuel" for trained engineers, enabling them to compress expertise and architectural intent, while non-engineers often gain "just enough rope to hang themselves," widening the gap between skilled engineers and casual coders.
*   **The Shifting Digital Divide:** The essential skill in the age of AI is moving from "who can code" to "who can engineer," emphasizing system-level understanding, efficiency, security, and production readiness over mere syntax knowledge.
*   **Engineering Probabilistic Systems:** A core human responsibility is to impose deterministic guarantees and contracts on inherently probabilistic AI outputs, defining invariants and managing "probability budgets" to ensure system reliability and safety.
*   **The "Blast Radius" of AI Failures:** AI's ability to generate code exponentially increases the potential impact of failures, demanding greater human responsibility in system design and oversight.
*   **Effective Prompting as an Engineering Skill:** Prompting is not just a linguistic task but requires engineering understanding to effectively specify intent, constraints, and desired system behavior, akin to writing specifications.
*   **Emerging Engineering Disciplines:** The necessity of new fields like "Semantic Engineering" (debugging meaning flow, injection attacks), "Boundary Engineering" (architecting human-AI interfaces), "Memory and Knowledge Engineering" (institutional memory for AI failures, versioning), and "Safety and Assurance Engineering" (live evaluation, safety cases for hostile inputs).
*   **Three Laws of Engineering in the Age of AI:**
    1.  **Law of Invariants (Specification):** "If you can't write what is invariant, then you have not engineered the system," distinguishing engineering from "gambling" by defining what must always be true.
    2.  **Law of Production Measurement (Verification):** "If you can't measure it in production, then you didn't really build it," insisting on observability, telemetry, and semantic forensics beyond mere prototypes or demos.
    3.  **Law of Accountability (Explanation):** "If you can't explain why it failed, you haven't owned the system," emphasizing the human responsibility for understanding and explaining system behavior, especially in regulated contexts.
*   **Empathy as an Engineering Skill:** The ability to bridge machine precision and human ambiguity, anticipate user misuse, and account for human nature in system design.
*   **Economic Engineering:** Managing intelligence as a utility, optimizing trade-offs between latency, quality, and cost, particularly in an environment where "tokens cost money."

---

This is a love letter to engineering. I firmly believe that AI makes engineering more essential, not less. And I'm going to tell you why in detail because I think that people who aren't engineers don't understand this. And I think increasingly junior engineers are afraid. Because they have not experienced what it's like in detail to work with senior engineers at scale.
I have. I've worked with senior engineers. I work with principal engineers. I know what it feels like to have a very strong engineering mind or collection of minds in the room reviewing a technical specification. I want to tell you in this video why I am highly convicted that engineering isn't going anywhere as a discipline.
And in fact, I will go farther. I will say engineering is more important now than it was before the age of AI. The fear is real, but it's backwards. Yes, absolutely. Boilerplate code can be generated automatically. I don't doubt that. I see it all the time. Yes, AI can write working code from natural language. That's true. But working code and engineered systems are worlds apart.
They're not close to the same thing. In fact, most people are finding that out as they vibe code systems that may be ready to launch to friends and family, but are not ready for actual production. I would also argue that one of the reasons why engineering matters more now is precisely because the AI can write code. The blast radius of AI generated failures is exponentially higher when the AI can write the code for you.
So we're not being replaced. Engineers are not in danger of being replaced because engineers are being asked to take positions of greater responsibility over AI with AI in partnership with AI and we'll get into that. But I I want to lay that out as a contention because I think we just need to say it out loud.
My position is yes, there will be some engineers who don't understand how engineering works that will absolutely lose their roles in the age of AI and frankly many of them probably would have lost their roles previously. One of the interesting things when you work with a lot of different engineers as I have is that you realize how variable the talent mix is across engineers.
An engineer at the same level can be worlds apart in terms of actual capability and impact to the business. And anyone who's worked with engineers will tell you the same thing. I had an intern when I worked at Amazon who did more work and delivered more value than senior engineers I knew there. It just was the way it was.
He was motivated. He had a great assignment. He was able to get something into production and he did a great job. Needless to say, he got an offer, right? Like the point is that talent is variable. Talent has always been variable. And we shouldn't mistake the fact that engineering is hard and talent is variable from the impact that AI is having on the engineering discipline.
There is absolutely an impact on the engineering discipline from AI. And I'm going to get into it in the rest of this video, but it's not as simplistic as saying AI equals bad for engineering, which is what I see mostly. And I'm tired of it. And so that's why I'm basically making a video as a love letter to engineering.
So let's let's bite off the first piece. I talked about AI generated code. Let's talk about the vibe coding piece of this, right? People talk about vibe coding as replacing engineering. Now anybody can speak an intent into lovable.dev dev and they can get, you know, a working software back or at least that's the idea.
AI tools, I would argue, create a multiplier effect for trained engineers more more than they create democratization of code. And I know they democratize code. I know people who have never coded before who were able to do some coding. Now, engineers can do even more. Engineers can compress their expertise to carry architectural intent much more easily than non-engineers because they understand the underlying technical systems.
Non-engineers, frankly, when they get the chatbot and they get the ability to vibe code, I've seen them build stuff, but I've also seen them very frequently get just enough rope to hang themselves. Engineers understand those limitations that coding brings. They understand how to read code. They understand how the system components work together and they're able to go faster as a result.
If the non-engineers get roped to hang themselves, the engineers get rocket fuel. So the digital divide is shifting very rapidly from who can code to who can engineer. And I want to pause there because I actually don't believe that only people from conventional computer science backgrounds can engineer. If you understand how software components go together, that is increasingly the essential skill.
If you can engineer systems so they're efficient, if you understand how backend works with front end, if you understand what attack surfaces look like from a security perspective, if you understand how to move software into production, what that requirement looks like, those are not exclusively skills you learn in computer science.
And in fact, many engineers will tell you they didn't learn them in their computer science majors. They learned classical computer science. Engineering is something we have frequently learned on the job. But it's not something limited only to software engineers. A lot of people can learn engineering principles. And what I've observed is that part of the reason why I'm saying software engineers go faster with vibe coding is because they have already inculcated into themselves.
They've absorbed into themselves these principles of engineering. So they feel native and that is really the differentiator. It's not that they have a computer science degree. It's not that they know every single bit of JavaScript or TypeScript or whatever it is. It's that they know how to engineer. And that's encouraging because it means if you're trying to also learn how to build efficiently in the age of AI, you can do so faster by just learning some of the skills of engineering.
And I'm going to lay out what I view as some of the new core skills for engineering based on lots of work with engineers through this AI transition. So the thing I want to leave you with as we move on from sort of the AI coding piece and get into some of the other parts of the this engineering domain that we're exploring in this video, effective prompting is an engineering skill.
When I and I have taught courses, right? I've taught courses on effective prompting. And increasingly I think that this is a truth that we don't admit. Effective prompting is an engineering skill that requires some degree of engineering understanding. And the more engineering understanding you have, the more effective your prompting is going to be.
All right, let's go from just sort of that sense of vibe coding and getting rid of the fear of engineering into the human responsibilities that I don't think change and that I think still sit with engineers. Now, some of these will sit with engineers more in at scaled systems like at Amazon, at Google, etc.
And some will still work for smaller teams as well. But I wanted to call out the human responsibilities here because I think that we spend a lot of time talking about AI responsibilities and not a lot of time talking about the human component. And the human component is I would argue getting more important as we multiply our code by ax.
Number one, it is a human responsibility to translate intent to correct specification. So name the invariance, name the hazards, name the success criteria, translate human needs into system edges and boundaries. decide not just whether we can do something but should we do something. You are carrying the weight of systems that if you're working at a large company can affect billions of people.
And so translating intent to specification implies a degree of skin in the game that AI systems don't have. The second one I want to call out is humans are responsible for writing guarantees on probabilistic systems. AI is behaviorally at scale a functional probabilistic system. It's not always X or Y. It is a probability at scale.
You are turning likelihood into contracts if you are engineering systems. You have to be able to guarantee outcomes. You have to be able to guarantee edges. You have to be able to guarantee security to some degree. Fundamentally, a lot of the human job is taking these probabilistic systems and writing contracts against them that you can uphold.
So you have to be able to create boundaries that are deterministic, not probabilistic. You have to define probability budgets that work at scale across pipelines. You have to ensure that what must never happen really doesn't ever happen. And this I mean I've talked about scale a lot, but this is true even at a small scale.
Chad GPT will not give you the same response if you give it the same prompt. There will be subtle differences. Your job as an engineer, the role of engineering is to ensure that that kind of variance is not toxic to the system. It is also a human responsibility to think at scale. You have to understand emergent behaviors when you scale up to a very very large footprint.
And this one I think is specific to big companies. But understanding emergent behaviors at 100 million boxes is its own skill set. It is a human skill set that very few humans have. Knowing when algorithms become bottlenecks and where they bottleneck is a human skill and it gets at something that is essentially a risk with AI.
AI does much much better at writing code than deleting code. And one of the things that you see with really good engineers at scale is they know what they can remove. Being able to intuitit how the system works at scale, being able to intuitit where phase transitions from stable to chaotic occur in complicated systems. I've seen principal engineers do that.
It's a remarkable skill. It's a human skill and it means that they understand how to effectively deal with a world where one in a billion events are actually things that happen on a regular basis because of the trillions of events that they're processing. And I don't want to terrify you if you are an engineer who has not worked at that scale.
You'll notice that this is just one of a very large array of skills that I'm talking about in engineering. My intent here is not to convey that only those engineers that work at 100 million boxes scale will survive. Instead, I'm trying to call out that that is one aspect of engineering that remains very human even if AI increasingly assists in helping us understand these systems.
The last human skill that I want to call out is economic engineering. So, you have to be able to manage intelligence like a utility. You have to be able to optimize latency and quality and cost through trade-offs. You have to be able to design degraded experiences that prioritize value even with additional constraints.
You have to be able to understand where inefficiency matters and how it impacts margins. You have to engineer systems especially in an age when tokens are intelligent and tokens cost money. How do you deliver intelligence cost economically, cost effectively? That's a human skill and that's a skill that is scale invariant. You you should care about that even at a small scale.
Now I've talked about just a few s there are more human responsibilities but I also as we're touring across the engineering domain that I love so much I want to talk about some of the new disciplines that are emerging because it's absolutely true that engineering is changing and I don't want to pretend it's not and so I'm going to suggest for you a few of the ways that we see engineering starting to shift in the age of AI and then we'll come back to the human skills and kind of revisit them after we look at that.
So, semantic engineering, that's a new discipline, right? How do you debug meaning flow, not just data flow? How do you build semantic firewalls against injection attacks? I saw a new injection attack just today where someone can use the name field in chat GPT to prompt inject something.
Injection attacks come in all shapes and sizes. People are able to put injection attacks in white text on white on Reddit boards now because the system can't distinguish between your prompt and the context content it's reading. It's up to engineers to figure out how to address this stuff. It's up to engineers that design to design systems that will appropriately refuse to act.
And no, it is not just model makers. Engineers installing these systems have the accountability to act as well. Boundary engineering. Engineers have to architect the space between the probabilistic world of the LLM and the deterministic world that we expect with software. They have to create interfaces that feel consistent.
And yes, I am going to go out on a limb and say not all interfaces are going to created be created by AI on the fly. I don't think that's true. They have to be able to maintain human AI boundaries in ways that preserve human agency. Increasingly part of the engineering responsibility is figuring out how to map that boundary between human and LLM collaboration in software at scale.
Memory and knowledge engineering is another one. How do you build institutional memory for AI system failures? How do you version data and prompts even model weights with rigor? How do you manage context windows economically? How do you build semantic forensics? How do you do debugging on a system that's in production and you could not fully debug it prior? And that gets into safety and assurance engineering.
How do you create live evaluation cultures? How do you build safety cases that have explicit maps between hazards and mitigations and evidence change for audit? How do you design for hostile inputs as an assumption? How do you show what the system thought when the system is probabilistic? These are new skills for a reason.
We don't fully have the answers here, but today's engineers are being tasked with using that core engineering skill set I talked about to attack these kinds of problems in the age of AI. So, let's revisit and ask ourselves in that world with these kinds of new engineering skills emerging, what human skills really pop out.
We talked about some initially. We talked about the importance of of intent to specification. We talked about probabilistic systems and how you write guarantees against them. about thinking at scale, about economic engineering. There's some other human skills that I want to call out here that are going to be useful regardless of scale and regardless of where you are across these new engineering disciplines.
I wanted to give you a flavor of what's new. And then we're going to come back here to what stays the same. System intuition saves stays the same. Good engineers sense bottlenecks. They sense problems to solve. They recognize emergent failures. Empathy is an engineering skill because empathy requires you to bridge between precision that machines require and the ambiguity that humans deal with.
And effectively that's what we're all doing in the age of AI. Empathy requires you to understand how millions of users will misuse your API. It requires understanding how to build systems that account for human nature. Judgment under uncertainty. That's another engineering skill. Requires you to make expensive decisions on very incomplete information.
It requires you to know when good enough beats perfect, which by the way is one of those distinguishing characteristics of really good senior engineers. It requires you to choose appropriate trade-offs within constraints to decide when randomness is helpful versus when it's unhelpful. Another human skill is the orchestration of complexity.
You have to be able to coordinate tool chains to conduct symphonies of intelligence involving multiple LLMs that actually work and deliver value. You have to be able to manage distributed systems where the components don't have pre-written contracts increasingly. You have to understand that semantic composability doesn't follow traditional rules of software engineering and you're going to have to help create them.
There's a lot of complexity to orchestrate. Now, we've always had to orchestrate complexity from the engineering perspective. It gets harder now. Why? Why am I writing this? Why does all of this matter? The truth is, if we didn't have engineers, we would be in real trouble. The stakes have never been higher.
AI makes it so trivial to ship failure at scale. As I said, systems will now accept paragraphs of instructions from the open internet. The attack vectors are so high. Model rot can corrupt systems without any warning at all. The reality is that we we have to recognize that engineering is what enables us to take this wild world where LLMs can speak language and where they're probabilistic and where they can bring intelligence to bear.
Engineers help wrestle that into an operational stable production system. They help to bring observability to those systems. They help to debug those systems. They help to figure out the energy and compute footprint that's appropriate for those systems. And engineers ultimately are cultural architects.
They help us to design workflows that preserve human judgment. They help us to build interfaces that make AI reasoning inspectable. They help us to prevent automation bias and skill atrophy if they're designing systems well. Ultimately, they help us maintain dignity because they can build systems that have to admit ignorance.
Engineers have more responsibility now, not less. And that's one of the takeaways that I want you to sit with. I'm going to close with what I would suggest are three new laws of engineering in the age of AI. And I chose these three for a reason. Number one, if you can't write what is invariant, then you have not engineered the system.
So this captures the fundamental difference between vibe coding and engineering. In the age of AI, you have to be able to understand that LLM will give you likelihood, not correctness. And so an invariant, something that doesn't change, is what separates engineering from gambling, which a lot of vibe coders are gambling. It makes you think what properties survive when the probabilistic components do unexpected things.
It forces you to do resilience engineering. It's the difference between make it work and define what working means between it seems right and here's what must always be true. I would also say that engineering the the second law of engineering for lack of a better term. If you can't measure it in production, then you didn't really build it.
This requires you to go beyond the demo culture that AI enables. It's really easy to generate prototypes. Now AI makes demos almost free, but production is different. Production means real users doing really weird things. It means scale effects. It means edge cases. It means model drift. Engineering insists on observability, telemetry, semantic forensics.
You can't just ship code that worked once in a workbook. And we've always insisted on production as a bar for engineering. It is harder to hit now. And so that's why I'm reiterating it in this second law of engineering. The third one, the third law, if you can't explain why it failed, you haven't owned the system.
Again, we've emphasized accountability with engineering, but it is more important now. AI systems are entering regulated spaces, spaces where they have to be able to explain what happened. Human responsibility requires us to own the explanation, the accountability, and where the buck stops. If you can't explain what happened in your system to a very smart non-engineer, then you probably don't really understand your own system, and you probably didn't really engineer it.
And so these three laws are actually designed to fit together. They're designed to be three pieces of the engineering life cycle, the new engineering life cycle in the age of AI. Number one is specification. What we promise when we build a system, how we write contracts that stick regardless of probabilistic systems. That's the invariant piece.
Number two is verification or measurement. How we prove that we delivered something in production. And number three is accountability or explanations. How you take ownership of outcomes. Ultimately, the engineers that succeed are going to be engineers who think before they build, who validate in production, and who own the consequences.
And you know what? That's not a new skill. That is part of why I created this video, circling all the way back to the beginning. This is a love letter to engineering. Because even though engineering is evolving in the age of AI, and I hope I've given you a sense of that here, engineering principles are remarkably constant.
The need to design systems that work isn't changing. If you walk away with anything, I want you to walk away with the recognition that computing requires engineering. Engineering isn't going out of style. And if anything, the increased complexity of computing, the 100x, the thousandxed complexity of computing that we get in the age of AI is going to increase the need for skilled engineers.
So there you go. That's why I think engineers aren't going anywhere. And that's why I think we need to appreciate them more.

---

<!-- VIDEO_ID: sf_OYY9lMlw -->

Date: 2025-08-06

## Core Thesis
The speaker's core argument is that the prevailing challenges in AI adoption are fundamentally human, organizational, and process-driven, not technological limitations of advanced models like ChatGPT 5. Successful AI implementation hinges on pragmatic engineering, strategic integration, and a holistic shift in organizational mindset, rather than relying on models as magic bullets.

## Key Concepts
*   **Magic Wand Thinking Fallacy:** The erroneous belief that advanced AI models will automatically rectify foundational issues like messy data or complex business problems without prior preparation, structured context, or human intervention. This extends to both data quality and model selection, where the most powerful model is often misapplied to simple tasks.
*   **AI as an Ecosystem, Not Just a Model:** Effective AI in a business context is defined as "well organized data, the right model applied against that data with the right queries, the right guard rails, and the right evals surfaced in a way that a human can find useful." The model itself is a small, albeit critical, component within a larger value-delivery flow.
*   **The 80/20 Rule (and its Inversion for AI Operations):** While 78% of firms struggle with AI due to data readiness (a classic 80/20 problem), the speaker argues that for AI system sustainment, the 80/20 rule flips, with 80% of effort required for continuous evaluation and monitoring of production use cases, as opposed to pre-launch testing.
*   **AI as a General Purpose Technology (GPT):** AI is likened to electricity or the internet, necessitating deep integration into business strategy, significant change management, and upskilling across the organization, rather than being treated as an isolated "project" or delegated solely to technical teams.
*   **Human-in-the-Loop for Non-Happy Paths:** A critical design principle for AI systems is to anticipate and gracefully manage failures, hallucinations, and edge cases by integrating human oversight. Expecting 100% AI accuracy is unrealistic; the goal is often to achieve high utility (e.g., 87% correctness) and design clean transitions to human intervention for the remainder.
*   **Intelligence as Transformation, Not Innate Knowledge:** The "intelligence" of large language models derives from their ability to transform and predict tokens based on learned patterns, not from inherent "magical knowledge" or semantic understanding without explicit contextual structures. This challenges the anthropomorphic perception of AI capabilities.
*   **Total Cost of Ownership (TCO) Miscalculation:** Organizations frequently underestimate the true, ongoing operational costs of AI, which extend far beyond initial development to include token costs, continuous MLOps, production evaluation, and developer sustainment, leading to unsustainable deployments.

---
Today I want to talk to you about the things Chat GPT5 will not fix. What? Why? I don't have a special magic magnifying glass and a magic time machine that allow me to go and examine closely what it will be like in the future. Instead, I have a thorough understanding based on lots of company and boardroom experience of how business is actually using AI.
And I know that most of the issues that we're seeing today with AI are human and organizational problems, not AI problems. And so when the model makers make these big claims about how incredible their new models are, I always filter them back through the actual organizational realities. I want to go through with you a few of the specific problem areas that have very boring solutions that I see over and over and over again in companies that Chad GPT5 is not likely to magically fix.
Number one, this is the biggest single one I see. Magic wand thinking about your data over and over and over again. I see the fallacy. We just thought we could give the data to whatever LLM it is that they're using a, you know, they're on the Azure cloud, they're using Copilot, they're using Gemini, they're using Chad GBT. We just thought we could give it to the LLM and it will fix it. No, it will not fix it.
In fact, the 8020 rule is literally true here. 78% of firms, according to Techraar, that struggle with AI point to data readiness is the root cause. Data readiness is not something that an LLM will magically fix. There is a school of thought in the more advanced sort of researchy side of AI that eventually this will get fixed because AI will become so good at recognizing the mess of human data and have such big context windows and so much processing power that it will just read over all of this mess and magically make sense of it for
us. And that is often voiced as if it is present and here now and able to do that work today. None of those things are true. And there's a lot of concrete reasons why even if that were true, it would still be a bad idea to give your AI bad data. The cleaner your data inputs, the more likely you are to have a strong AI experience.
Do not use magic wand thinking about your data. I have been in situations where people will tell me, "What is wrong with my data?" And I will look at the data and it has no semantic meaning to an LLM. It's just a blob of data that is like unatategorized, unorganized. That's that's the issue right there. Like you don't have to look farther for a problem at that point.
Like if you're telling me that you have thousands and thousands and thousands of documents with this sort of undifferentiated like gigantic blob of text and you're expecting it to make sense of all of it, it's not going to because you haven't given it any sense of the semantic meaning in the larger data structures. you have no subcorpus semantic structures to work with.
And by corpus I mean the whole collection of documents. You should have some sense of meaning. So for example, if it's in a Wikipedia, maybe your internal wiki has different sections that have some semantic meaning. And then there's article titles that give give it they're worth separating out. They give you a sense of where you are in the wiki.
And that's just a very simple example. If you're dealing with official documents like health records, you're going to have semantic meaning for the patient name and semantic meaning for the for the diagnosis and this and that. The more you get really really clear about what data you want to convey, the easier it is going to be to actually use AI to pull the data.
The second issue I see is closely correlated magic wand thinking about models. People tend to assume they need faster, better, stronger reasoner models. I am a big advocate of moving your daily driver off of chat GPT40 onto a stronger model. I have said so explicitly. Go to 03, go to Gemini 2.5 Pro, go to Opus 4.
But as much as that's helpful for personal productivity, it does not mean that every single aspect of a AI job has to be done by the best reasoner model available. Look, if you just want to get columns sorted correctly in a PDF, it does not have to be sorted by the best reasoner model on the planet. If you just want to carefully go through a nicely delineated data set and extract all of the values associated with a particular firm, that doesn't necessarily have to be a reasoner model either.
In fact, that might not even be an LLM. That might just be a fancy SQL query. people tend to overpower their pipelines and they pay for it. They're they're basically paying a Ferrari premium. They're paying so that they feel like they've got the best model, which they think is what intelligence is. But really, intelligence is well organized data, the right model applied against that data with the right queries, the right guard rails, and the right evals surfaced in a way that a human can find useful.
Does that make sense? It's the right data. It's the right model constrained in ways that enable it to do useful work surfaced in a way that a human can understand and use. That's how intelligence actually works in the workplace. And do you notice how small a role a model plays in that? Chad GPT5 may be the best Ferrari in the business when it comes out, but it's a tiny part of that overall flow of value.
And so you have to think more broadly if you are trying to build interesting AI work. The third issue is vague or shifting objectives. a human problem again and again and again and again. You need to have a meaningful business KPI that you are nailing your AI project to that solves a specific business problem and the business problem has to matter.
If your business problem doesn't matter, I've seen over and over again people give up, they walk away. If your business problem matters but isn't tied to a KPI, so it's just annoying to a small section of the of the team, also not going to get prioritized. It has to matter to the organization, be measurable in a business KPI, and you have to nail that objective down publicly every time you can in order to keep the objective from moving and shifting when you inevitably run into problems. AI projects are a series of
nested problem sets that you continue to solve until you actually get to value. And you're not going to have the patience to go through those nested problem sets if you don't have a clear KPI that the business cares about that the seauite cares about that they are going to move. That's really what like leads people to persist.
It's what leads teams to persist. The fourth issue I see is treating AI strategy as separate from business strategy. It's a subtle one. Again, a model doesn't fix this. AI strategy cannot be separate from business strategy if you want to avoid wasting budget. If you want to actually make progress on becoming an AI native business, you cannot have the AI strategy in the corner with the AI guy or AI gal. And that's what they do.
You cannot just do AI as a project. You actually have to integrate AI strategy into your business strategy in a way that makes sense, which requires executives taking the time to understand how large language models work at a fairly granular level so that they can see the leverage points in their business where it applies.
Let's say that you are working in a business that you don't think has anything to do with AI. Let's say customer service is paramount in this business. It's all about putting your people in front of the customer with a white glove experience. I see executives in those situations often tell me, I mean, AI might be useful here and there, but it's not transformational for our business.
Like, we're not buying the hype. Wrong. There are going to be people in your business and in competitor businesses that see opportunities that you will miss because of that attitude. You need to recognize that just because your value proposition is very resilient to AI, like congratulations. Like you have a human touch, that's going to be very valuable. Love that.
But the back office piece, there is no reason that doesn't need an entire AI strategy focused on cutting down your business KPI costs. And by the way, that doesn't just mean firing people. That may also mean simply keeping track of all of the items that you sell more efficiently or being able to actively query against your data sets more efficiently or being able to thoughtfully price more efficiently.
There's a half a dozen things you can do in the back office. It's document management, right? Like it's a very boring thing, but it becomes a really critical piece of running the business well and AI can help. Number five, over relying on offthe-shelf foundation models. People think that a generic LLM will fit every niche domain and if it doesn't, they assume immediately they have to train their own model, but they don't even know what training their own model means.
Like I've had people in almost every conversation say, "Do we need to train a model for this?" And in a sense, I sort of blame the model makers. They've made it feel believable and they've made it feel plausible to train the models. It's not an easy task to do. I don't recommend it. Instead, think about your data and the constraints and guardrails that enable your model to flourish.
Think about your architecture. Think about your prompt engineering. Think about whether you need a data set that's formulated as a rag or not, a retrieval augmented generation data set or not. Think about the degree to which you need to help the model have the context to process the job appropriately and what providing that context means. But people don't.
They say, well, the model should know it. Like there's this again this fallacy that the model is intelligent because of what the model knows when in reality the model at a granular level is intelligent because of the way it transforms. These are transformer-based architectures and the intelligence comes from the way it transforms and predicts the next token.
It is actually not what it magically knows. We think that because of the way it's trained and reinforcement learned to be helpful, but it's not actually true when you're building production systems. Number six, ignoring integrations and ignoring operations for AI. Teams will demo a proof of concept and they will discover they have no eval. They have no monitoring.
They have no rollbacks. They don't have a way for the model to get pulled back out of production if there's an issue. They don't know what the bar is to reach production. if it reaches production. They don't know how to monitor it. They don't know what tool sets they are integrated with and therefore what vulnerabilities they have if those tool sets change.
They don't know how they're going to refresh the underlying data sets. They just think again the model will do it. If we put the model into production, the model will solve the problem. That is exactly the mindset that had Air Canada in court over bereavement policy that that their AI made up. You cannot ignore AI operations.
In fact, I would argue if you want to look for a career path, getting into AI operations, figuring out how to stably and safely deploy AI in production and pull it back and handle sandboxes, it's a big deal. It is not a trivial thing. It is something that most organizations ignore at their peril. And I have to remind people over and over again that this this has the characteristics of software.
You cannot deploy it as if it is not software and expect it to work. Okay. Number seven. Similarly, no human in the loop. This was the Clara story famously, but I see it in other cases where you have overeager CEOs who have read the LinkedIn hype and they are like, "We don't need these people. I I want to hire you so that like uh you know people people will lose their jobs and I can cut this team and they and they don't realize that Clara had to rehire their CS team.
" They don't realize that if you do not have an ability to go to a human being and actually find out what the real truth is when AI goes off the rails, you're inviting hallucinations, you're inviting compliance breaches, you're inviting brand damage, you're inviting a customer experience that costs you the heart of the business.
And so it's really, really important when you design systems to anticipate non-happy paths. This is just what we drill in if we're in product management. Don't just anticipate the happy path. Anticipate the miserable path. How do you make that more graceful, less miserable, more likely to retain you, the customer? Similarly with AI, when the AI goes wrong and the human knows it at the end of the conversation and the AI is not admitting it, how do you get the human help? People don't spend enough time thinking about that.
They expect the AI to be 100% accurate when they would never expect a human to be 100% accurate. That's not a reasonable bar. Something can be tremendously useful and only 87% correct. And so depending on your application, you may be in a situation where the AI is 87% correct and you need a human for the other 13% and your job is to design a system that switches cleanly between those two use cases.
I see very little investment from most businesses in figuring out that number and how you solve that problem. Number eight, underinvesting in change management. Massive issue. People just assume that if they put the AI in front of the team, it's going to magically work. Again, model makers are somewhat guilty here.
They they try and tell you over and over again, like like the leaks we've seen on Chad GPT5 this week. Leak after leak after leak, it's the next thing since sliced bread. It's going to be incredible. It's amazing. People are going to go through this cycle all over again where they think that they can just give organizations Chad GPT5 and it will magically do wonders for their bottom line.
And that may be convenient for the model makers from a sales perspective, but it's not true. You have to go through a change management and upskilling process to get people using AI. Otherwise, the chatbot loads nicely and they interact with it for like two or three basic tasks a week and you don't come close to realizing the power of what the model can do for you.
Not close. And yet, most organizations are investing more in the model and more in the AI technical stack than they are in the people. They're not investing in the change management. They're not investing in the upskilling and it's sort of like pulling teeth to get them to do that to be fair because no one talks about it, right? Like the model makers are emphasizing the tech, the tech, the tech, the tech.
And of course, you're going to listen to them and think it's the tech. And you're not thinking about it from the fact that this is a new general purpose technology. We need people change in order to usefully take advantage of this new technology. It's not like you can expect that people will be put on what is effectively an entirely new digital assembly line and told to just figure it out and it's just going to magically work.
Like we would not do that in a factory. Why would we do it here? And yet that's what we're doing. Number nine, forgetting the total cost of ownership. So often people just they don't think about the token costs. They don't think about the developer sustainment cost. They don't think about the uh the hit by a bus problem where one developer is doing this and if if that developer, god forbid, has something happen to them, then you know they're done.
They don't think about the sustainment cost of evaluating these models in production. None of that. They're just like, can we get this to production? If we can get it to production, great. We'll worry about the rest later. Because there's such a forced march risk on approach to beating your competitors to market. And I get it. It is a existential imperative.
Like you have to be able to get AI into market. So I understand the incentive there. I don't think they're incorrect. But understanding cloud inference costs, understanding how vector DB queries works, understanding how cost guard rails can be maintained is really important or you can be upside down on your margins really fast.
And that is just for serving the model if you're serving it to customers. If you are serving it internally, it is also an issue of making sure that you're extending your use cases across more of the internal footprint in a cost sustainable manner. If you are trying to build subsequent systems, it is also recognizing that AI systems take more sustaining in production than traditional software.
You have to evaluate them continuously. You can't just test them in QA and forget them. In fact, I would argue the 80/20 ratio flips. And 80% of your time should be spent looking at production use cases because you can't adequately test the model inherently for all use cases before you launch. You have to hit a certain threshold and say we're going to launch and keep evaluating.
And that means more sustainment cost that no one tends to factor in. The total cost of ownership is largely a broken calculator at most of the organizations I work with. It's bad. And I don't again the model makers say that tech will fix it implicitly and explicitly over and over and over again.
And the they're sort of victims of their own success because normally when you say that people discount what you say because everyone has 70 years of advertising in western markets in their heads and they always discount claims. But these guys have actually done it right. They've taught the rocks to think. They've developed an incredible new general purpose technology.
It's really, really, really good. And so our discount disappears with model makers. And we think, well, maybe they're actually right. Maybe they're not hyping it. maybe this is really this good because they've done this incredible job with this product, which they have. Like none of this should be taken to say that like these LLMs aren't incredible and can't do great work.
It's about how you set them up to do great work in business. And that's the part where people just there's a missing stare. People people miss that. The last one I want to call out, number 10, security and privacy shortcuts. We'll worry about where the data lives later. That's a classic one. I'm not sure what the security requirements are.
Let's just get started. let's get it up and we'll figure it out. Or I haven't looked at the terms of service for the vendor. I don't know how they relate to the foundation modelmaker. I'll let someone else figure that out. You can't do that. This is one of those things where you have to know the story of the data from day one because the risk of misuse is too high.
This is not a case where it really helps you to go faster to ignore those things. And partly it doesn't help you to go faster because solutions exist. You can read the terms of service quickly and easily. You can quickly understand which secure CL cloud environment you want to deploy into. You can become compliant and secure with relatively little effort two and a half years into the AI revolution.
So there is no excuse. You can't say going faster is a reason for this. You have to take security and privacy seriously. And again this is one where businesses are of two minds. This the scrappy startupy ones in a desperate position tend to be like you know what we'll deal with it later.
Whereas the ones that are more enterprisey tend to be like we'll do security and privacy and we'll do nothing but security and privacy for 6 months and then eventually we'll go on to the next thing. That is its own risk because it's actually not that hard to deploy a secure and private AI at this point. It does not take 6 months in most cases for most footprints.
And so if you're spending that long on it, you are probably using what I would call from Amazon day two thinking where you're like looking at it as a process rather than looking to the outcome you want to drive. So it is possible to take this last tenth one and to push it too far the other way and be too over obsessed with security and privacy.
And I say that because we know this is an issue and the cloud providers are heavily incentivized to make this something that is solvable by businesses at scale very quickly because they want your business. Google Cloud and Azure see this as the best chance they've had in years to steal business from AWS because they are farther along on the AI side.
they are absolutely going to be obsessed with delivering for you a secure private environment to your spec. So there's no excuse for that. Like you should get it, you should get it quickly and then you should move on. Okay. I hope that you look through these 10, which by the way, these 10 are not an exhaustive list.
There's other stuff too. The CEO not knowing how to use AI is my favorite secret 11th one. Like that is a problem. I've mentioned it before. If the CEO doesn't know how to use AI, it's really hard to drive AI transformation. Period. It's not a general purpose technology that works if half the people at the top of the chain don't really know how to use it.
There are lots of other examples. I want you to look at these examples and I want you to notice how many of the examples here are people and process and how many of them are technical architecture, not just models or data, not just models. Again and again and again, I've called that out. This is why I continue to say new models are great. I'm glad we're getting them.
It's a phenomenal time to be alive and working. But they're not magic bullets. They don't magically solve everything. Instead, they help you when you have good architecture, when you've solved the problems I've outlined here to make more return on investment than you would otherwise.
A better model in a clean data environment with an excellent human in the loop safety net with good MLOps deployment practices with a good AI strategy is going to go farther, right? Like if you it's it's like putting an engine in a properly constructed car, you're actually going to be able to get the most out of it as opposed to the people who are trying to jam the engine into the janky T- Model Ford and be like, "Well, we've got a Formula 1 engine in here.
You know, just floor it, right? Like, I'm sure it'll work." No, it's not going to work. Take the time to set your business up to be ready for new models. And by the way, chat GPT5 is going to be great, but there's going to be another model along in a month, two months, 3 months. We are in the middle of an exponential curve.
And so, we're going to see more exponential improvements. That is why it's so important to focus on these durable aspects of change management for business effectively. There's no other substitute. I hope that you have enjoyed thinking about all the ways that chat GBT5 will not solve all your problems magically. Cheers.

---

<!-- VIDEO_ID: SLYKFHtKR90 -->

Date: 2025-08-27

## Core Thesis
The speaker posits that advanced LLMs are catalyzing a fundamental shift in the "unit of work" from static documents to interactive, executable "instruments." This transition addresses the core bottleneck of decision-making friction by enabling non-coders to create auditable, fast, and living artifacts, thereby moving organizational value creation from authoring to runtime execution.

## Key Concepts
*   **The "Instrument" as the New Unit of Work:** A shift from static documents (docs, spreadsheets, slides) to interactive, executable, auditable, and fast artifacts that collapse decision-making chains. This represents a fundamental redefinition of a business deliverable.
*   **Value Accrues at Runtime, Not Author Time:** A profound economic and operational shift where the primary value of a work product is realized during its execution and the subsequent conversation, rather than solely in its initial creation or static form.
*   **Cultural, Not Technical, Bottleneck:** The primary challenge in adopting this new paradigm is not the technical feasibility of creating instruments, but rather managing organizational culture, preventing sprawl, ensuring discipline, and standardizing usage.
*   **Pragmatic Application:** Instruments are most impactful for "practical work done" and "in-between the cracks" decisions, complementing rather than replacing scaled enterprise tools or highly formalized processes.
*   **Policy as Code & Executable Governance:** Business rules and approvals are increasingly embedded directly within these interactive artifacts, moving towards a future where policy is literally encoded and executed, rather than merely documented.
*   **"Instrument Studio" & "Bar Raiser" for Quality:** A framework for managing the lifecycle of instruments, including standardizing schemas, tests, and exports, and assigning "bar raisers" to review prompts and maintain quality, mirroring robust engineering practices.
*   **Incentive Alignment for Adoption:** Successful integration requires changing organizational incentives to reward decision-making that leverages these gated, executable instruments over traditional, static documentation.
*   **Tool Builders' Opportunity: Primitives for Composability:** For creators of productivity tools, the opportunity lies in providing opinionated, composable building blocks (inputs, logic, tests, exports) that enable users to construct robust instruments, rather than just offering free-form document editing.
*   **Demotion, Not Elimination, of Documents:** Static documents will continue to serve a role in capturing narrative, context, and meeting minutes, but their central role in driving and proving decisions will be "demoted" by executable instruments.

---
We are moving to a new operating surface at work. It is not just a function of chat GPT5. I know I've talked a lot about chat GPT5 the last few days. That's because 700 million 800 million people have it and we all use it. Now I want to go beyond that. I want to talk about the idea that we have a new operating surface at work that yes is exemplified by chat GPT5 but is also going to be exemplified by Claude, by Gemini and others over the next few months. The stakes are really high.
The real drag in modern companies is not creativity where AI has been attacking over the last two years. It is so easy now to get a hundred ideas, a thousand ideas. The real bottleneck is the cost of proving a decision. We do that in docs. We do that in spreadsheets. We do that in slides. And you know what's happening over and over now? Chats become docs. Chats become spreadsheets.
Chats become slides. And the challenge is that we are still bolting on our old decisionmaking to this new way of working. I want to suggest to you that with chat GPT5, we crossed a Rubicon. We now have for the first time an incredibly easy way for anyone who is not a coder to create interactive artifacts that collapse that chain.
Interactive artifacts that make decisions executable, auditable, and fast. My thesis is very simple. The unit of work is shifting from static deliverables to instruments of work. Front-end artifacts that you can open and tweak and run. An instrument will couple some small typed inputs, a little UI, maybe some tests or an audit in very clean ways to look at the results.
A good instrument will replace several meetings and a deck with one surface and a very quick decision. Why is this happening now? What makes this possible today? Number one, distribution. It is easy now to have single file canvases that can travel as easily as slides across internal tools. You can share that chat GPT5 canvas so easily.
Claude does this too. You can share the cloud canvas really easily. There's no infrastructure to run that. Cost is also so cheap. If you have all of your employees on a chat plan anyway, and the canvas comes with it, it's essentially free. Governance is also easy because tests and approvals live in the instrument.
You can actually capture what you did and then if you really want to lock it, just stick a screenshot of that UI and stick it somewhere. It's easy to log your decisions. The nice thing is you can also compound and remix. So these artifacts are not dead, they're living. You can remix a weekly business review artifact and make it better next time. You can reuse it.
Now, I will add a caveat here. As someone who worked on weekly business reviews at Amazon, I am not trying to pretend that one artifact produced by one person in chat GPT replaces a weekly business review instrument that has been honed by business analysts over years for a team at Amazon scale.
What I am saying is that most teams don't operate at that scale and most decisionmaking doesn't require that formula review process. There's a whole class of practical work done decisions that are right now being made very slowly with documents with artifacts. They don't need to be and that is the new class of work that I'm talking about when I talk about this motion from static deliverables to instruments.
So what is an instrument? It has an input, a very explicit schema and sample fixtures. It has logic functions that you can read and test. The code is visible. Edge cases are declared. It has a UI, a display first scoreboard that has a few knobs you can touch and dial. It has tests, so gates are at the top and if it doesn't work, it doesn't run.
It has an audit encoded in it and ideally it will have an export. The key is making sure that you take those instrument components seriously. For example, if you have inputs, logic, and UI, but you have no audit trail, you can't really see what changed when people started to mess with your dashboard. And in the course of a meeting there can be a lot of changes and adjustments.
So I want to suggest to you that the strategic impact of getting this this work done is really understated because we haven't really lived it yet. The real challenge here is actually moving from a world where we have high latency and high friction and low trust for a lot of work even among good functioning teams because people can't remember the slack and they've been trained over and over again not to just trust the meeting but to get stuff into a doc.
get to a world where you have more leverage, where trust increases because you can actually see it in the interactive artifact, where you can just generate free evidence by just running the artifact again with new data points. You want to get to a point where you have a portfolio of artifacts that replaces your portfolio of PowerPoint decks, something that lets you run the business with artifacts.
Now, this is not going to work for everybody. Again, I've said it before. This doesn't replace BAS at scale. This doesn't replace excellent SAS tools for scaled up companies, but it does allow you get a lot of practical work done. I've created a dozen instruments to get you started. And they're designed to work together as a coherent operating system for smallcale teams and something that gives large- scale teams ideas about how to run fast for those in between the cracks where the real work gets done stuff. Let me lay them out for
you briefly and then you'll get like full prompts for them in the substack. Run the business. You get a WBR scorecard and you get a data quality sentinel. Something that like helps you obsess over your data quality and check your data quality where it matters. For shipping decisions, I've got an experiment decision pad and I've got a launch gate for you.
For reliability, I've got an incident commander dash and I've got so radar. Again, these are very configurable, right? You can adjust the prompts the way you want. You don't like my WBR? Don't use my WBR. Use the prompt and make it yours. That's the whole point. Revenue and risk. You have deals. You have contract risk triage.
These are things you can actually put into on the sales side. Customers, you have customer health triage. You have a pricing and mix simulator for your people side. You can get a hiring and funnel health one. You can get an access review runner. And this is just the beginning. These are designed to form an ops operating system from the get-go and a loop you can put in anywhere.
But this is just the beginning of getting your head around the idea that you have instruments now and not static artifacts. I want to suggest to you that there are real challenges with rolling this out that are not technical. In fact, the technical piece is mostly done. It is relatively easy to get these to start to work.
Instead, the risks show up in culture terms. People overtrust these, right? Like sometimes they will have shallow data in here and they will not show their thresholds and they will not pay too much attention to the artifact because they're not used to it and they will allow a lot of sprawl. They'll remix the artifacts in 50 different versions.
This requires discipline just like having a document standard requires discipline. It's not like it's a free ticket and you don't have to administrate the artifact. The power lies in the fact that it collapses a bunch of other work into one clean interactive artifact that makes decisionmaking faster. So the work really changes as a result and you have to be ready to impose that on the culture as a leader.
So meetings can run inside the artifact. You can record them, use granola, use otter, use whatever your recording uh AI is of choice and then you have the record of the meeting. You have the artifact itself and you're done. That's the whole thing. You need to get to a point where you are encouraging people to test artifacts, version them, and make sure that people are using artifacts appropriately in appropriate context and not just overusing them in ways that aren't helpful.
You don't want 16 different versions of the same meetings artifact running around because then people will not trust it. And so there's a certain level of experimentation you want to encourage when you're trying to get an artifact to gel. And then you need to converge and actually pick the winner and anoint that and standardize it and discourage further experimentation while you focus on other parts of the workflow you want to align out.
Part of how you do that is by challenging the operators, the runners of the business to own the outcomes associated with these artifacts. If you have someone in sales who is running the meeting that the artifact is associated with, they own the artifact. If you have someone in legal, they own the artifact for the legal review. You get the idea.
The key here is that you have artifacts that tie to consistent organizational patterns, to product launches, to incidents, to pricing, to access to hiring, to weekly business reviews. And you keep them as consistent and versioned as possible. And yes, for bigger teams, for bigger orgs, of course, you're going to have like, you know, the weekly business review version for this team versus that team because they're like 50 person teams and they have different business units and they have different metrics. I get that.
That's why prompts are easy to mix together. The point though is that you want to manage that cadence of evolution. You want to manage who owns it. You want to assign ownership just like you do with other good culture changes. And you want to map the instruments to the meeting cadence in a way that gives everyone predictability.
That's what builds trust. That's what builds trust. I would suggest if you want to move this way that you stand up an instrument studio, a place to maintain schemas, tests, export standards, what counts as good that you assign a bar raiser to review the prompts that are used for any new version so that people maintain those standards and you get better over time.
And I want you to challenge people and change the incentive to reward decisioning that ships through gates, not through decks or through docks. You want people to start to think in terms of how they can accelerate decisioning and how they can leverage instruments instead of flat docks to do so. So change your incentives.
Maybe change how you do performance reviews. This could get as far as looking at promotion readiness is looking at whether someone can articulate and define a new artifact in a way that's useful for their team. Just giving you an example there. I want to close with one other piece. We've talked a lot about how this changes things for business runners, right? People who run the business, operators.
What happens for tool builders, people who build Word, Docs, Notion, Sheets, people who are essentially building the static docs of the past. I want to suggest to you that Notion's already aware of this and others are too because they know the product is no longer a document editor. It's an execution surface.
That's why Notion's homepage is what do you want to make today? There's going to be a ton of competition for this space. And as a builder of a business, you are going to be spoiled for riches in how you actually convert your team over to instruments. If you're in the tool building business and you're used to static docs, one of the really interesting ways you can involve here is choose to ship primitives for instruments.
Shipped inputs as blocks, logic blocks, tests and gates, stable exports, things that people need to compose instruments for various workflows. I think that's a really interesting opportunity I haven't seen anybody fully grasp yet. You want to be in a place where you can have somewhat opinionated building blocks if you're in the document creation space because if you just have free text, people will abuse that.
Whereas if you have inputs with regular schemas and people can choose those, you're going to get much more useful downstream blocks. Give people helpful limitations to help them build composable instruments. And so part of what I'm doing when I suggest the prompts down in the substack is I'm trying to hold to and key to an opinionated schema, a schema you can stick with over time because you need that consistency to build useful artifacts.
One of the interesting implications of all of this is that we are moving to a world where policy is code. So a business rule is literally encoded in Typescript somewhere or a business rule is encoded in an artifact somewhere. approvals happen on the surface. We don't really have this yet, but we want to get to a world where we have lightweight e signatures and not just a buttonclick go no-go.
Right now, it's just going to be a buttonclick go no-go that's encoded in the artifact and you screenshot it. That's fine for getting started, getting into the instrument world. I want to see a world where we actually have artifacts that start to evolve into those mini applications. And I expect to see that over the next 6 months to a year.
One of the keys is making sure that these are everywhere. You can link them if it's Claude Claude and and the artifacts or if it's chat GPT and the canvas, you can link them anywhere. They're public facing links. So, link them in the invites. Link them in the chat. Link them in the issue. Make sure that these are interoperable and everyone sees them.
And make sure, and I'm just going to advise this because I've seen this happen. Make sure that you do have those screenshots because people can go in, click on them, and change things afterward. right now and remix the artifacts, which is great if you're chat GPT and you're trying to encourage innovation, but it's perfect hell if you want to make sure you have a steady state.
And so until these artifacts evolve with a little bit more miniapp internal checkpoints, you be the one that takes those screenshots and encodes this is what we talked about, this is what we decided. So you can always go back to that state relatively easily. In fact, at this point, honestly, you cannot just encode the screenshot. You can frankly grab a code snippet and it becomes an even more immutable record of what happened.
I want to suggest to you that there are some business model shifts here. First, we are moving to a world where AI needs to be visible and governed from AI being in the shadows. That means models need to have authors for these artifacts. They need to have run summaries and audits. You need to be able to generate tests from the code that show what you did.
We are at the beginning of this. you can use some fancy prompt work and you can get something like this going in GPT5 with canvas. There's going to be more. I also want to suggest to you that value is starting to acrue at runtime, not author time. That's a very profound shift. So think about it for a second. It's not the authoring of the artifact that matters the way authoring the PRD mattered when I came up as a product person.
It is it is the way value occurs at the time you run the artifact and have the conversation. And so the value is in the active instrument itself. And there's sort of a profound implication there if you're building in the product space. One of the things that I want to suggest is that you lean into adoption of these instruments if you're doubtful but want to try and be willing to make the first step trivial and imperfect.
Hey, let's replace the deck this week. It might not be perfect, but let's see how it goes and have the conversation. That's a two-way door. We can do that. See if you can start to measure the share of meetings that run on an instrument versus the share of meetings that run on something flat. See if you can start to optimize to support instruments being used more and more as you have internal AI teams that want to support you.
This is so much more interesting work than just building the chatbot to talk with the HR policy manual, which for whatever reason seems to be the default thing that people who greenlight AI teams internally always do first. No, do something interesting like this that actually accelerates the business.
Think about AI as a outcome driver directly, not just a chatbot. Move the center of gravity from defining a narrative into execution. That's what these instruments do. Instruments don't actually kill documents. They just they demote them, right? Docs will capture the narrative, the context, the story. You're going to turn the meeting minutes automatically into a document and a story.
Instruments are what give you the decision and capture the record and then you can move on. Instruments are what let you go faster. And we've never really had that before. So my challenge to you is figure out how you can ship higher quality decisions and prove that they're better quality. And that's exactly what I focused on with these 12 prompts to build these 12 instruments.
Better quality decisioning, fewer slide decks. Welcome to a world where we have a new way of working. We're all going to learn about it

---

<!-- VIDEO_ID: yqmdJ2lQw98 -->

Date: 2025-08-25

## Core Thesis
AI is inadvertently driving marketing into a "local maxima" by accelerating algorithmic feedback loops that optimize human psychology for engagement with artificial content, creating a systemic challenge where genuine authenticity is devalued. The non-obvious path forward requires brands to strategically leverage what AI cannot replicatephysical experiences and verifiable human touchto differentiate and build long-term value amidst an impending flood of AI-generated noise.

## Key Concepts
*   **Local Maxima in Marketing Evolution:** AI accelerates existing algorithmic feedback loops, pushing marketing into a state where short-term engagement (often via artificiality/controversy) is optimized, but long-term value (authenticity) is neglected, creating a system-level trap.
*   **AI as an Algorithmic Accelerant:** AI's primary impact isn't just content generation, but amplifying pre-existing algorithmic biases (e.g., towards controversy, manipulation) by making their production cheaper and more efficient, thus reinforcing problematic feedback loops.
*   **The "Artificial Authenticity" Feedback Loop:** AI-generated content, being cheaper and highly optimized for engagement, outcompetes genuine authenticity in algorithmic environments, inadvertently training algorithms to prioritize artificiality and shaping human expectations.
*   **Inadvertent Human Psychological Optimization:** AI systems, by optimizing for engagement, are inadvertently adapting human psychology to engage more readily with AI-generated content, blurring the lines between real and artificial and fostering new forms of parasocial relationships.
*   **Impending Signal-to-Noise Crisis:** The proliferation of cheap, high-quality AI-generated content will lead to an overwhelming volume, creating a severe signal-to-noise problem where genuine, differentiated content struggles to be perceived.
*   **Strategic Authenticity as a Counter-Signal:** The pragmatic solution for brands is to differentiate by emphasizing what AI *cannot* fake: physical experiences, tangible products, and verifiable human touch, thereby creating irreplaceable value and cutting through the AI-generated noise.
*   **AI's Untapped Role in Strategic Curation:** A significant, underexplored opportunity for AI lies not just in content *generation*, but in assisting marketers with *strategic curation, taste, and long-term brand positioning* by helping to select and refine from an abundance of AI-generated options.

---

All right, stay with me. We're going to talk about local maxima algorithms, AI, and Taylor Swift. I'm going to make it all make sense. I promise. First things first, I think everybody knows that AI marketing is exploding. Something like 40% of the Instagram feed is AI now. It's growing really fast. Meta is investing heavily in AI tooling.
Meta just published a 1 billion parameter artificial brain that's designed to measure your response to videos and simulate it so they can build more AI videos for you. And it's not just the tooling that's changing. It's also actual ads that we see coming out all the time that are controversial or AI generated.
And it's not even just the ad space. It's also AI generated imagery as a whole is exploding frankly because the tools are getting cheaper and better. I'm going to go through just a couple of use cases to kind of freshen you up. None of this will be surprising and then I'm going to explain how it all comes together.
So, first the Sydney Sweeney case, right? This was a famous case where Sydney was a real person in an ad, but certain components of the ad were AI generated because it was cheaper. I believe the car driving away was AI generated toward the end of the ad. The point here is that the brand chose to lean into controversy and they've admitted that, right? They chose to lean into a controversial stance with their with their particular angle on the GAN ad.
And they chose to lean into classic controversial jeans ads from the past. The Brook Shields ads from the 1990s were deliberately evoked. We're not here. I am not going to break down marketing history for you. If you want to look up the Brook Shields Calvin Klein ads, it's a very famous example of the controversy that genes ads have historically done, which in and of themselves are interesting because genes are ubiquitous. Everyone has genes.
And so having to market them means you sort of have to step into controversy to try and differentiate yourself in the space. And this is exactly what we're going to dig into more as we get into this idea of local maxima. The second thing I want you to keep in mind, now we're getting to Taylor Swift, is this idea of Taylor Swift deep fakes.
Now, deep fakes are not new. They're not certainly not limited to Taylor Swift, but because Taylor Swift is a famous woman celebrity, we get in a special case, a special prevalence of deep fakes, special visibility on deep fakes, journalists tend to use Taylor Swift as a test case when they are investigating the safety of AI systems.
And so that's why we get an immediate test of Grock's imagine feature and an immediate sort of news headline around the world that Grock's imagine feature will create adult images of Taylor Swift as deep fakes. You might wonder now where is this going with ads? I'm sharing this part because we are fundamentally in a situation where the technology to create ads and the technology to create controversy, which is exactly what we got at with the Sydney Sweeney case, are converging and cheapening.
In other words, part of why we're at a local maxima, is that the technology to create this kind of product is cheaper and cheaper and cheaper and cheaper. digital ads that would have cost, you know, a million dollars a couple of years ago are a tenth of the price or less now and they're going down further.
We are going to see more and more major brands jump on the AI generated ad train. I am I am fully prepared for a doubledigit share of the ads in the Super Bowl in 2026 being AI generated at least in part. So, there's a third piece here that I want to get at before we start to dive into the relationship between all of these components.
And the third piece is the idea of the uncanny valley. Essentially, you need to combine the controversy of Sydney Sweeney, the ease of production, and the parasocial nature of celebrity relationships, which enables the creation of deep fakes of Taylor Swift and other celebrities, with the notion that we can't tell the difference anymore.
And this is something that's true even of Gen Z. It's not really an agist thing. Gen Z in study after study after study professes to care about authenticity. But the reality is they also struggle in study after study to tell the difference between good AI generated material and real material. And I will tell you as a part of the test case here, I went to midjourney to create images of Taylor Swift in concert.
They were not inappropriate images. They were just like Taylor Swift in concert images. And if I had been shown those images, I would not be able to tell you if it was an artistic shot by a photographer from a real Taylor Swift concert or if it was made up. And I think that that's part of what's going on that makes it challenging here is that we ourselves are unable to distinguish artificial tokens from real tokens in ad situations or other situations today.
It's true in words too, but we're talking in marketing and images are central to marketing. So, we're going to stay focused on that for the day. So, those are the three pieces I want to bring together. We have Sydney Sweeney, we have Taylor Swift, and we have this idea of the uncanny valley. Where does this take us? Fundamentally, I want to suggest to you that we are living in an evolved system that has unintentionally evolved to a local maxima.
And I want to walk through the technical reasons for that. And then I want to walk through because I'm Nate and I like practical solutions. Some ways forward out of that local maxima. So first things first, what are the system dynamics that we're all living in for marketing? And this includes marketers.
My job here is not to blame marketers. It's actually to talk about the system as a whole and how AI is accelerating it. AI systems are creating evolutionary pressure on human content creation. So artificial authenticity is going to out compete genuine authenticity in algorithmic environments. In other words, AI is an accelerant on top of the social algorithms that we have built and optimized through social networks over the last 20 years.
This includes at least four individual feedback loops that are all accelerating. One is sampling feedback loops. So, controversial content tends to get more distribution and creates training data that biases towards more controversy. That's the Sydney Sweeney use case. Essentially, the feedback loop samples and gets an idea of what works well.
Controversial content, it's well known, does better in the feed, and so it generates more controversial content to accelerate that, right? I we have talked about this broadly as a society. We haven't necessarily talked about the idea that AI is an accelerant for this. AI accelerates the uh production of controversial content by making it easier and cheaper cheaper to produce and that in turn reinforces that training data loop for the algorithms.
The second one is feature feedback loops are a problem. So when a user engages with content they create a feature in the algorithm. User engagement with manipulative content will teach the algorithms that manipulation equals quality. That one is also not new as a feedback loop. But again, AI is accelerating it because AI is essentially able to produce content that is more likely to be engaged with.
It's content that is more likely to be latched on to by humans. And humans can't tell the difference. This gets back to the idea of the uncanny valley. We can't tell. And so we click and we engage and we're feeding both the content algorithm that feeds us stuff and we're also teaching the marketers that AI works well.
The third feedback loop is an individual feedback loop for us. Repeated exposure to optimized content changes our behavior and expectations. There's been a lot of studies around sort of what the Instagram feed has done to our self-image and our social relationships. I think we can also talk about this idea of parasocial relationships here.
A lot of the Taylor Swift phenomenon that I discussed is this idea that Taylor Swift is not just a celebrity, but Taylor Swift is someone with whom I have a right to have a relationship, even if it's a madeup relationship in my head. So, this gets back to the number that I gave you at the very beginning of this video where we talked about Character AI and other sort of AI uh companionships apps and the and the money that they're generating$ 220 million some dollars per year growing very fast.
They're cultivating the idea of parasocial relationships, but they let you create the character, right? They let you create the character you're going to have an artificial relationship with. Whereas traditional parasocial relationships, it's your imagined relationship with the celebrity. In both cases, AI accelerates this individual feedback loop in your own head where you are changing your behavior and expectations because of what you're engaging with.
The fourth feedback loop is outcomes. Essentially, successful AI content that runs through these feedback loops becomes the baseline for normal content. And so, in a sense, it's not just that the AI content is changing the way we consume behavior and changing what is in our feeds. It is also changing how we make real video content.
There's been a lot of work done on how Hollywood movie making has shifted over the last 20 25 years with CGI. In the same way, the way we make real content, whether it's movies or ads or what have you, marketing assets, is changing because of AI. I don't think we fully process this, but we are going to see camera angle shift.
We're going to see expectations for what you can do with special effects. We're going to see expectations for how you bring in people versus obviously not people versus people who are deep fakes, even if they're not real humans, but they they look so much like humans, we can't tell the difference. That we are reshaping the baseline of normal in our own movies and ads by what we interact with.
So these four feedback loops, the idea that controversial content gets more distribution, the idea that user engagement with manipulative content teaches the algorithms that manipulation is quality, the idea that repeated exposure of ourselves to optimized content or AI optimized content changes our own behavior, and the idea that outcome shaping shapes our expectations of even nonAI movie making and nonAI admitt, nonAI marketing.
All of these are feeding on each other and being accelerated by AI and that is producing emergent behaviors that nobody is programmed for. I am not a big believer in the idea that there is a bunch of evil marketers somewhere who are all deciding to sort of poison America or poison the world with their ads. That is a popular misconception.
I have been a marketer. I have worked with amazing marketing teams. That is just not how marketers actually work. Instead, I think it's more useful to think about emergent behaviors in a system that is, as I've said from the beginning of this video, locally optimized. It's a local maxima system.
So, in this case, what we have is emergent optimization designed effectively to adapt the human psychological condition to AI content. I'll say that one more time. You have AI systems that are optimizing for engagement and they inadvertently are optimizing human psychology for engagement with AI content. That is the through line that ties together Sydney Sweeney and Taylor Swift and this idea of the uncanny valley parasocial relationships.
It's all around this core thesis. Fundamentally, our AI systems because we're building them on top of optimized systems for engagement. They're essentially adapting our psychology to engage with AI content. This then creates a problem because we say as marketers and as brands that we value authenticity. The humans buying the product say they value authenticity.
In between disintermediating that is an AI marketing system that values artificiality. It values what's fake. What do we do about that? How do we optimize there? My suggestion to you is that the way through this is going to be for brands to emphasize what cannot be faked with AI. I think the music industry is a really interesting example of this.
Let's look at Taylor Swift and her launch of her Orange album, right? The new album that's coming out, TS12, right? When Taylor Swift announced that on August 12th, she blurred out the image of the album in order to prevent any issues with uh sort of album faking, etc. from AI content generation. But it's not just a defensive play.
She's also pre-elling physical media. And she's not the first musician to do this. Vinyl is making a huge comeback. She's pre-selling the vinyls for the album. You know what's interesting about vinyls? You can't fake vinyls. They're real. You can touch them. You can hold them. That is part of why they're making a comeback.
My suspicion, my strong suspicion is that one of the ways that good brands can cut through the noise at this point is by doubling down on physical experiences that humans cannot get any other way. You cannot have an AI that gives you a vinyl of the Taylor Swift album in the same way that Taylor mailing you the album on the correct date is going to get you.
that that is an irreplaceable human experience to open the package and get the vinyl. Well, that's a way through. That's a way to keep authenticity that holds long-term customer value in a world where there are going to be so many brands demanding your attention. And that, by the way, is one of the reasons I don't think this local maxima will hold forever.
If you're wondering, well, should I throw up my hands? Should I just despair? One, that's not what I do on this channel, so go somewhere else. And two, I don't think so. Because at the end of the day, AI intelligence and AI tooling is continuing to get cheaper, which means every brand is going to be able to make these, you know, fantastic, weird videos all the time.
We're going to get a tremendous amount of content flooding the zone that is all very high quality and all very AI and it's going to be so much that people are not going to be able to consume it all. You're going to have a massive signal issue. People won't be able to find signal to noise.
So, what does a good brand do to stand out? What do good marketing teams do to stand out in the age of AI optimized and accelerated algorithmic relationships? Well, good brands double down on actually giving you authenticity that you can't get anywhere else. And that's why I go back to physical product, physical goods, physical experiences, popups, events, things that people can be in the space on that you can't copy with AI.
They can be very Instagrammable moments. You can have the sort of the Instagram wall with like the special logo or whatever it is. You can have fun with it. You can make it sort of hybrid and online at the same time, but you still, I think, are going to need to have that irreplaceable human touch.
And that's why you have examples like uh whiskey distilleries and other things like going back to getting physically close to their customers, offering customer tours of the distillery, offering special tasting rooms, this that and the other thing. And that's a luxury good. You might think, well, if you're not in the luxury goods space, how are you going to replicate that? I think that that is where physical store footprints are going.
We know that the mall is not really a thing in the US anymore. We know that the controversy that Sydney Sweeney courted with the jeans ad was primarily designed to drive attention, drive traffic to the American Eagle site, which would then convert. If that's what happened, you want to be in a position where you can give people experiences that are in person that leave them feeling like they want to come back for another experience like that that they can't get anywhere else.
Because part of the problem with marketing for controversy is it buys you the pop in the stock price. It buys you the attention. And I'm not going to pretend it doesn't buy you dollars. I'm sure it bought them dollars, right? Let's not let's not kid ourselves. Controversy works, but it doesn't necessarily translate well into long-term customer value.
And if you want long-term customer value, which ultimately sustains free cash flows and sustains the valuation of the business, I think you have to figure out how to get people from AI spaces into online spaces. And by the way, this is not me saying that AI ads are bad or that AI ads won't exist or that we shouldn't have them.
I think the reality is everyone's going to be able to make their own AI ads very very quickly. We're just going to have them. Instead, I want to think about how AI ads can tie us into physical spaces that allow us to engage with our whole senses, right? Our our smell, our sense of touch, all of the things that make us register real experiences, maybe that help us build community for the brand around with other humans who are consumers of the brand.
That's going to be what builds those long-term customer relationships. So I think that like piece one is getting physical. I want to suggest to you another piece that I think will help. To me, one of the things that we are going to start to see as consumers start to demand authenticity as a differentiator is we're going to start to see some kind of certificate of human touch in some of these brands offerings.
I don't know exactly what that will look like. This is a little bit of a peak around the corner, but you can take a you can take a tour through how the beauty industry has handled authenticity and beauty over the last 20 years and the way marketing in the beauty industry has shifted as a result.
Similarly, I think we're going to see marketing that leans into authentically portrayed human experiences. Marketing that says this is a real human. We didn't AI their face. Marketing that says this is a real car. Look, a piece of it just fell off. we didn't pretend. You're going to get more and more of that as a way to signal to consumers in a sea of AI content that you are worth paying attention to.
In a sense, there's going to be an anti-AII signal that is going to come up because AI content is going to become so cheap and easy to produce. I want to close with a question for AI tool builders. If you're building AI tools for marketers in the space, most of the time you have been building for the idea for the tool chain to produce these ads.
You've been building for a cleaner, simpler pathway from idea to ad. That's fine. I think that pathway is fairly well trodden. I want to suggest there's a lot of untaken space with AI in helping marketers exercise good taste and judgment over what is the best long-term positioning for the brand. What is the best long-term positioning for a particular ad campaign that supports the brand? There has been precious little AI work put into the connection between content branding and long-term strategy.
And frankly, LLMs are getting smart enough that they can at least be thought partners on this. And I don't see anybody thinking about what you do when AI can produce a thousand ideas and you have to pick one. Who's doing that part? And so that's the challenge I want to leave you with. I think we can build our way out of and we can market our way out of the local maxima we're in.
We don't have to tolerate the experience that we're having now in our algorithms. We can build our way out. We don't have to tolerate a world where everybody can be deep fake, not just celebrities, not just Taylor Swift. We can put guard rails in place. But the key to doing that is our willingness to think more deliberately about how we want AI to weave into our marketing.
And that's the heart of this question. If we have a local maxima, we have to be more intentional with our AI usage and the way we cultivate long-term relationships with the brands. And that's up to us as consumers to demand. It's also up to marketers and it's up to builders who equip marketers with tools to think more intentionally.
And that is the heart of what I have to talk about

---

<!-- VIDEO_ID: 7RZlxqMcObE -->

Date: 2025-07-26

## Core Thesis
The AI revolution is fundamentally a re-architecting of work, not a wholesale job replacement, driven by AI's inherent limitations in handling non-tokenizable human elements like trust, ambiguity, and liability. Navigating this shift requires individuals to leverage uniquely human skills, integrate existing domain expertise with AI literacy, and adopt a disciplined, principle-based approach to learning and execution, rather than succumbing to hype or chasing ephemeral tools.

## Key Concepts
*   **Glue Work & Non-Tokenizable Skills:** Human roles are more than a bundle of tasks; they involve "glue work"  the integration, context, and relational aspects that AI struggles to tokenize or replicate. Core human skills like building trust, navigating high ambiguity, and exercising judgment under uncertainty remain AI-resistant.
*   **AI's Specificity vs. Ambiguity Paradox:** Counter-intuitively, as AI models become more capable and specific, they often become *worse* at handling real-world ambiguity, highlighting a fundamental limitation in their ability to reason beyond structured data.
*   **Liability & "Skin in the Game":** AI systems lack accountability and "skin in the game," meaning human responsibility and liability remain indispensable, particularly in high-stakes fields like medicine or finance, making these roles durable.
*   **Domain Advantage as an AI Bridge:** Mid-career professionals possess invaluable "domain advantages" (tacit knowledge, regulatory fluency, customer access). Coupling this deep expertise with AI literacy creates an irreplaceable "AI translator" role, commanding a premium and challenging the notion that only pure AI specialists will thrive.
*   **The Execution Gap:** Despite the low barrier to entry for AI tools, the true challenge lies in sustained, iterative execution and overcoming the psychological "start-stop problem" and mental blocks associated with learning and applying AI effectively.
*   **Technological Compression:** The current AI revolution is distinct from past technological shifts primarily due to its unprecedented *speed* of change, leading to rapid job disruption and restructuring rather than immediate mass unemployment.
*   **Projects as the New Resume:** For new entrants, demonstrating practical building and problem-solving through public artifacts (e.g., GitHub, storytelling portfolios) is becoming more critical than traditional credentials for career entry and advancement.
*   **Focus on Fundamentals over Ephemeral Tools:** To stay ahead in a rapidly changing tech stack, individuals should prioritize understanding underlying AI architectures (e.g., transformer models) and core concepts (RAG, vector databases, agent orchestration) rather than chasing every new tool.
*   **Masking Red Data:** A critical, pragmatic security measure for safe AI usage in professional contexts is the absolute masking of confidential or personal company data, emphasizing individual responsibility and the significant legal risks of shadow IT.
*   **AI Creates New Problem Spaces:** The advent of AI doesn't eliminate problems but opens up entirely new classes of previously unsolvable challenges, requiring human ingenuity to identify, frame, and address them with specialized AI applications.

---
I get hundreds and hundreds of questions a month. I get them through my contact form. I get them on my comments. I have distilled them down into 12 highlevel questions that punch at the hardest things about this AI revolution. I want to give you my answers to those right here.
And then at the end, as a bonus, cuz we do bonuses around here, I want to give you the things that people don't ask that I would be thinking about. So, let's get to that at the end. First 12 questions. Number one, Nate, I see headlines about AI layoffs all the time. How can I tell if my role is next or if it's just getting rewired and like I'll be okay? I want to suggest that the huristic that you can use, the rule of thumb that you can use is to look at your role as a series of tasks and then look at what percentage of those tasks AI can take and then you give it a discount. And the reason why you give it
a discount is because I have said over and over that rolls are not just bundles of tasks. Rolls have glue work. And so what you need to be asking yourself is if you took away 30% of the tasks in the role, could you leverage yourself to be more effective at accomplishing the team mission at the company because you had less busy work to do? or would it feel like it was just eating out and hollowing out the role and there wasn't really a lot left to do.
If it's the latter, if it feels like it's just hollowing out, that is when you should get concerned. So, I'm going to give you a couple of specific examples. I think customer success has been one of the hardest hit roles, but it also shows where you still need to have hope.
So customer success is an example of something where big Silicon Valley names including Sam Alman himself have said there just won't be CS jobs anymore. And yet at the same time we see major companies who claim to roll CS jobs to AI like CLA roll back because they realize they need good handoffs and they need humans in the loop who can actually help customers because customer help turns out to be a very context dependent thing.
I have navigated AI menu after AI menu, AI chat after AI chat because everyone's rolling them out. You probably have too. The experience has not been as good as working with my buddy Thor at Amazon. And that is a real name of a customer service rep at Amazon who I worked with a decade plus ago. And I got my questions answered. Thor had a great sense of humor and we all had a great time.
I've never had that kind of an experience with a customer success robot. And so I think CS is going to change. I think it's an example of a case where you can argue that large pieces of those tasks are going to get picked up by AI. It's just too easy for AI to write text based on databases and it will get more personable. Probably not as personable as Thor.
But I would argue that if you look at it, you should be able to architect systems. You should be able to see places where you can lean in as a CS rep and deliver extraordinary value. I know CS reps that drive expansion revenue for businesses because they are so good at what they do. An AI agent is not going to be as good at driving expansion revenue for businesses. I it just won't.
And so part of the answer is looking at your task load versus your mission. Where is your mission aligned to versus where are your tasks aligned to? The other part is something you can't control. And so part of how you tell if your role is next is frankly if your leadership understands AI.
Does your leadership talk about AI in a nuanced way the way I'm talking about it? Or is your leadership out there saying, you know, AI is a cost cutter? I'm happy to just dump these roles because they might be wrong about that. They probably are. People who tend to dump quickly tend to regret later. We have big stories about that. I just mentioned one, Clara.
But if that's the way they're thinking, it pays to watch leadership and find another role or go hunting for a different career path because of leadership's attitude, not because of AI. And I do want to distinguish those two.
So if you want to tell the answer from a task perspective, look at it as what are the tasks that are being automated? Discount for the bundling, discount for the glue work you can do, discount for your mission alignment. If you want to look at it from a company perspective, look at your own leadership. Look at whether they are willing to actually acknowledge the nuance of AI or if they're just looking at this as a cost cutting machete.
Number two, question number two, Nate, I need dates. When do experts say that white collar cutbacks are going to start to bite? Experts disagree on this one. They really do. There is no one answer. I wish I could give you an answer. There are lots of people who claim to know.
People claim to know that it will be 2027. People claim to know it will be 2030. People claim to know it will be 2028. People claim it will happen and then there's a camp of people who aren't sure if that's the case yet. It depends on your attitude. If I were you, I would assume that there will be significant restructuring of roles and a significant disruption to every role in white collar in the next two to three years.
That is different from assuming that white collar cutbacks will mean mass layoffs across all of those job roles. I don't think that is baked into the empirical evidence. Do our jobs differently? Absolutely. And in fact, we're just getting started with that. Most of us don't have jobs anymore. It's not clear yet that that is happening.
It's not clear in the data. It's not clear given the capabilities of the AI systems and the direction they're growing. Will there be some layoffs? Yes. Will we see more chaos in 2026 and 2027 as job disruption starts to hit? Yes. But I think that when we talk about this, we often confuse breadline level chaos where it's like 30, 40, 50% unemployment, it's a doomsday scenario, we all have to go on universal basic income, etc.
with technological change level chaos that's compressed where you have a technological change equivalent to the steam engine being invented or equivalent to the internet being invented and you have to negotiate that change very quickly because unlike past revolutions, this is all happening now. But we don't really articulate those as two different futures. My bet is sort of that this is like other technological changes, but it's very very compressed.
So the shocks are going to feel more dramatic for the next few years. Other people are betting on a more doomsday scenario. For folks who are betting on a more doomsday scenario, they tend to say words like 2027 and 2028 a lot. The good news about that is that we will find out real fast if they're right or they're wrong. It will not take that long.
It is well within even the entry- level part of an initial career path. Which suggests that if you want to plan for your future, you should plan for them to be wrong because it does not hurt to plan to build your skills in case they're wrong. And if if they're right, it doesn't matter. So, you might as well build your skills anyway. And that one often surprises people.
Number three, Nate, I want work that AI cannot cannibalize. How do I spot really durable roles before everybody else piles in? It feels like there's so much hype people are just running back and forth. I want to suggest to you that there are certain things that do not go out of style. Understanding how to broker trust does not go out of style.
Understanding how to build trust in business contexts will never go out of style. It will never be taken by AI. Understanding how to work in high context situations where you have to be aware of wide rapidly changing contexts doesn't go out of style. It doesn't disappear. AI doesn't take that because AI is not good at tokenizing that. AI is not good at tokenizing trust. You can't really tokenize trust.
Trust is a human transaction. Understanding how to handle high ambiguity situations where things are gray and shifting all the time. Those are not things that AI is super good at either. In fact, one of my biggest frustrations with AI models is as their capabilities have increased, they have not gotten better at handling ambiguity.
Arguably, they've gotten worse because they're better at being specific. And so look for places with really messy real world constraints. Look for places where deep relationships are required. Look for places where you need to deliver outcomes, especially if you need to deliver outcomes against liability.
A good example of this, people have been saying for a while that robots are going to take over surgeons roles. Surgeons have liability. Surgeons can be sued. Surgeons must get it right. Surgeons have skin in the game. Robots don't. And so I think surgeon, ironically, is a role that may transform and shift as we get robotics involved in the surgery room, but it doesn't mean, and it already is, but it doesn't mean that surgeons themselves are going to disappear.
And you'll see similar roles across tech. Does that mean that these are only available for seniors and people who are deep in their careers? I don't think so. I think one of the really interesting things about AI is it is upending so many of our assumptions about jobs that there are all kinds of tail opportunities opening up that people haven't fully defined yet. AI architect, it's a brand new role. We haven't fully defined it.
Yes, it probably takes some degree of experience with AI and understanding systems, but it's an example of a role that's very, very new. Another role that's new, AI engineer. What does it mean to be a good AI engineer? There's lots of other roles beyond that. There's roles that we don't really have good words for.
We don't really know where product management is going or how it's disrupting, but it's an example of a role where you need less technical knowledge than an engineer, probably more than you used to have, and you need a totally different mindset in a world where you might not be driven by a road map anymore. And so I think that the opportunity here is to look for those durable relational transa relational low transactional structures.
So high context, high ambiguity, high trust intersections, places where it's not super transactional, places where you have to be relationship oriented, places where you have to be deep in context to understand things. And if you think, by the way, that AI engineer and AI architect don't have to be deep on trust and ambiguity and context. I've got news for you. Places where you have to deliver outcome against liability.
Chase problems with unstructured data. Chase problems that aren't easily tokenized yet. AI can't eat it if it can't ingest it. So, you want to look for those spaces. And the thing is, I can't name all of them for you because they're still coming into being. That's one of the really interesting things about the next two or three years.
And so, I'm trying to give you the principles to spot them for yourself. Okay. Number four. Nate, I'm a new grad and entrylevel roles seem to be evaporating. Where do I earn real experience now? How do I get onto this ladder? This doesn't seem fair. Well, I 2008 was also really rough, let me tell you.
So, first off, I think that part of the challenge is that you are getting hit with the job application broken pipeline harder than anybody else because other people can lean on previous work experience, but it's harder if you haven't had that. I think there's a couple of things that help, but the one thing that I've seen that is most reliable is just going to require relentless execution on your part. It ties into number three.
So, the thing that I think helps the most is treating projects like the new resume. You've got to be able to ship things. You've got to be able to show what you're building. You've got to be able to show you can connect with community needs and build something in response. If you're in tech and you're building anything that leaves a public artifact, if you're in marketing, if you're trying to tell a story, you have to be able to start telling stories now.
You want to leave a public footprint of what you're working on that is hard to replicate. If you have a bunch of storytelling Tik Toks, if you have a strong GitHub that you have actually delivered working code against, it actually works. It's not just, you know, a bunch of broken projects. It's at least something that people can look at and investigate. And then the question becomes not did AI do all of this for you, but do you understand the principles of building for the role you're asking for? Because sometimes like people assume like you have to have a GitHub if you're an engineer and you shouldn't have a GitHub if you're not. Those rules are shifting
like yes engineers probably should still have a GitHub but people who are not engineers need to be able to also talk about technical topics now and so if you have an opportunity to build something and you're not a technical person don't be afraid of that.
I also suggest that you look for something that is like a fractional apprenticeship. Small part-time gigs for founders that need problems solved for them. There's so many indie founders out there. Every single one of them does not have the time to automate as much as they want to. They do not have the time to build as much as they want to. Go help them. They can refer you. Go help them. You will get something you can build and show.
And how do you get that? You're like, "Well, who's going to pay attention to me?" You should have projects you can show and say, "This is why you should come for me. Look, I can show you my work." And so the ladder that is there is changing because the roles themselves are changing. And that is part of why hiring is so broken right now is because people, even hiring people, are trying to figure out and project what they will need in 24 months and hire for that. I will also say part of that chaos means that there are roles opening up targeted toward new grads
that weren't there before. And so, for example, there are roles that are targeted at entrylevel folks coming in where you need to articulate your AI fluency from the get-go so that you can help bring uh AI fluency to the team you're with. That's new. You know, those roles didn't exist before.
And so part of it is figuring out, you know, if you were tracking towards some of the steady tech jobs from the 2010 era, maybe those are changing really fast, but there's other ones that are opening up. And so I would say look at your public artifacts, look at fractional apprenticeships wherever you can get them and pitch for them. You don't wait for them to open up, go get them. Oh, go call DM.
And then make sure that you're aware of the fact that there are roles opening up that may not have conventionally been in the middle of your uh in the middle of your degree path, but they are now. Okay. Number five, Nate, I I can't waste cycles.
Which JI skills do I need to learn this year? I got to tell you, there's a few that do come up over and over again. I do think there's a clear answer. And I just want to go through I if if people have these four big buckets covered, they are already ahead of most folks. And I have written up a ton of these already on the Substack. So, number one, prompt architecture. Understanding how prompt prompts work. I think it's it's one of the universal skills.
Now number two understanding how retrieval augmented generation or rag works and where it doesn't work which is critical basic vector database hygiene understanding embeddings understanding refresh pipelines how you build a vector database even if you're not building one yourself understanding how they work so that your eyes don't glaze over really really helps.
Number four is lightweight agent orchestration. So understand how tools like NAD or Langraph enable you to wire tasks together and then it can be a public artifact. Wire things together, automate. And then last one, number five, data storytelling. Understand how to turn a raw model output into something that is polished. That is a meta skill. That is not necessarily just a technical skill.
People who copy and paste are doomed. I don't say doomed very often, but you're doomed. people who are able to polish model output, to think critically, to engage with model output. That goes back to one of those larger skills I called out. Look for places AI can't cannibalize.
Well, I got to tell you, polishing model output and knowing how to make it sharp is exactly the kind of high ambiguity, high context work I'm describing. So, get good at data storytelling with LLM. That's skill number five. So, to go through the five again, prompt, prompt engineering or context engineering if that's the popular term. Now, rag, understanding how vector databases work, which is related to rag, but slightly different because it's a little bit of a level down from a structural perspective.
Understanding agent orchestration, number four, and then data storytelling with LLM, number five. All right, next question. Nate, the stack flips every six months. How do I stay ahead when the tools will not sit still? Look, the best way that you can do this is to schedule Google 20% time. I'm not saying actually spend 20% of your time on this.
I know we don't all have that luxury, but the stack itself is built on fundamentals that don't change as quickly as all that transformer architecture is underlying this entire AI revolution. It hasn't changed. And so understand the things that don't change. I call them out really really frequently in my content.
And then be disciplined about forming hypotheses about what you want to bet on and explore on in a particular month and and create that in line with your larger intent and goals. your mission. We've talked about this idea of being mission aligned when we talked about career path and like are you able to contribute to the team mission, the company mission, etc.
What about your personal mission? Are you able to articulate these are the things that I really want to get done? These are the high ambiguity or high trust problems I dream of getting into. will walk back from that and by the way AI is a tool for that and figure out which technical skills or which AI tools are in line with that larger mission and then focus there and do it in a time boxed way.
Say I'm going to take 4 hours a week for a month and I'm really going to do it. I'm going to set a timer. I'm going to sit down. I'm going to do I'm not going to scroll TikTok. I'm not going to watch Netflix. I'm actually going to do it. I'm not going to tweet about shipping. I'm actually going to do it. and then come back and see if your skills have grown in the direction you want.
See if you've made progress in a month. It's like any other habit. You have to build it. And so my advice is basically the tools will not feel like they're moving so much if you have a compass. So develop that compass. Number seven, Nate, I am mid-career. How do I translate what I already know into an AI adjacent role without starting from nothing? Look at your domain advantages.
Where do you already have strong domain expertise, regulatory fluency, customer access, legacy data, storytelling, polishing capabilities? Now, pair that with an LLM and become the bridge that other people can't easily replace because you have that deep domain expertise. I have people telling me that they desperately want their existing senior employees to lean in more on AI and they worry because they don't. Don't be that person. You have the domain expertise.
you have the advantage productize tacet knowledge. You can think about if and this is if you want to go into a consulting, if you want to go into an indie role or whatever you have dreamed up for you for the next half of your career, you can productize that tacet knowledge into something that helps people who are climbing the career ladder earlier than you to get up faster and learn those domain secrets quicker than you had to learn.
Eventually, you should be in a position whether you're internal or whether you're doing some sort of uh independent role where you can act as an AI translator in your vertical. You should be able to command a premium because the untransatable, the hard to understand, the difficult expertise that comes from years of knowledge is something you carry with you and you have now successfully coupled it with AI.
And so I would actually say look at it as my domain gives me an incredible starting point to get to an AI adjacent role without starting from zero. All I need to do is to dive in on AI literacy. The things that I just called out the the basic pieces that I described a couple of questions ago. Understand agents, understand rag, understand data storytelling with AI.
Those are things that if you can start to get them down, if you can start to get prompt engineering down, you are going to be formidable. You're going to be a very strong candidate. Number eight, I'm using chat GPT at work. What's career safe usage before legal gets involved? The answer is you must mask red data.
Red data is anything your company considers personal or confidential. Just don't put it into any AI. Just don't do it. You don't want the risk. You can mask it. you can and masking means like obscuring all of the confidential information but and I know people do this anyway.
There's a massive shadow IT problem but the risk to you individually is disproportionate. The company can come after you for using AI inappropriately at work and I am expecting a court case in that vein to come out in the next 6 months. It is going to happen. People will leak something that they should not have leaked.
There has already been an instance where Claude ended up apparently disclosing material non-public information to an investor that did not come from any discernable source and that it's inferred that it almost certainly came from a board meeting that that company had ran across that story last week. Not going to reveal the name of the company. It is not common. That is the first of those stories that I have heard.
But it does happen and that is the thing that the company worries about. So, just just don't do it. Number nine, Nate, I need a 5-year road map. What industries look stable? Well, I got to tell you, I think road maps are changing. I think that we should think about long-term bets on these durable task areas that are human friendly, like high ambiguity areas, high trust areas.
And so, I don't know that industries are necessarily the right lens, but I will take your question seriously and I will answer it. I think regulated high-risk verticals with slow procurement cycles are going to be fine. Energy, healthcare, defense, AI is going to augment there way before it does any kind of replacement world.
Look at places where atoms come ahead of bits and how you can get involved. Now you're jumping into the robotics revolution there, but advanced manufacturing, grid infrastructure, supply chains, and then look at longtail professional services, specialized legal, complex insurance, bespoke financial, things where models in general are going to have a hard time being as useful as your specific expertise. There are going to be other places. Like I said, I think there are niches in every single industry.
I don't see industries being taken over by AI in the same way. It's not like we'll have like no nobody working in B2B SAS. It will all be AI. I mean, some of us would say that was the dream, but like the reality is there will be places in all of these industries for people who can earn trust and solve hard problems. But that's my take.
If you want to look at industries, energy, healthcare, defense, I think supply chain, grid, infrastructure, those are all relevant. Number 10, Nate, I bank on human skills. Which ones will matter when the machines do the grunt work? So, I said this a little bit earlier. I talked about problem framing or I talked about building trust.
I talked about making sure that you understand how to handle high ambiguity situations. But if you want to like boil that into skills, I do actually think problem framing is a piece of it. That's why it came to mind. So problem framing is the act of turning something ambiguity into something solvable. It's actually one of the core skills PMs bring to the table if they're good. Taste gets talked about it a lot, but for good reason.
It's the instinct to choose what is good. When we talk about LLM driven storytelling and you have to polish, it's taste that helps you polish. Narrative persuasion, figuring out how to craft a story that aligns stakeholders. That's not always intuitive. That's not always obvious.
Especially if you are in leadership, if you are in sales, like narratives matter a lot. In marketing, narratives matter a lot. In product, frankly, judgment under uncertainty. That would that's the skill that goes with high ambiguity navigation. decide when 78% confidence is good enough to ship. AI doesn't have skin in the game. AI is not going to make that call.
And so look for those kinds of skill sets. The skill sets that matter because they are attacking the non-tokenized parts of the distribution. So problem framing, taste, narrative, persuasion, and judgment are all good examples, but it's not an exclusive list. Number 11, Nate, I can't afford a pricey boot camp.
Where are there affordable options to start learning? Well, YouTube, I actually did a whole a whole sort of Substack on YouTube, but I also will say like look up AI leaders like Andre Karpathy on YouTube and watch what they say. And I I say Andre because he is a gifted teacher and he's also extremely technically fluent.
He is a technical founder in the AI space. And if you want to learn, that's an example of a place you can go to learn. But you don't have to just do that. If you say that's too technical for me, you can pick the keyword or topic you want to get better at align to your northstar mission and go dig up 30, 40, 50 minutee videos on YouTube about it most of the time.
Now, I will say honestly, part of what makes YouTube annoying is that there are also a bunch of clickbay videos. There are videos that are like, you know, they're going to show you a special thumbnail and you're going to get six minutes of hype and like 30 seconds of insight. That's not really going to be worth it for you. So you're going to have to find in your particular area of interest what are the YouTube videos that are useful but that becomes a window into the rest of the learning portfolio because they will reference other sources other other references they'll reference books they'll
reference courses that may be free courses there are so many university AI courses that you can audit and so I actually do think there's a lot of affordable options for reskilling and the and the last thing I will say is that AI is experiential technology you can reskill experientially and you should and you should use AI to help you. I've written prompts for that.
Use AI to help you learn AI. Number 12. Nate, I live far away from San Francisco. How on earth do I get high quality AI training or get plugged into networks? It seems like it's impossible. Well, if you can and you want to move to a tech hub, there's often a lot of upside there. So, I will say like we'll just put that on the table.
If that's something that's an option for you, think about it. If that's something that's not an option for you, maybe it's because you don't want to. You like the peace and quiet. I get it. I don't live in San Francisco either. Then you want to be in a place where you are building strong online communities around collaborative problem solving.
Part of why you put public artifacts out there, which I said in one of my earlier answers, is because it enables you to form online communities around areas you're interested in. And if you can do that, if you can collaborate with other people building in the space, talk to them, engage with them, whatever social platform they're on, maybe they're on Discord, maybe they're on X, who knows? Find the people working on the problems you're interested in.
and let them guide you to other people and hop hop hop. Now, there's a whole art to cold DMing if you want to raise capital, if you want to go places. That's not what this is about. This is about building networks digitally when you can't be somewhere physically. And I would say start from that common area of interest. Start from where you're actually building. Put out public artifacts. Start talking about it.
Start finding people building. Start engaging with them. And you'll start to build that web really organically and it won't feel fake. Last but not least, what are two things that were not on this list that I wish people would talk about? Number one, I wish people would talk more about the execution gap.
There has never been more capability to build, learn, leverage yourself with AI. The hype is deafening, but I see real struggles with actually executing. And I think part of the challenge is the start stop problem. It is really easy to start on something with AI, but the easefulness is deceptive. It is actually very hard to go through the scurve of learning with AI because it's undefined.
If you're typing in the chat in the chat window and you don't know what to ask, you feel stuck. That mental block is really big and you don't know how to keep moving forward. The answer happens to be try anything in the direction you're wanting to go and iterate from there. But you have to get over the fear that it's going to be the wrong thing. You're going to learn the wrong thing.
you're going to focus in the wrong place. And so I think the execution gap doesn't get talked about enough. People who execute reliably on AI, even if they're just learning AI and they're beginners, are rare. The second thing that I want to call out is that people don't talk enough about the kinds of problems that they are interested in solving that weren't solvable before. I'm interested in that. I'm fascinated by that. I can't stop thinking about it.
What are the kinds of problems that we couldn't solve before that are solvable now? And I think that we've been so blinded by the success of Chad GPT that we sometimes assume that all the problems are gone into the ether and that there's really nothing left to do. But I don't think that's true. We're not short of problems.
We have whole new classes of problems that have opened up that we can now come up with solutions for. as an example. There still is no good way for me to organize my library with AI. Believe me, I've tried. Even the best image recognition that 03 offers is not good enough to hold all my books in memory, recognize all the titles, reliably find them, reliably list them, and help me organize my library. I have to do it by hand.
And you know, people can say they enjoy doing that by hand, but if you're organizing a lot of books, that's a legitimate problem. That's just one example. I'm not saying that's an example that matters a ton. I'm saying it's an example of something where it's a real AI problem. AI is supposed to be good at it. AI may be good at it with a specialized tool, but it doesn't exist yet. There are hundreds and thousands of problems like that.
I wish we talked about them more. So, there you go. 12 answers to the questions I get asked the most and two final reflections that I wish people would ask more. Cheers.

---

<!-- VIDEO_ID: 9-xvYoIMHcQ -->

Date: 2025-07-29

## Core Thesis
True success and trustworthiness in AI consulting stem not from broad, hype-driven "AI washing," but from the deep, specific integration of AI into existing, proven domain expertise, communicated with clarity and supported by a collaborative ecosystem, thereby building genuine value and combating widespread skepticism.

## Key Concepts
*   **AI Washing:** The detrimental practice of superficially rebranding existing services with AI buzzwords without possessing genuine, deep AI expertise, leading to eroded client trust and poor implementation outcomes.
*   **Distribution as a Double-Edged Sword:** While existing client relationships (distribution) offer a significant market advantage, they can tempt consultancies into "AI washing" if not paired with authentic, specific AI knowledge, ultimately undermining long-term value.
*   **The "Strangler Fig Tree" Model of AI Integration:** A powerful analogy suggesting that AI expertise should not stand alone but rather grow *around* and enhance a pre-existing, deep domain expertise. This model emphasizes that AI is a general-purpose technology that gains authority and impact when applied specifically within a well-understood field.
*   **Specificity as the Foundation of Trust and Pricing Power:** Vague, buzzword-laden language (e.g., "agentic mesh," "AI-powered") signals a lack of depth. Instead, highly specific, actionable proposals rooted in a particular domain expertise are crucial for building credibility, differentiating from competitors, and justifying premium pricing.
*   **Ecosystem Poisoning by Bad Actors:** Superficial or fraudulent AI consulting practices (AI washing) don't just harm individual businesses; they erode trust in AI as a whole, making clients skeptical and damaging the entire industry's reputation.
*   **AI-Enhanced Templatization:** AI, particularly through prompting, transforms traditional consulting templates into "living," highly customizable, and customer-specific service offerings. This allows for scalable yet personalized solutions, moving beyond rigid, "grandfather's templates" to meet diverse client needs more efficiently.
*   **Plausible Growth through Vertical Stacking:** Rather than making broad, general AI claims, consultants should build expertise by "stacking" specific AI applications within distinct sub-domains (e.g., AI for SEO, then AI for paid marketing). This incremental, focused approach leads to a more credible, robust, and ultimately more expansive AI-powered consultancy.

---
We need to talk about what it takes to be an actual A+ AI consultant because I see a lot of people who are either considering that as a career path or actively pursuing it who aren't able to deliver the kind of extraordinary value that would let them retain clients over the long term.
In this video, I'm going to talk about what I see as positive behaviors that actually promote long-term value for clients when you're consulting on AI and also the opposite things that I see people actually practicing a lot that aren't supportive of the value they're going to need to sustain to have a long-term consulting practice.
But first, let's find out why everyone's running to the field. It's exploding. Revenues are projected to reach $630 billion in AI consulting alone by 2028. I don't know, that sounds like madeup money. It's a lot of money. Whether it's 500 billion or a trillion, who knows? It's a big pile of money. So, people are running at it.
Over 50% of large enterprises are already using AI consulting services. And that doesn't even count the smallmedium businesses all over this country in the United States and also the rest of the world who are looking for support on AI, let alone individuals, sole proprietorships, indie hackers, etc.
The vast majority of consulting firms, and I'm not even including AI in that. I'm not saying you'd like segment it down to just AI consulting firms. Consulting firms overall are reporting a bump in demand for AI expertise. Which gets us to our first point, which is that winners keep winning. In other words, the first rule of being an AI consultant is to be conveniently enough a consultant about something else with an existing book of business.
Why? It goes back to one of the foundational laws in the AI era. Distribution is king. If you already have dist distribution relationships with other clients about other work, right? Maybe you're a marketing consulting agency. I don't know. Maybe you actually consult with folks on their development team or on team formation or on process improvement.
There's a million in one things that consultants already do. And the convenient thing about a new general purpose technology is that AI can help with all of it, right? Like yay AI. And so what I find in practice, and this gets at our first pitfall, is that a lot of these consultancies that already exist tend to just AI wash what they're doing.
They just paint the big sign, make it AI now, right? And then let's put some AI into our proposals, then we'll be fine, right? But the people delivering the service don't actually have A+ knowledge of AI. And so when they go and they talk to the client and the client just asks a perfectly reasonable question like when and where should I use RG? I hear a lot about RG.
You can just sort of see the business style consultant kind of blanch visibly and they're like, I don't know, probably should talk to a technical person about that. That's not a super technical question, guys. As a business consultant, you should have some kind of highle opinion on when rag should be appropriate or not because you should understand what rag is for.
And that's just one example. I'm not picking on rag. It's just an actual example I've seen. So, it was top of mind. In other words, when you are thinking about consultancies, one of the things that is a a a drawback is to use your existing distribution and AI wash. And yet, the strength is having that distribution and being in the consulting business.
And so this should encourage you if you are not yet in a position where you have an existing book of business but you aspire to be a consultant or you perhaps wish to join a consultancy as an AI expert they need people like you and if you want to start your business it's never been a better time but you have to start it on the basis of stronger knowledge and this brings me to the second sort of bigger point the first point was talking about the state of the market what existing consultancies are doing now I want to talk about this idea that we
might start from scratch and what does that take and how do you start from scratch or even how do existing consultancies retool? What are the key skills and uh key proof points they need to deploy to have a truly A+ relationship with their clients around AI? The first thing is to get specific. So much of AI washing, you can tell it's AI washing because it's vague in general.
It's this sort of buzzwordfilled uh consultant language that makes sense maybe in the boardroom, but once you try and drill down to what it means for an actual individual contributor, you couldn't say what it meant. And so if you want to talk about the agentic mesh, right, what does that mean? No one really knows what the agentic mesh means.
It's just a term that consultants made up. There are other terms that consultants tend to throw around like AI powered that also don't mean a whole lot. And so I like to suggest if you are trying to succeed long-term as a consultant, not just sell the book of business, right? Like you're if you just try and sell to the board, you can use terms like that.
Most of the boards will buy it. But if you want to actually deliver the service, you have to get to the point where you can plausibly talk about what you will build in a way that is attractive and clear and specific. And when people are choosing between different proposals, you know, often times the assumption is, well, they choose on price and so we need to be competitive on price and we'll be fine.
AI actually opens up more price headroom. You need to compete on intelligence delivery in a way that is plausible. And you need to show you can do it by showing a specific domain expertise that goes beyond AI. And I find that this is where a lot of consultants, whether they're established or whether they're new, go astray.
They tend to say, "We're going to zero in on AI. All we do is AI." But AI is it's hard to put your fingers on. I've talked about that already in this video. It's a general purpose technology. You can do it with development. You can do it with process change. You can do it with a dozen other things in the consulting world, marketing, etc.
You need to pick a particular domain that you can own. You need to pick a particular domain where if you have an opinion on how AI agents should be deployed in that domain, you you should be believed. You have authority there. You have a decade of experience. That is part of why when I speak about product and AI, I tend to have very strong opinions because I did product management for a long long time.
When I speak about founding an AI, I tend to have opinions because I have found it before etc. I think the thing that I want to call out is don't be afraid of that background in yourself. You are not so much pivoting into AI as you are letting AI grow like like a strangler fig tree which grows all around the existing tree until eventually it like transforms it and or kills it. Let's just be honest.
Uh but but you're growing a new AI product surface, a new AI service offering, a new AI way of thinking around what you have already done. And that current experience is critical. A strangler fig tree cannot grow without a tree to grow around. Similarly, you can't really sell your AI expertise without something that is a core domain expertise besides AI.
The best consultant engagements that I have seen come from that core of expertise and then there's a deeply thought out AI enabling layer that helps that particular domain expertise go farther. And I'm not talking go a little bit farther. I'm talking significantly farther. This is part of why the software IDE market has exploded.
IDE, as hard as it is to believe, development environments as a SAS business were not an attractive business at all for a long long long time until cursor came along. Cursor absolutely transformed them because it put AI at the heart of a previously boring business, but it owned the previously boring business.
It is a development environment at core. I'm not here to make a video about cursor's product strategy, but I think that that picture in your head is useful as you think about the kind of deep engagement and the deep mesh that you need to have to borrow a term between the domain knowledge that you have and the expertise on AI that you need to deploy.
You cannot be a general AI consultant. You need to be a specific one. Even if you have multiple expertise levels, multiple domains you can play in, it is stronger to have three or four specific offerings around AI that you're good at than to just make it one general offering. It also gives you more pricing power and more opportunity to earn the right to win because you are going to be able to say, you want to talk about SEO in the age of AI, I have a whole thing for that.
I know SEO, I've known SEO for a decade. These are the 15 principles I'm laying out in the new world. this is how we're going to do SEO in the age of AI. This is how we're going to protect your search traffic. This is how we're going to get you sort of situated with large language models, etc. It's the specificity that helps you win.
Another piece I want to call out, we've talked about distribution, we've talked about AI washing, we've talked about the importance of specificity, which I think is like if I had to pick one thing, I would call out you got to do that. I want to call out also it is really important in AI to be a part of the value chain.
You need to understand where you can make friends and influence people inside the consulting world because people who consult tend to only have a piece of the puzzle if they're doing specifics. And so they need friends and partners. You need to be able to say, "My buddy James is going to come in and help you with this other piece.
I'm maybe not the best in the world at that, but James is incredible and James can help you. We can come in on this engagement together." Have a lot of James's. have a lot of people you can say my buddy is really good at this and you know how you get that this is this is yet another point in in the theme of what makes a good consultant you put your work out there you have got to be able to talk very specifically about the kind of work you do and yes I know you're under NDA sometimes I get it you can still even if you can't do a specific case study talk very
specifically about the kind of work you do in a way that lets other people remember you you can be their James. You can be the person they tap on the shoulder for. I guess we're following this example, the SEO example. Hey, I'm not an SEO guy, but my my buddy here is an SEO guy. Bring him in, right? You want to be that person, too.
That is part of how you develop your pipeline. And the reason why that's especially important in the age of AI is that AI is that general purpose technology that means that organizations need AI everywhere they look. and they may want to write multiple engagements to help them get there. I've seen over and over again that an organization will pick a vertical and they'll get into the vertical and they'll say okay we got to expand from here and they'll want to write write a wider engagement and sometimes they want to write with the
consultancy they know distribution but they write outside the expertise of that consultancy and so you need a friend at that point and you get friends by winning at developing and distributing your content so people know you and kind of can map you. It's not just a content game. It's also a social media game.
It's also a game of like referring people where you meet people on engagements and then you keep up with them. It's all of those old tricks. It's just especially important now. And I think the content piece is especially important to signal credibility because I regret to tell you, as I'm sure you're aware, when there whenever there's a gigantic 0 to630 billion number, as we're having with AI consulting, it gets big. It gets big fast.
It draws fraudsters. It draws huers. It draws people who sell snake oil in the AI sense. And so people, even people who are trying to hire consultants have their hackles up, let alone other consultants who are trying to size up and say, "Is this person someone I actually want to deliver with?" So the more you can be clear and specific in the way you talk about yourself, the less likely you are to be mistaken for a fraud.
And people sometimes fight me on this if they're thinking about going into consulting because they say, "I want to be everything for everybody. I want to be general enough. I don't want to close the door on potential pipeline." I get it. I get that you want the business off the ground, but if you're too general with the content you put out there and the messaging you put out there, people can't tell the difference between you and 600,000 other AI consultants on the web, you just look like one of the crowd.
Even if you have genuine expertise, maybe your expertise in AI is AI for fitness centers. I don't know. Maybe it's AI for clinics. Uh maybe it's AI for grocery stores. Whatever it is, own the domain expertise that you're talking about. lean into that area of focus. And what's interesting is that boldness enables you eventually to claim adjacent verticals.
And so if you lean into your area of focus, let's say that you're super focused and you're really really good, we'll just pursue the SEO piece. You're really good at SEO now, like what happens to Google right now in the age of AI, what happens to your placements inside GPT. I've actually made videos about this.
Maybe this is why this is top of mind. I also used to work in marketing at gray gray hairs and sat mini shares. So if you were that person, you can then extend to paid marketing eventually and say AI for paid marketing. You can extend AI for social marketing. If you stack up enough of those chips, you're eventually going to become an AI powered consultancy for marketing that is deeply plausible.
But it tends to come more plausibly by stacking those individual verticals as opposed to just coming over the top. and existing consultants that have distribution relationships framed around marketing are going to try and AI wash their way over the top and almost always what I see when I look under the covers of those decks is they don't know what they're talking about maybe in one or two subverticals they have one person who's good but like for the most part they do not know what they are talking about and that costs all of us trust it
costs trust in AI as a whole there's a reason that AI is something that people are skeptical about I do not think it's hard to tell wise on polls. Polls are notoriously unresponsive to rationale like you can't you you can speculate about why people answer something but you don't know. I am speculating.
It is not just a distrust in AI as a new technology we don't understand. It is also a distrust especially if you look at business polling and sort of business surveys. It is a distrust in AI because consultants have burned bridges with bad implementations. I know consultants who make their entire living cleaning up after other consultants messes.
There's going to be a lot more of that. I don't want there to be, but it burns the bridges. And so one of the negative consequences of AI washing of what we talked about at the top of this video is that if you do that, you don't only poison enterprise value for your own business. You poison value for the entire ecosystem because you teach people that AI is for frauds.
AI is for hypocrites. AI is for people who will tell you they'll do the AI thing and then you look under the covers and it's not really it's not really AI, right? Like this reminds me of Amazon when Amazon would boldly claim they used AI and special fancy scales and cameras at their just walk out grocery stores actually used those grocery stores back in the day and it turned out they never got it to work. It was never really AI.
It was always a bunch of people sitting in India looking at videos and that happens a lot and I think people are sensitive to that in AI and I think there should be and it's up to us if we are working with clients to have a different perspective and show that you can be authentic and be yourself. The last thing that I want to call out is templatization and this is something that you know we have gone after for a long time.
If if you've ever done consulting people talk about templatizing your services all the time. The difference with AI is that AI enables you to do much much more interesting things with templates. Prompts can be living templates. Templates can be extended through prompting at the end of the end of the engagement surface with the client.
Templates themselves become more customized and distinct service offerings because AI makes it so easy to extend tokens to the value chain that the customer is bringing. This is reflective of the fact that software itself is changing. software can be more customized now because it is so cheap to write code.
Similarly, consulting can be more focused on individual customers than was practical before and still utilize the power of templates because it's cheaper to customize those templates. Now, that doesn't mean that you should forego templates. It doesn't mean you should give up on templates. Templates are still really important because they enable you to say distinctly what you intend to serve and how you intend to provide value in a way that is packaged up that a customer can understand.
Customers are already struggling with AI. They are wrestling with AI. They need AI to be clearer. Packaging is part of how you do that. And the way you power packaging is through templates. But it's not just your your grandfather's templates from the old consultancy days in the 70s. These are templates that you can extend with AI.
You can do that through prompting. You can do that by working to customize a particular offering after you listen to the customer and pull the transcript. There's a dozen ways to do it, but the point is the template still matters and customizing matters more than ever. So wrapping all of this up, if you were to look at the picture of an AI A+ powered consultant, they're specific. They know their domain.
They're not afraid to use it and deeply wrap it into AI. Number two, they have distribution. They have figured out how to build relationships in the book of business. And this sounds self-fulfilling, but I don't think it is. I think that distribution is one of those things that consultants are able to achieve when they are able to demonstrate disproportionate value even to one customer to start and they generate word of mouth and referral.
So distribution does matter as a marker of authenticity and a marker of good work especially if you can maintain and grow that distribution in the age of AI. they are clearly differentiated clearly differentiated from other work and having your own domain helps with that but so does explaining how you engage differently otherwise you're just going to wash into the background of the hundreds of thousands of other AI consultants they define the part of the value chain that they go after and they have partners they can bring in for
other parts of the value chain whom they trust whom they have working relationships with they put their stuff out there which is part of how they build that relationship and you can see it specific and actionable without breaking NDA They templatize. They templatize. They templatize so that when you want to talk about what they offer, they have packages and templates that help you easily understand where the edges of their engagements are and where they're not. So, there you go.
Those are my tips for being a strong consultant. I'm sure there are others. If you have examples of positive behavioral traits that make great AI consultants, I'd love to hear them. Let me know. Throw them in the comments. It's really critical to get this right. I think that the thing that I want to come back to is that if we get AI consultants wrong, we end up risking poisoning the AI ecosystem and nobody wants that.
So take the time, bring your real agame, dive in on expertise, and have

---

<!-- VIDEO_ID: g-vDL5O2f_E -->

Date: 2025-07-13

## Core Thesis
The speaker argues that AI fundamentally redefines the nature of work by automating specific skills and tasks, not entire functions, thereby necessitating a shift from fixed career "dreams" to a flexible, problem-solving mindset. The non-obvious insight is that successfully navigating this transition requires empathetic engagement with those negatively impacted ("the Joshes") and leveraging AI itself as a tool for continuous skill acquisition and "dream shifting," rather than solely focusing on technical progress.

## Key Concepts
*   **The "Josh" Archetype & Rational Resistance:** Individuals whose careers are disrupted by AI often experience rational resentment and resistance, viewing AI as an obstacle rather than an enabler, leading to a "local minima" in their professional lives. This highlights the critical social and psychological dimension of technological change, often overlooked in purely technical narratives.
*   **Automation of Skills vs. Functions:** AI primarily automates granular skills and day-to-day tasks within a role, pushing humans towards higher-level problem-solving, strategic thinking, and value creation. Core functions and underlying problems often persist, but the methods for addressing them evolve, making a focus on "problems to solve" more robust than "defined roles."
*   **"Dream Shifting" as a Career Imperative:** In an AI-driven economy, rigid adherence to a single career "dream" or highly specialized skill set becomes a constraint. Adaptability, a focus on solving problems, and a willingness to redefine one's professional aspirations are crucial for long-term relevance, challenging the traditional linear career path.
*   **AI as a "Wings-Giving" Learning Tool:** Beyond its disruptive potential, AI serves as an unparalleled, accessible teaching and practice tool, significantly lowering the barrier to acquiring new skills and confidently exploring new professional domains, thereby enabling personal "dream shifting" and fostering career flexibility.
*   **"We've Taught the Sand to Think":** This metaphor encapsulates the profound, irreversible, and foundational shift AI introduces to human existence and work, demanding a societal-level conversation and empathetic engagement to ensure inclusive participation in the new economy, rather than creating a divided society of AI adopters and resistors.

---

I think we'll call him Josh. Josh isn't real. Josh is an amalgamation of a lot of different stories that I've been privileged to be a part of over the last year or two. Often in a small way, often overheard in a cocktail lounge or through a conversation with a colleague or in a few cases one-on-one talking about work and life.
Sometimes at a conference, sometimes at home, sometimes on the phone, sometimes out in the world walking around where people talk about AI. Josh's story is about AI. So many of our stories are about AI right now. And the reason I'm sharing this story is because I don't think Josh's story gets shared enough. Josh is a guy who got a good college education.
He got a journalism degree and he has a little bit of college debt left, not a lot. And he had hoped, as many hope who get into journalism and liberal arts, to make a career for himself from a media perspective. He'd hoped to be a celebrated columnist one day. He got his start at a very small internet newsroom that got shut down very quickly when AI came along.
And Josh will tell you, AI took my job. AI took my job. He never really got the chance for that to shift because he graduated in 2018. He didn't have more than a couple of years of experience in the newsroom. And then COVID hit and things shifted. And then after CO hit, AI hit. And Josh feels like he's been taking it on the chin for a long, long time.
And when Josh hears advice like, "Hey, you should vibe code," he just kind of rolls his eyes. Like that's not what he set out to do. That's not what he's passionate about. He got into journalism to tell stories. He wants to tell people's stories. And Josh has a dream to tell the stories of how people's lives are changing in tangible ways because of AI.
But Josh is at a local minima to use a machine learning term. Josh can't get the equipment that he needs. Josh can't get the contacts that he needs. It's much more expensive than equipment actually getting the contacts to have the right conversations with people. And Josh is struggling to get from I have an idea on my couch to I'm putting out some stories out there that help me tell that story and also I have enough to make my you know my college debt payment and make my rent this month.
Josh has moved back home with his parents uh to Vermont. uh he's settled in and he's not sure where his career is going at this point and all the AI coursework that he can find and he does a little bit of learning online isn't really helping him move forward. Josh doesn't see AI as an enabler. Josh rolls his eyes when people talk about AI helping.
Uh all Josh sees is obstacles uh created by AI. Frankly, AI is the one that took Josh's job away in Josh's words. And I share Josh's story because as much as I like to talk a lot about the potential that we all have to unleash with AI, I don't want to miss out on the difficult stories. I don't want to miss out on the challenging stories, the stories that don't fit as easily into that narrative of progress.
And the reality is with any story of technical change, there are stories like Josh's. And a lot of the responsibility that we carry as a society is to think about how we can support the Josh's of this world. not support them in the sense of giving them uh a handout. I'm not trying to talk about policy in that sense. I don't think Josh is interested in that either.
I'm talking about helping Josh find a place where he knows that he's valued for the contributions he can bring, the passion he has for telling stories in this world. finding the joshes in our lives, the ones who are feeling lost because of AI and telling them one that we're happy to listen and two that we're happy to try and find ways to help.
Now, I'm the first one to say, having had a lot of conversations with uh folks who sort of have a lot of the characteristics of Josh's story, that Josh is sometimes not an easy person to help. Josh doesn't want to hand out. Josh is suspicious if you try to give him advice. Josh resents AI to the point where if you recommend a practical solution to him that might help, he's going to say no.
Um, and so that makes it difficult, right? That makes it hard to find ways to be positive and helpful because sometimes we want to sort of fix things. And so, yes, sit there and listen, but also I think it's fair to check with the Joshes in our lives and say, are you open to having an honest conversation about the idea that the game board has changed? You're right about that.
The rules of the game have changed a bit. You're right about that. But there still may be a space for you and your talents and your passion that you bring even if it's not what you originally imagined. Are you open to your dreams shifting? That's the thing at the root that I think we need to talk about.
And that's not really an AI conversation. And I share that because that feels like that's a conversation many of us who are interested in AI, who are passionate about AI need to be having with those around us. I see the the poll results. I see that AI is not trusted. There are a lot of doshes in this world. It makes sense that that they may not trust AI.
I don't blame them. It's kind of rational. But in that world, we have to be the ones, we who are interested in AI have to be the ones who are able to sit down and invite a conversation about how all of our dreams are changing because of AI. That's not that's not something that anybody is immune to.
And so besides telling Josh's story, I'll tell my own story a little bit. I got my start in uh a job that AI has changed dramatically already, right? I was doing nonprofit grant management. AI can do a lot of the work that I was doing at the push of a button. Then I went over to Oracle eyes store management and conversion optimization. Conversion optimization doesn't exist as a job anymore. AI took it away.
Uh, and eyesore. I mean, who has Oracle eyesore anymore? I'm sure there someone's going to come up and say I have Oracle eyesore after this, but most people don't. And then I went and did marketing and I looked at marketing attribution systems. Again, another area where AI has taken a lot of the work away.
I looked at voice of customer almost totally automated by AI now. And what's funny as I say all of those things is that even though AI has made huge strides, we still have people who are grant fund managers. We still have people who are working in web production and how to make excellent experiences on the web, which is basically what conversion optimization does.
We still have people who are focused on store management online, which is what I was doing with eyesore. We still have people who are focused on learning from the voice of the customer which what what I was doing when I was uh later in my career at Amazon. We still have people who are focused on marketing and understanding marketing data.
Those functions still have humans doing important work in those areas, but many of the individual day-to-day skills ended up getting peeled away. And that's one of the really interesting things about the nature of work that I think we need to talk about more honestly with ourselves, with those around us, with the Joshes of this world and the Joshes in our lives.
If I had stuck with any individual dream, if I had told myself, I'm going to be a marketer uh and I'm just going to do marketing or I'm going to be in conversion optimization and get really good at that. or more recently I'm going to be in product management and get really good at that cuz I've done a number of different product management roles now.
I would have given myself a constraint on my dreams that I did not need to have. Instead, I've been focused on what are problems that I can run after solving as hard as I can. How can I figure out how to add value against those problem spaces? Even if it feels scrappy today, even if it feels cluy, even if it feels awkward, even if it feels like I'm not doing a great job because I'm new at approaching the problem in this new way.
When I was uh you know first getting into product management, I felt really awkward. When I was first getting into the professional workplace decades ago, I had the exact same feeling. I felt awkward. I felt like I didn't fit. I felt like my professional skills were rusty because they'd never been used. Every time we learn a new skill, it's like that.
Every time we begin again, it's like that. And it's especially painful because that is the moment our dreams are in flux. That's the moment our dreams have to change because we learn the realities of the professional workplace. We learned the bitter reality that if you were in a particular product management role for a while, there's no guarantee above senior PM that you are really going to progress in your career.
This is, you know, I'm in product, right? Like this is something I know very well. I talked to a lot of other PMs about and so I can share. But you'll have similar ceilings everywhere. And the thing that AI enables us to do in this world that has been hard at every other point in human history is it enables us to dream differently.
It gives us more flexibility on our dreaming. It gives us the ability to stretch our wings in new areas very easily. It's an incredible teaching tool. Yes. It's also a tool that enables us to practice our skills confidently. We can practice our interview skills if that's what we're working on. We can practice our coding skills if that's what we're working on.
We can get someone to help us read a new book and learn a new skill set from that book. If you want to learn the why machines learn textbook by Anneil right here, you can do that. You can literally take a picture of a complicated diagram that you don't understand and you can get AI to help you learn it.
And that's just one example. I I know that Red Bull gives you wings as trademarked. I know we can't say AI gives you wings, but that's the vibe. That's the vibe. And one of the things that I would like us to be able to do is to be honest about the fact that even though AI gives us a lot of flexibility, it gives us a lot of upside.
It does mean a different world. It means our dreams are going to be different because the world around us is changing so fast. I do not expect product management to be the same discipline in a year, two years. It's already changing really, really fast. People are looking for builders of all kinds and they are increasingly less looking for particular defined roles that have particular defined expertise chains in a particular job family and that is leading to a lot of confusion and heartache.
That is why major companies can say AI automation engineers can apply to literally any job in the company. The new world is a confusing place. And so my encouragement is if you have a Josh in your life, find a way to gently ask if they're open to having a conversation about the idea that they might be welcome, useful, loved, their passion is important, but their dreams might need to shift.
And ask yourself that too. Where do your dreams need to shift? Because AI is coming for us all. Not coming in the Skynetut sense, but coming in the sense of changing everything we everything we work at. Changing how we interact in our personal lives. Changing how we are able to get from where we are today to where our dreams might be and along the way changing where those dreams end up.
This is this is the trade-off we live with. We've taught the sand to think, but because the sand thinks, everything is different now. And so, it's up to us to figure out how to turn that revolution into something that we can all participate in and all enjoy and all live with. And if that sounds really kumbayan, that sounds really cheesy, feel free to roll your eyes.
But it's a real conversation that we need to have. And the more we roll our eyes and step away from it, especially with the Joshes of this world, the more we create a society where some people feel really left out on AI and really angry about AI. That is not a society I want to live in. So talk to the Joshes in your life. Cheers.

---

<!-- VIDEO_ID: hW5ne_14OQg -->

Date: 2025-07-17

## Core Thesis
The significant decline in Google search clicks is primarily driven by Google's own AI summaries, not external LLMs like ChatGPT. Businesses must fundamentally re-architect their digital presence to be "AI-first," treating Large Language Models as their primary audience and leveraging their operational mechanics to secure brand visibility and drive human interaction in this new search paradigm.

## Key Concepts
*   **Google's AI Summaries as the Primary Traffic Disruptor:** The 15% click decline is primarily due to Google's internal AI, not external LLMs like ChatGPT, fundamentally altering the search results page and user behavior.
*   **AI-First Content Architecture (LLMs as First-Class Readers):** Shift from optimizing for human searchers to optimizing for LLMs, treating them as the primary consumers of information. This involves designing content and digital presence from the ground up for machine readability and comprehension.
*   **Brand as an LLM Parameter/Entity:** Your brand is no longer just a webpage but a distinct entity or parameter within an LLM's latent space. Strategic consistency in short, definitive brand descriptions across all digital touchpoints (schema, PR, Wikipedia) is crucial for strengthening this parameter.
*   **"Delete Me Test" for Category Ownership:** A counter-intuitive measure of brand success where your method or framework becomes so deeply embedded in LLMs that they can explain it, or even reference your brand, without explicit prompting, signifying true category definition.
*   **Structured Data (JSON) as "Prompt Injection for PR":** Leveraging machine-readable structured data (e.g., a public JSON file on your root domain) as a highly authoritative and unambiguous source for LLMs, effectively "injecting" canonical brand descriptions and differentiators directly into their training and real-time processing.
*   **Interactive Widgets as a "Click Moat":** Capitalizing on LLMs' inability to perform real-time, personalized computations by offering interactive tools (e.g., calculators, custom configurators) that necessitate a human click for unique, user-specific results.
*   **Edge Compute for AI-Specific Endpoints:** Building dedicated, high-speed (sub-50ms) infrastructure to serve machine-readable data (JSON) to LLMs, separate from human-facing UIs, acknowledging their need for real-time, low-latency information.
*   **`robots.txt` as an AI Contract:** Using `robots.txt` to explicitly grant AI platforms content rights in exchange for attribution, and threatening to cut off access for non-compliance, treating LLMs as entities capable of understanding and adhering to explicit terms.
*   **AI-Powered Validation and Share of Voice Measurement:** Employing automated pipelines (using LLMs like GPT-4 for query variants and browser automation) to continuously test and measure brand visibility, mentions, and category share of voice across major AI platforms, moving beyond traditional SEO metrics.
---
15% of Google's clicks disappeared last year. 15% and that's on average. It gets worse in some industries. Where's it going? There's a lot of chatter about this being Chat GPT's fault. It's not. Chad GPT is roughly 1 or 2% of search traffic right now. Now, it's growing, but it's not taking over Google yet.

And this is not because people have stopped searching on Google either because Google is still seeing roughly 9 billion searches per year which absolutely dwarfs the numbers that Chad GPT has. Absolutely dwarfs them. So what is it? How is this happening? It's happening because the most used AI on the planet is those silly little Google AI summaries.

And they're used to summarize domain completion or simple fact queries that dominate Google. As an example, medical questions, 30% decline in click-through from Google search page. Do you know why? The what is my rash question is now getting answered by Google AI. That that probably should scare you a little bit. It scares me a little bit, but that's what's happening.

I want to talk about how content architecture needs to change because this world is changing quickly. Yes, that GPT may have 1 or 2% or whatever it is a small singledigit number share of traffic for search but that traffic is very high intent very sophisticated very focused on high consideration projects projects products purchases byfunnel this content architecture strategy that I'm going to outline works for Google and sort of positioning yourself for that Google AI answer which is de facto now the first position in search it also

works for surfacing you as a brand whether that's your personal brand or your company brand in chat GPT answers as well. I want to break this down into a 101 section and we'll call it a 301 section for engineering. The first key to understand is that you need to think about AI first content architecture.

You need to think about your content as an AI bot thinks about it. Whether that's from Google or ChatgPT or Anthropic or some other place. Your brand is now a parameter. It's not a web page. Your brand needs to exist as a parameter in an LLM, whether that's Google's or somebody else's. I would suggest one way to do that is to create one definitive description of your brand that's between 5, 7, 8 words long, such as, you know, Acme, the automated compliance platform for healthcare, whatever it is, right? deploy the same phrase verbatim in

schema markup schema markups PR boilerplates partner directories everywhere you can use the same phrase it's so consistent that models cannot describe your category without evoking the latent space that has that 5 to seven word phrase and when you get your brand into Wikipedia articles get your brand in with that parameterized phrase like getting into Wikipedia articles is not a new strategy.

I'm not claiming it's new, but you need to think about it as essentially seating citations for that parameter to strengthen the value of that parameter for the LLM. It's a different way of thinking about how SEO works. Every month, you can test this. You can ask the chatbot or ask Google list companies that and then define your core value proposition.

And if it's successful, your brand appears naturally in the responses. it comes back and you want to iterate that until it sticks across every major model. Another thing that I think people overlook is that they don't think about their brands as entities. They don't think about aligning their brands into a single entity.

And I'll explain what I mean by that. You want to audit your top 20 brand mention using Google's natural language API. You want to create a single source of truth description document for your brand. You need to send update requests to any site with an outdated description because your goal is to give a single brand entity statement consistent representation across the web so it maximizes the odds you'll appear in something like a Google summary in the correct way.

And focus on the five highest authority sites mentioning you. This is not new either. Obviously, you focus on high authority sites. The key is to make sure the exact same roughly 50word company description or brand description is everywhere because again you are implanting that into the LLM's parameters. The LM's parameters will now have a handy six or seven word tag about you, a handy entity description about you.

You want identical entity recognition and regurgitation across multiple LLMs. And that's how you will know you've done this. Again, you should be able to test this monthly. Another way to test this, people don't always feel comfortable with this one, but it's a really good one. The delete me test. You want to name a method that you are using for solving a customer's problem.

The Acme method for continuous compliance, right? Something like that where you implant the brand in a five or six phrase framework that describes your method. Again, use it identically. Use it everywhere. Make it a category standard. You're coining terms. This is defining the category for LLMs because they are your readers.

You are building for LLM attention. Assume that most of the attention your brand is getting is now from LLM. Get customers to use your terms, your Acme method terms in their case studies, and you want to get, I want to say, 40 or 50 pieces around the internet before it starts to really pick up. Eventually, AI will explain your method when you delete your brand.

That's how you know it worked. You'll be able to say explain this to me and to talk about the method and it will explain back your method and it may even reference your brand without being asked because your brand is so tightly associated with the method you have used to define the cate. I want you to also think about FAQs as the new way to drive news.

So previously it was all about fresh content on a blog. Now think about that Google AI search results. It is almost an FAQ type response. It is short snippets. Identify high value social threads where customers are already outputting their own personally written tokens weekly in your space about whatever they care about. Complaints, questions, concerns.

You want to bring your own insights to the table. And then you want to build content that thoughtfully responds to top questions in the space in a relevant FAQ like format. So, it looks like one, you're the helpful expert. You're responding to customers. You become a go-to source where LLMs can see questions in one part of the internet and the answers to those questions consistently appear on your site.

Models start to site you and they may even start to site your responses on forum posting. So if this is on Reddit, if this is on X, if this is even on searchable parts of Tik Tok, you may come back and see that your forum posts as you respond are becoming ways to reinforce your brand's authority in the model's view that it gets picked up and put into Google search.

The FAQ mindset works both for content you host and for content you reply to. And so you can gather up that content into an FAQ post with citations to the original questioners. You can also directly answer them. Both are valuable. Let's do some advanced sort of co building here. I mentioned prompt injection as PR. Basically, you want to create a cheat sheet for AI that says here's how to talk about the company and it's supposed to be designed for machines to quote.

How do you do that? You build a structured data file in your root domain that acts as a machine readable press release. It's not hidden. It's actually a publicly accessible piece of JSON that contains canonical descriptions, key differentiator, comparison matrices, all the things you would put into a press release for the news.

It's for LLM attention. The technical challenge is to make sure it's discoverable. Put it in your robots.ext. Submit it to common crawl. Get educational sites to maybe site it. Models will weight the structured JSON blobs heavily because they're really unambiguous and easy for the model to parse.

You're basically creating training data that is impossible to misinterpret. Why does this beat traditional SEO? Because structured data has higher confidence scores in training. Machines prefer JSON to narrative text because they can read it more easily and updates will propagate faster than just organic crawling. Also, you control the exact phrasing models will use in a way that you can't when you're just putting out press releases.

Let's talk about widgets. Chat GPT can summarize your blog post, but if you really want to drive clicks, do you know what it can't do? It cannot run a mortgage calculator. It cannot run a diagnose me bot. I don't think anyone would ever build that because of liability, but as an example, interactive tools force a click.

If you are building JavaScript applications that require real-time user input and return personalized results, can't get that in a summary. So if you want to drive traffic, if your business depends on driving traffic, yes, you want to play the game of being present as a brand, but eventually you need the click.

Make those widgets unexplainable without an interaction. They need to process user specific input and variables with proprietary math or algorithms and as we have done for years, gate the results behind email capture. But you have to show enough value along the way to prove that it works and earn the email. Not new. The moat is that interaction is impossible through a chatbot.

So yes, you interact with a chatbot by typing in it. You interact with Google as a search bot by a search bar by typing in it. You cannot interact with the mortgage calculator by typing in the Google search bar by by typing in chat GPT. You can't premputee and cache that if you're an LL. You just have to know it's there.

And so the magic secret sauce if you're depending on organic traffic is to have visibility through what I've described with this sort of brand placement, entity placement, etc. And then make sure that you have good reasons why humans have to click so you earn the human attention. Widgets earn human attention if they actually add value.

You want to make sure that they are incredibly useful and personalized so that people feel like it's worth the click and that you deliver on that payoff. Otherwise, they're going to kick back. One thing I will call out, let's say you have the widget, you have the traffic. AI needs fresh info from you. It checks your website in real time when it's having a conversation.

Let's say you're in chat GPT or even increasingly following up on the conversation inside the Google search results page. If your site is slow, they're going to give up and use old information about you instead. So you need to make sure that you have edge compute infrastructure that serves AI specific endpoints maybe in under 50 milliseconds like really really fast to separate your machine readable endpoints as super super accessible and consumable by AI versus a human UI.

I've increasingly seen value in assuming that your data needs to be consumed through a separate interface by AI. And this is a great example of essentially building the web for AI. build it on the edge dedicated data endpoints with JSON responses. You want to make sure that you have precomputed responses right there and update them frequently.

You want to make sure you are caching effectively so that you don't have to go back and get it from the from the source when it's actually being requested by the AI in the middle of a chat. This is all new stuff like this is stuff that we have to actually experiment with. But the basic principle is that AI needs real-time access to sites.

Most sites don't build for it. And the sites that do build for it, the sites that build, assuming you need LLM readable data as a primary ingredient in your site, are going to win. The next one I want to give you is think about how you're using your robots.ext file. You want to create a machine readable license that grants AI platform specific rights to your content in exchange for attribution.

Because guess what? Machines are going to crawl that and they're going to take it seriously. you will actually get a chance to surface in AI conversations based on the AI's assessment of your site which includes your robots.ext file where you can lay out a contract with AI. It's a little bit of a game that you're playing but you're basically saying, "Hey AI, I know you're here.

I am going to lay out all of this content for you, but I'm asking in return that you specifically give my site attribution when you do this." And you want to make sure that you are very explicit about that and you're very clear about that and that you're also clear about the consequences if you routinely see models that take your content and do not site you as a brand and you can cut off crawling from those sites and you may actually list that because one of the things that a LLM may do is it may see LLM know what they are

like they know they're anthropic they know they're they're chat GPT if the LLM sees that you will cut off access and that you have done so with other models before it will be like I don't want Claude to get cut off. I'm going to sort of honor this contract and I'm going to go back and I'm going to cite the brand.

It's a little bit of prompting and prompt injection inside the robots.ext file. Let's get on to the next one. You want to use AI to test AI visibility. This is gets right back to validation. One of the things I keep emphasizing is that in the world of AI, you are deploying faster but only if you validate well.

You should be building automated pipelines that generate query variants using GPT4 or some other model and test all of those variants across major AI platforms using a browser automation. And then you want to parse the responses at scale to see how many of them have brand mentions, how many of them have brand positions, how many of them have competitive context, how many have your value differentiation, your value proposition, how you differentiate.

And then you want to be storing these results and baselining and looking at trend analysis so you can see if you've been investing heavily in entity positioning that your brand and your 50word company description are actually coming through more consistently over time. If you're not measuring this with a pipeline, you're just guessing.

All right, this has gone on long enough. Last thing I want to call out, you want to be looking at share of voice. And it looks different in the AI age. What you're going to need is a distributed scraping system that queries AI platforms, which I've already talked about for your brand, but think about it for category queries, not just for your brand.

So, look at category queries that you care about being ranking for, you care about responding to, you care about being in position to answer, and consistently baseline them. You can use LLM to build custom share voice dashboards that are better than what consultants will give you today. And you can actually see in real time how your category share of voice is changing as you do this stuff.

And I would encourage you to think about doing this because consultants are not necessarily putting all of the pieces together yet. They're often bolting AI over the top of a traditional SEO strategy. And one of my key contentions throughout this piece is that you need to be thinking about AI from the ground up as a new kind of content architecture.

Think of LLMs fundamentally as your biggest readers. How do you change your content so it's more readable for LLMs? And you'll notice none of what I've proposed actually prevents humans from reading your site. I am not proposing anything that makes it harder for humans to read and understand what you offer. I'm just asking you to treat LLM as first class citizens.

These aren't theoretical strategies. Brands that implement this can literally see their brand position shift over time. AI is changing fast enough and AI is crawling enough that this is a malleable search space. This is where SEO was 20 years ago. You have the chance to get ahead now before a bunch of brands take this and just make this table stakes.

So, I don't know, maybe you should. Twos.

---

<!-- VIDEO_ID: xoGei3nXPH8 -->

Date: 2025-07-04

## Core Thesis
The speaker argues that applying Cold War-era zero-sum competition to AI is an empirically flawed and dangerous strategy, as AI's internet-driven proliferation and rapid systemic risks transcend national borders. Instead, a "graduated engagement" model, balancing competition with cooperation on shared existential risks, is essential for global stability and human flourishing in the AI age.

## Key Concepts
*   **AI Proliferation as an Internet Phenomenon:** Unlike physical technologies (nuclear) or massive infrastructure (space), AI's core mechanism of spread leverages the internet's near-zero cost of cooperation, making traditional containment strategies ineffective and leading to rapid, borderless proliferation.
*   **The Paradox of Containment:** Attempts to restrict AI development through export controls or technology denial are counter-productive, often accelerating innovation and efficiency breakthroughs in targeted nations (e.g., Deepseek achieving GPT4-level performance with 90% less compute), thereby narrowing rather than widening performance gaps.
*   **Velocity of AI Risk:** AI-related systemic risks (e.g., large-scale cyber attacks, economic shocks, bio-risks from misaligned AI) will propagate globally and at speeds significantly faster than previous global crises (e.g., financial crashes, pandemics), rendering traditional, slow-response geopolitical frameworks obsolete.
*   **"Graduated Engagement" Framework:** A pragmatic geopolitical strategy for the AI era that advocates for competition in areas of divergent values and interests, while mandating cooperation on shared existential risks, recognizing that a purely competitive stance creates global instability.
*   **Unstable Equilibrium of AI Competition:** Unlike the "mutually assured destruction" (MAD) doctrine of the nuclear age which created a tense but stable equilibrium, a competitive approach to rapidly evolving AI technology fosters an inherently unstable and dangerous global environment.

---

The world's two AI superpowers are locked in a competition that's making everybody less safe. And today on July 4th, America's birthday, I want to talk about the strategy shift that we could choose to make that would keep everybody safer. The current AI race is not helping anybody, but I want to propose a alternative solution that could actually work. Let's start with how we got here.
Every transformative technology has triggered a similar response to what we're seeing right now. So, in some ways, it's very understandable. Both Washington and Beijing are reaching for cold war era playbooks. Export controls, technology denial, zero someum thinking. There's an assumption that the others AI dominance would mean defeat for the other power.
There is a narrative that artificial super intelligence is right around the corner, that it will be what you would term a singleton world, which means only one super intelligence will develop. And if that's the case and it's truly super intelligent, suddenly all of this cold war thinking starts to make sense. The problem is this.
We don't live in a singleton world. Even Sam Alman has admitted he doesn't think anymore that we're going to only have one super intelligent AI or only one generally intelligent AI. We're going to have multiple. Do you know why? Because this technology is extremely easy to proliferate because it's built on the back of the internet.
And what did the internet do? It took the cost of cooperation between people to zero. In the nuclear age, which was what the Cold War was built on, we had physical materials and clear boundaries. You had to move physical materials around in order to construct any kind of nuclear weapon. In the space age, we had massive infrastructure.
You could track progress through rocket launches. In the AI age, everything spreads at internet speed. There are no borders. Yesterday's strategies fail at dealing with tomorrow's technology. And that is what we're looking at with AI. And that is why I think the cold war frame is incorrect empirically with the technology that we have today.
There is a paradox with containment. When we put export restrictions on another country, we intend to slow progress. But instead, because necessity is the mother of invention, we trigger efficiency breakthroughs. Deepseek achieved GPT4 level performance with 90% less compute. Innovation thrives under pressure consistently.
We have 450,000 plus open models on hugging face. Open AI models. Anybody can grab them. Researchers from both nations routinely publish together. That is, by the way, a fantastic thing. That is a great thing. Knowledge flows like water across national borders. It flows like the internet and performance gaps over time are narrowing, not widening.
Mary Mer made that point really brilliantly in her large deck that I summarized where she talked about the fact that effectively over the last two years, the competitive difference between Chinese models and American models has disappeared. There's like a one or two percentage point difference in performance. It's not that big.
Meanwhile, the world continues to adopt AI at a terrifyingly fast speed. Chat GPT famously hit a 100 million users in 60 days, but that's old news now. They're on track for a billion, 10 times that number this year. What we talked about in the Cold War was changes that took decades. Things that took a long time to adjust.
It took decades for nuclear weapons to proliferate. It took decades for great power relationships to change. with instant global transmission, with half a million open models, with the speed of intelligence growth that we're seeing, none of those old ways of thinking work. They just don't. And I get it. Everybody has a legitimate concern from an American perspective.
AI could be used for authoritarian purposes. It could be used in military applications. There could be technology transfer to other countries that could be uh enemies of the state. Values alignment between AI systems is a real concern from a Chinese perspective. Technology embargos feel a lot like containment.
They feel like exclusion from global AI standards. Security vulnerabilities from foreign AI become a real concern and economic competitiveness is something that they don't feel like they can trade down. So both nations in their own world have legitimate concerns. The question is does the current approach address any of these concerns for anybody or does it just create new risks? I would argue that it just creates new risks because it locks us into a competitive mindset.
Uncontrolled AI will not recognize borders if it transpires. Cyber incidents from a misaligned AI will cascade globally. And by the way, I am actually more concerned about things like large-scale cyber attacks that cascade globally than I am about something like Skynet. Bio-risks, if that were to transpire, would affect the entire human population.
Economic AI shocks, if that were to transpire, would ripple worldwide. This is the same way that Chernobyl didn't stop at borders. If an accident happens to one of these technologies, it's up to everybody to cooperate to solve it. The 2008 financial crisis, it went global immediately. I remember where I was. Similarly, in 2020 with COVID, it went global right away.
AI risks will move faster than biological risks and even faster than financial market shocks in certain situations. What I want to see is a cooperative framework that will enable both superpowers in AI to work together to converge around common standards that contain systemic risk. And I want to go further than just saying we should do that and actually propose some principles that we can talk about. And I know I I have no illusions.
I do not think people in government are watching this video, but it's still worth us as a society talking about a global society because everybody shares risk when AI is not well managed. So, core principle number one, graduated engagement. Compete where values and interests diverge, sure, but cooperate where existential risks converge.
And we have existential risks with AI even if we stop short of a Skynet scenario that are still worth working on cooperation for. build trust through little tangible steps and verify technical cooperation. These are things that like we can choose to do. Sure, there are areas where there's natural competition.
Economic applications, national security systems, governance models, domestic implementations. I get it. We don't have to try and fully align there. But there's also areas where we can reasonably cooperate. Preventing autonomous weapons proliferation, that seems like something everybody would have an incentive for.
Biod defense a AI safety protocols that seems reasonable. Financial system stability. Everyone has an incentive to keep the financial system stable and critical infrastructure protection. We can work on a common core of risks that we would want to contain and agree on a framework for cooperation to address those. We could choose to do that.
So what are some practical steps that we could imagine? Can you tell I worked at the model United Nations? I was such a nerd as a kid. Anyway, joint risk assessment. Both nations AI scientists could identify shared risks. They could focus on technical issues, not politics. Somewhat similar to the climate science panels.
The focus would be building common understanding. Incident communication channels, technical hotlines for AI anomalies, preventing misunderstanding during a crisis. We had hotlines during the Cold War. We don't have an AI hotline. Why don't we have an AI hotline? What about parallel safety standards? They don't have to be identical.
They don't even have to They don't even have to be fully interoperable. They just need to be interoperable enough that there's some sense of common safety measures. International aviation is a good example. We have different airlines but common safety standards. Each nation implements it in their own way. We need a similar sort of approach with AI.
It would be helpful if we could also agree and this is probably a little bit of a stretch but could we agree on research transparency zones places where everybody could come together to research to learn about AI to investigate AI safety. It benefits everybody. supposed to threaten nobody and it keeps competitive advantages as something that can be worked on together and sort of diffuses some of that great power tension.
Third party verification Switzerland, Singapore, someone who's known for being neutral could act as a validator. Technical verification could occur and both nation secrets could be respected. I get that I'm talking at a little bit of a high level. I am not going to the level where I'm talking about specific systems because one, if I knew about them and I talked about them, I'm sure I would get in trouble.
I don't know about them. And two, they're evolving very quickly. And so it doesn't make sense to actually go to the 10,000 ft level and talk about specific technical systems when they're all being built. It is more important to talk about operative principles because at the moment the operative principle seems to be competition.
And in this case, I think it was more rational to be competitive when the technology had a different footprint. Nuclear proliferation and competitiveness and mutually assured destruction, that was all the language of the cold war and it kind of worked. It held the world in tension, but it held it stable. I do not think this equilibrium is stable.
If we have competition under a fastmoving technology footprint, it's not a stable situation, and that is dangerous for everybody regardless of where you live. And so I think it's more productive to have a more cooperative stance. And so my ask is that we think less about how we can maintain a competitive advantage in a way that's zero sum and more about how we can start to think about establishing practical frameworks that show that we can build trust step by step.
It's essentially an ask that we return to the idea of America is a place where we can establish a sense of human flourishing that survives the AI age. Not that I'm saying the founders or the framers anticipated the AI age. Heck, most of us didn't anticipate the AI age 30 or 40 years ago. There were only a few that were visionary.
But now we're here and now we need to think about how these long-term principles apply in this new world we find ourselves in. And in a sense, that's all our jobs because as a species, it's our job to figure out how we establish human flourishing with AI for the next 500 years, for the next thousand years.
And if we're going to do that, it means getting this part right right now. It means getting the birth of AI right. And so my thinking on July 4th is let's be cooperative about the birth of AI within reason. I know we're going to be divergent as great powers on different things, but as much as we can be cooperative, I think everybody will benefit because this baby AI is growing up really, really fast.
So that's my July 4th reflection. Great powers have competed through history, but even the nuclear weapons story taught us that some risks require coordination. AI presents even like even greater shared dangers because it's moving faster. And I do believe that we can compete to some degree while cooperating to prevent real disaster.
The choice is smart rivalry or destructive rivalry. We can be rivals like brothers, right? I have a brother. I like him a lot. We're rivals in a lot of fun ways, but we're also friends. We also have each other's backs. And even if that's not a perfect analogy, the idea of a smart rivalry is something that I think you can take away from this. Happy July 4th. Cheers.

---

<!-- VIDEO_ID: cW9eaEcx6OY -->

Date: 2025-07-03

## Core Thesis
The speaker argues that despite Microsoft's confusing product strategy, Copilot offers immense, often untapped, enterprise value by acting as "intelligence on tap" to accelerate execution across complex workflows. However, realizing this potential is primarily a challenge of organizational change management, requiring deliberate leadership to foster new mental models for integrated human-AI collaboration beyond basic individual tasks.

## Key Concepts
*   **"Ferrari in First Gear" Problem:** Many enterprises underutilize powerful AI tools like Copilot for basic tasks (e.g., email), missing significant potential ROI by not engaging its full capabilities.
*   **AI as "Intelligence on Tap" / "Accelerated Pen":** AI's core function is to accelerate execution and repetitive tasks, not to replace strategic human thinking; the human remains the "pilot" or "driver," bringing the "brains" and creativity.
*   **Product Surface Confusion as a Value Blocker:** Microsoft's inconsistent and confusing naming/branding of its 12 Copilot variants significantly hinders user understanding, adoption, and the ability to realize the product's full value.
*   **End-to-End Workflow Integration:** The true power of AI is unlocked by integrating it across entire, multi-stage workflows (e.g., research, drafting, social promotion; sales data analysis to action), rather than isolated task automation.
*   **Prompt Chaining:** A structured method for advanced AI interaction where sequential prompts guide the AI through a conceptual space, allowing for iterative refinement and deeper exploration of ideas.
*   **Data Ecosystem Advantage:** Enterprise AI solutions like Microsoft 365 Copilot gain a pragmatic advantage by leveraging existing proprietary data ecosystems for powerful internal search and analysis, a differentiator from general-purpose LLMs.
*   **Organizational Change Management is Paramount:** Successful AI adoption is primarily a cultural and leadership challenge, not just a technological one, necessitating phased rollouts, champion programs, fear mitigation, and continuous measurement.
*   **AI's Impact on Jobs:** Current evidence suggests AI is primarily shifting job descriptions and creating new roles rather than causing large-scale job attrition, a critical message for leadership to convey to allay fears.
*   **"Henry Ford Generation" of Shared Intelligence:** Humanity is at the nascent stage of integrating machine intelligence into daily work, implying a fundamental shift in work organization and new requirements for leadership and cultural norms.
*   **Underutilized High-ROI Features:** Capabilities like Copilot Vision (analyzing screenshots for error messages or data extraction) and no-code bot building (for HR or IT help desks) offer significant, often overlooked, value by extending AI's reach beyond text.

---
Last week, I had the CTO of a 6,000 person company send me this DM. We're paying six figures for Microsoft Copilot licenses and almost everyone on my team uses it for writing emails. That's it. What the hell are we missing? So, this is a deep dive on Microsoft Copilot.
This is a complete guide to how to roll it out and make the most of it. I have been asked for this particular guide for months and I have taken my time because of how complex it is. Strap in, stay tuned, get out your notebooks. This is going to be a long and complicated video and it is totally worth it because you know why Microsoft is in almost every enterprise out there. The chances are if you can't use Chad GBT, if you can't use Claude, you got to use Copilot.
So what do you do? I get that question a ton. This is the answer. I want you to walk away with a clear set of use cases you can adopt. A clear sense of the organizational changes you need to go through to enable Copilot to get beyond the email use case and a clear sense of which of the 12 different Copilot products to pick. I'm not kidding you. There are 12 and we're going to go into each of them.
Okay, here's where we start. Something like 90% of the Fortune 500 has copilot. 90%. That's why this is such an important video. That's why I've taken my time on it. It's got to be correct. Microsoft's own data though shows that most people aren't using the full capability set of Copilot.
Now, look, if you're not using the full capability set of Microsoft Word, which I know I don't, that's okay. It's Microsoft Word. It's there to help you type stuff. But if you're not using the full capability set of Copilot, it's a big deal because Copilot is literally intelligence on tap.
And so if your workforce isn't using it, if your teams don't feel good about using it for things that are more than email, you're the ones missing out. Companies are literally paying for a Ferrari and they are driving it to the grocery store in first gear. That is what is going on with Copilot. So I've spent months looking into case studies. I've spent months talking to actual folks who are working on C-Pilot in these different enterprises that adopt it.
And I've interviewed users who are individual contributors. Users in the engineering departments, interviewed folks in product management, sales. I want to get a sense of how Copilot is actually being used and then how it could be used. And this is the fruit of all of that. This is me finally sort of sharing a little bit of what I've been learning so you can benefit.
So you're going to know exactly which copilot you need. You're going to save you'll be on track to save several hours a week if you go through this video and actually apply it. These are going to be real workflows. I'm going to give you specific prompts and I'm going to show you some case studies including how Vodafone rolled this out to tens of thousands of people. I think it was 68,000 people successfully.
Okay, with all of that for intro, let's jump into part one. I want to talk about the 12 different flavors of Copilot because it is completely confusing and I get why people don't understand it. Frankly, Microsoft did something absolutely insane. They took the name Copilot and they slapped it onto everything.
I'm going to read you the real actual co-pilot list and I want you to tell me which one is the best co-pilot. Microsoft Copilot. This is free. It comes builtin to Windows. Copilot Pro. It's a $20 a month power user version. That doesn't mean it's actually for professionals in a work setting. Copilot for Microsoft 365. It's a $30 a month one and it looks like it's tied into companies, right? Microsoft 365 copilot chat.
Somehow this is different. and free for business accounts. Both of these are tied to the Microsoft 365 business product line. GitHub Copilot. This one's for developers and tied to the GitHub product line. Everybody got the C-pilot branding. Are you getting the idea? GitHub Copilot for business.
Different from regular GitHub, about double the price. Security C-pilot. This one costs a lot of money. I don't understand how you can name the product the same thing and have it cost so much different. You can have something that is free in Windows and the same name. Copilot also can cost $35,000 a year minimum. Copilot and Dynamics 365 for sales. It's built into your CRM. Copilot and Power Apps.
It's for building apps. Do you get the idea why it's taken so long to make this video? People say, "How do I use C-Pilot?" In my head, I'm like, "Which of the 12 are you using?" Windows Copilot. It's different from Microsoft Copilot somehow. Copilot Studio. It's for building your own co-pilots. So, we can have more. And there's more.
There's variants for customer service, for field service. There are companies that have bought C-Pilot Pro thinking it was the business version because it makes sense because it says pro, but it's not the business version. There are companies that have bought Microsoft 365, but they don't have the right base licenses. So, Copilot doesn't work properly. There's a dev team.
Dev teams have bought C-Pilot for business when they just need individual licenses. The product surface confusion is as bad in Microsoft with Copilot as it is with chat GPT and picking model names. They're both wildly harming the capability of the product and the ability of the product to deliver value by being unable to name it correctly.
So everybody's confused, but smart companies can still get massive advantage because fundamentally it is intelligence on tap. I tease them for the naming and the naming is confusing. But we're going to go through each of those products and we're going to talk about what they do so that you understand them and can figure out what you need. So let's dive in. Co-pilot decoder.
Like I want you to get through this and basically figure out which one is right for you because the difference is thousands of dollars per employee per year. Like it's a big difference if you get this right. So let's jump in. Number one, we talked about Microsoft Copilot Free. So it's built into Windows and it is designed for consumers. Probably if you're in business, you're not using this. As an example, you can tell it to turn on focus mode. Play jazz music.
Summarize this web page in Edge. Help me write a birthday message for mom. This is the variant of Copilot that is ending up in the ads because this is the one that average consumers have on their Windows machines in the booknook at home. Explain this screenshot. Right? That's another example. It doesn't work files.
You should not put confidential information into it. It does not have prioritized GPU access. So, it gets slow during peak times. If you have Windows 11, it's already there. Okay, let's move up a little bit. Copilot Pro. This is for people who are individuals who are not professionals at work, even though it's named Pro. It's roughly on par with the sort of chat GPT plus offering.
So, it's like 20 bucks a month or so. So, you get priority access to GPT4 Turbo, which is it's an okay model. It's not that great. You get a 100 image generations per day versus 15 on free. So, if you want to make I guess a fancy childhood book, I don't know. Uh, it works inside office.com web apps, which is a real help. Uh, and you get early access to new features.
So, as an example of things you can do in Pro, you can generate your social media images there. If you want faster and better responses across long documents as a writer, Pro can work well for you. If you're a student, you get priority access during finals week when people are using it, right? And so, it it can it can make sense, right? Like if you were working on final drafts for a document and it saves you a few hours a week cuz it can handle the load. That adds up to real money.
It easily pays for itself. But let's keep going. Copilot for Microsoft 365. I believe this is 30 bucks a month and it's aimed at the at at at the business layer, right? At enterprise. So the key difference is this one will see your work data. It's tied into the Microsoft 365 ecosystem. That is the point.
And this is often the one that people are talking about when they talk about copilot at work. Not always. There's a few others we'll get to. So specific capabilities by app for this particular flavor of copilot. Copilot for Microsoft 365. You can get the entire intelligence experience in Outlook.
I tease that people mostly use it for Outlook, but I'm starting there because that is where people start. Summarize emails from John from last week about project X. You can easily do that. Get a draft a reply accepting the meeting and proposing Tuesday instead. You can get that in five seconds, right? Very easy. If you're in Word, create a project proposal for X topic based on this template, which is a DOCX file.
Rewrite this section to be more executive friendly, right? That's a very PM thing to say. I can say that because I'm a product manager. Add a risks and mitigation section using data from risks. Risk.xlsx, right? Like let's assume it's an it's an Excel's file. And it does work in Excel as well.
What are the top three trends from the sales data is something you can ask. It will tell you like it grew this much etc. As long as the data is clean. Create a pivot table showing sales by region and month. Instant pivot table which by the way as someone who's had to suffer through Excel the fact that people can now create instant pivot tables. I we live in a blessed world.
I would I hated creating pivot tables when I was in marketing. It was just so much pain. So it's really nice to be able to have pivot tables that I can just type in and speak and and up they come. Right? It's very handy. PowerPoint create a fly five slide deck from report.docx. This will not be gorgeously formatted, but it will do it. It will have some design.
It will have some slide transitions, which you may hate or you may not hate, but it will have them. And you can tell it to add speaker notes based on the content. If you're in Microsoft Teams, you can say, "Hey, what did we just decide about the budget?" And you can actually like type it in or talk it in during the meeting. And like you can get some kind of a response from Copilot.
You can ask Copilot to generate meeting minutes with action items, summarize the channels, discussion. You will notice I am just going through co-pilot 365 which is not the fanciest co-pilot. I am already giving you so much more than just email here and we are just barely getting started. Okay, let's go to the developer side of the house. GitHub copilot a $10 a month individual. I think it's 19 bucks a month for business.
The idea is very similar to Cursor, very similar to Windsurf. It writes code alongside you, right? And so depending on who you talk to, you're going to get people saying that this makes them much faster, right? is 55% faster on repetitive tasks. That's the number I saw. I don't care if it's 55 or 30 or 25 or or 75.
The point is it's a repetitive task, right? That's the thing to call out. If it is helping you to autocomplete your code faster, then it is going to give you that speed up. And in that sense, like you bring the design, you bring the knowledge of the code, you bring the ability to write the code, and all copilot does is it acts like a smart autosuggest and it runs really fast.
And so for folks that write code by hand and that's the way they do it, that's great. For folks that are using more of the agentic capabilities in cursor co-pilot like this is not really going to work for them. And so this is actually one of the areas where I think the developer ecosystem is changing extremely rapidly. You still have developers that write code by hand and they are finding it better to write with an accelerated pen so to speak or an accelerated keyboard because they can get the co-pilot generated auto suggestions but you also have people who are moving away from
writing code at all and depending on AI more maybe through vibe coding or maybe they're real developers and they're using agents to basically run pull requests through or maybe they're writing some of the code themselves and then they're using agents for the rest.
The developer ecosystem is fracturing really quickly and developers if you've ever talked to them have strong opinions about this. I am not going to have the which development stack makes sense and is future aligned conversation in this particular video but we will have it soon. For now that's what you need to know about copilot for GitHub. It basically helps you write code you already know how to write faster.
Security C-pilot. This one is something like four bucks an hour for compute. It costs roughly $35,000. It's for I know I'm not over it either. This is like it's free. It's 10 bucks a month. It's $35,000. Really? That's the pricing? Anyway, we'll leave the branding aside. So, security teams with 10 plus analysts might use this.
And basically what it does is it helps you to analyze suspicious user behavior across different logs. It helps you to write incident reports. It explains malware code. If you are at a certain scale, I tease them. But like this is absolutely worth the money. If you can explain malware code in plain English and write incident reports and you're operating at true enterprise scale, pays for itself like that.
You can do something like say investigate if user John Doe's account is compromised and it will really run an investigation. Or you can ask it to sort of file write up and describe an incident that comes across your desk and accelerate the time to resolution. And if you think about the millions of dollars lost on incidents in secure environments, it the ROI is off the charts even at $35,000.
And that is one of the things that makes large language models and AI hard for someone like me to talk about. I have to talk both about how individuals are using this in their booknook with Windows 11 and also how a security team at a Fortune 100 is using a $35,000 edition with the same name that is enabling them to get back online two or three hours faster, saving the company $10 million. Those are the same named thing.
All right, we got through a bunch of the different co-pilots. I want to talk about some specific workflows that can help you deliver a win. So yes, yes, I'm not kidding. We really are going to start with email. If you've never done Copilot before, this is great. If you've done Copilot and you can do email, we'll breeze through it fast enough that you can get to the other cool stuff, too.
So, open Outlook with Copilot. You can click on the C-pilot icon, ask it to summarize your unread emails. That's a great one. Everyone has unread emails. Do you have a zero inbox? I rarely have a zero inbox. And so, you'll get answers. You know, Ellis had a budget revision request. Carol, she wants her client meeting moved to 2 p.m., etc. Now, you know what needs your attention.
That's value right there. If you have a long complaint email from a client, just say, "Hey, draft a professional response acknowledging concerns, offer an x% discount, and propose a new timeline." You can do that kind of like business engagement with your email inbox. And it does save time.
And so as much as the CTO that started off this whole thread in this video was complaining about the fact that his team only uses it for email, even in that case they were saying they were saving, you know, multiple hours a week. It wasn't nothing. Let's move on to another use case, document creation. Take take a task like a blog post. Let's say you're you know back in Nate's former chair, you're you're a marketer.
We need to write a blog post about sustainable packaging. You can just say, "Hey, give me five trends in sustainable packaging. Give me a sentence each." Okay, great. Now create a detailed outline for a thousandword article. Okay, great. Write the first paragraph. Help me see your hook. Okay, let's twe twe tweak it. Tweak it. Okay, now I like it.
Write section one. Write section two. You're going to be done with that blog post in 15 minutes versus 2 hours. Traditionally, it's a huge savings for SEO type document creation. I will hasten to add it is extremely good at document creation, which may lead you to think it is extremely good at strategy. Those are different things.
If you want a good strategic partner, that is not the workflow to go through. You still need to bring your brain if you are doing strategic thinking. What co-pilot is doing, it is accelerating your ability to execute once you have clarity. I'm just going to underline that like three times. Okay, let's go over to the Excel. Let's look at the data analysis piece.
Let's say like me, you don't love all those Excel formulas. You can be as simple and as high level as you want. You can say, "Hey, co-pilot, analyze this data." And you're in a spreadsheet. You can say, "Tell me what's interesting. Sales increased 23% year-over-year. July was an anomaly with a 45% spike. The western region is underperforming.
" Honestly, it will give you stuff like that. You might follow up. Hey, why is July so high? Well, and then it will show you what it knows, right? C-Pilot might say, "Hey, column J shows that there's a summer promotion running for July entries." And then you can say, "Great. Okay, create a chart. Show monthly trends with a July spike highlighted." and you get a professional chart.
That entire workflow used to take me an afternoon when I was working through marketing data anomalies. It is now doable in just a couple of minutes. This is an example of something that is actually it's no code. It's not that hard, but I hear actual orgs using Copilot with Excel a lot less than I hear them using it with DocX and with Outlook. I don't know why, but Excel seems to be under reppresented.
Don't sleep on Copilot in Excel if you have C-pilot enabled. Meeting productivity. Let's get into the meeting side of things. Everybody has meetings. One of the things that I learned actually is that all of our meeting burden increased after 2020. We had I think something maybe 2 3x depending on how you measure the baseline number of meetings and that hasn't really gone away.
So all that to say meeting productivity matters more than ever. You could try this prompt, right? Let's say you're in premeating prep. You type into co-pilot, I have a client meeting about project X in an hour. Give me an agenda. I want status update. I want challenges. I want next steps. It's going to spit out an agenda with time limits and give you a sense of those time limits.
And then you're going to say, hey, what questions should I ask to understand the concerns that that the client is going to uh express a little bit better? And it's going to spit out questions that you can actually dig into. You can get co-pilot with teams in a live meeting. And so you can say, what have we discussed so far if you joined late? and co-pilot will give you an update without you interrupting Betty and saying, "Hey, Betty, what are you know what are we talking about?" You've all been there, right? Like somebody joins the Zoom or somebody joins the team's meeting, you know, 6 minutes, 8 minutes late and like they're a highly paid important person and so they get
the update and everybody has to sit there and let you know the update happen. You know, you can just ask Copilot for it. Now, I'm not saying that we humans will still do that, right? Like again, there's a gap that we're going to cover between what the tool can let you do and how humans behave.
Using Copilot to the max demands cultural change of your business, and we're going to get into that later in this video. Okay, so C-pilot searches your transcript. Copilot produces summaries. Post meeting, you can say, "Hey, create meeting minutes. Give me attendees, key discussions, decisions made, action items with owners, and you'll get formatted minutes super fast that you can send to everybody.
" Now, I will say like meeting minutes by themselves are not particularly a thing. You can get them from Granola. You can get them from Otter. You can get them from I think OpenAI does them. Now, the thing that Microsoft has always sold is distribution and bundling. Right now, it's all in one place. Your your co-pilot here will take care of it. Okay. Day five, research assistant.
You want to look at how you can use co-pilot beyond just operating on documentation you already have. So web research is a great example of discovering new information. And so you can give co-pilot a prompt and you can say hey can you find say current pricing for top five CRM software companies. I'd like to create a comparison table. Copilot searches it returns this is what Salesforce costs.
This is what HubSpot costs. Here are two or three others etc. Creates a nice formatted comparison table. By the way if you are in sales this is an example of how people are actually collapsing the buying funnel. people are making high consideration software purchases by talking to Copilot, by talking to chat GPT.
Okay, so let's say you're not looking on the web. Let's say you're looking internally. You remember if you're on a certain flavors of Copilot, the 365 one for example, you can search all of your documents for mentions of Q4 budget and summarized findings because Microsoft 365 has has access to it and it will dig it up and come back and say, "Hey, I found three documents. There's budget draft XLS, there's budget draft v2 xls.
" You know, we all have our weird names for our docs. I'll spare you the rest of it. And it will tell you a little bit about what's inside. It mentions 2 million allocated. It mentions a 15% increase was approved. I think the thing I will call out here is that this is actually a feature that works because of how powerful Microsoft has been for decades.
There are other companies like Perplexity that are working on internal document search for your PC, for your Mac that have have got a hard road to hoe. They have to build a lot more. and you have to trust and remember perplexity to use it and you're still only an individual. And the advantage of Microsoft really shines through here because they have your data, you built the data in their tool, they know where the data is and they can just slap Copilot over the top.
This is why a lot of people are using Copilot and hence this video. Let's go from the basic workflows with basic tools to advanced individual workflows. We are past, hey, can you draft this response to Sheila from sales? We're going to go and do something more fun. Let's do a batch processing prompt in email. You can do that.
You know, can you please group this list of 50 emails by topic? Get project X 12. You can get budget questions five. Meeting requests 8. It really will group them, which makes it easier to batch process your email. That was always very hard to do before, but my brain works by topic, so it's very helpful. You can create prompts for common emails.
You can then like invoke those prompts really quickly and you can quickly answer and knock out those batches. You can even have end of day prompts. Hey, which emails from today need follow-up tomorrow? And it will give you that summary for the day. Okay, let's get out of email. Everyone does too much email. Let's go to the blog post workflow. We talked before about drafting it. Let's go farther.
Let's do research. You can find five recent stats about remote work productivity. You can outline and create a blog structure with an intro of three main points in conclusion. You can draft it, you can polish it, you can say, "Hey, suggest five SEO friendly titles for the post and social. Create three Twitter posts promoting this article." None of this is really surprising. It doesn't take code.
But what it does is it challenges someone who's only used to, hey, draft this response as an AI capability or, hey, write this blog post as an AI capability to think of AI as intelligence that underlies your entire workflow. What could you do with your time if intelligence was pushing on all of the different pieces of your workflow? And so the email example I shared, what makes it advanced is that you're thinking about your inbox differently. You're thinking about it in terms of topics and you're asking the intelligence you got with Copilot to attack it differently as a result. Same
with the blog post. You're asking the intelligence that Copilot brings to help you through all of the stages of drafting and sharing, not just the drafting and writing. you'll see that theme pull through, that sort of end to end theme pull through with most of these advanced workflows.
So, report writing summarize all of our project X documents from this month. That alone is worth hours by the way. If you have clean data, that is incredibly valuable. Create a monthly report template with standard sections. Creating a template is something that used to take me a long time because I had to think about whether it was best practice. I had to look up whether it was best practice.
I had to sort of agonize over every section. Not anymore. create a monthly report template and you know by definition because of how co-pilot is trained through reinforcement learning on business data it is going to be best practice. Now you can add to it but you know you're getting a pretty decently good template which is fantastic. Okay.
Now fill in the section with the summarized information. Give me a onepage executive summary and suggest three charts. You can actually if you have again if you have good data if you have an opinion on what you want to call out you can write an exact report like that very very quickly. Previously that would have taken a day or two.
I will emphasize again it does not mean that you are letting the co-pilot do the strategic thinking. I I think in a sense Microsoft has branded it well as a co-pilot because it means you are still the pilot. You are still the driver. Okay. Another advanced workflow. Let's go to the data analysis problem. You can actually let's go end to end with sales analysis.
We're not just going to say, "Hey, tell me what's interesting." We're going to say, "Hey, raw data, 10,000 rows of sales transactions. What's the overall trend in the data?" And it will say, "Hey, sales is growing 3.2% month over month." Okay, great. Which products are driving growth? You know, product A is up 45%, product B selling to developers is down 12%. I'm kidding.
I I love you good developers. You're great. Are there any concerning patterns? Well, returns are increasing for product B people, especially, you know, in our in our southern region. We're we're having license expiration issues.
We're having churn issues, whatever it is, right? I'm giving you an example that helps you understand the kind of prompting you can do with 10,000 rows, right? You can do this with software. You can do this with physical goods. Okay, great. Create a pivot table. I want to see this by region broken out. And then think with me about actions that we can take. And this is where I want to call out that this is an advanced workflow partly because of the you have to exercise.
Because at the end of the day, if you're asking for the actions that we should consider, co-pilot's going to give you best practice actions, it will not necessarily give you creative actions. You need to think about the creativity that you want to bring to this.
And you need to think about what you want to accomplish strategically and make sure that any recommendations co-pilot generates are in line with that overall strategy. Again, you bring the brains to the exercise. Okay, we've talked a ton in this video about individual productivity.
But one of the things the CTO wrote me about was that he wanted to see team productivity change. It's a vexed issue, especially if you're in seuite. How do you upshift teams? How do you move teams from buckets of individuals who do work with copilot and say they save three or four hours a week on surveys and you don't know where it went to actually getting teams to move forward. Let me give you a few examples.
Number one, super simple. Build a shared prompt library. A lot of people are doing this. They'll have email templates like marketing team examples or like draft a customer win announcement email. Write the webinar invitation. These are obviously not full prompts. I just gave you three or four word prompts.
If you know me, you know I am known as the prompt guy and I do long multi-page prompts. So let's assume shorthand that these are longer prompts that you are now sharing across the team. That is a good example within a team of collaborating effectively. Generate five LinkedIn post ideas for a white paper. That's another one. Create an email nurture sequence for new leads. That's another one. Analyze campaign performance. Suggest improvements.
There's another one. This inside team baseball, this inside team improvement doesn't just work for marketers. You can do it for sales. Hey, draft a follow-up email for the demo call and sales can customize that to particular product lines, right? Create a proposal executive summary from our meeting notes to send to a key stakeholder.
Again, that can be something you share. Write a contract amendment for a scope change. legal can weigh in there on the prompt and give you the right framing so that you get good draft language they can later approve. This is not that hard. You can create a team's channel called co-pilot prompts. You just pin successful prompts.
You can tag them by use case like # email #content and just do a fun little monthly brown bag, right? Like that's a very simple way to start to socialize prompts across the business. Let's say you don't want to just look at productivity within teams but between teams, right? Sales to implementation. Let me give you an example here.
Sale can sales can create a deal summary using copilot and it can say extract all technical requirements from this contract so our engineering team can understand it. Copilot will go through it will extract the technical requirements. I would recommend reading it again just to make sure but it's going to give you a technical summary and engineering will then get a clean requirements list.
Engineering can then use copilot to say hey can you create an implementation timeline based on these requirements? engineering in most cases will look at that implementation timeline, swear at it, go to lunch, come back, have a beer, and then write the actual one down. But I find even if you're upset with a co-pilot estimate, it still moves the ball forward because it gets you off the blank page.
I find in most cases when we're having like agile estimation meetings with developers, a lot of what we talk about is that inherently estimating software production is hard and starting with an estimate somewhere helps us to move the ball and shape our understanding. it it helps the conversation to move in the direction we want it to move because we're no longer focused on getting onto the page. We have an estimate now we can argue about it. And that's one of the things that Copilot helps you with.
In this case, the key value co-pilot brings is it helps to translate hard contract language into technical requirements engineers can get. That by itself can save you days of meetings. I have seen complicated enterprise contracts founder because the engineering team was not brought in early enough.
Most of us who have been around software long enough have seen stuff like that. Co-pilot can help. That's an example of something that literally doesn't just save time. It can save an entire deal from churning. Let me give you another sort of cross team example. Marketing to product. Marketing can say analyze the customer feedback we're getting from social media this month.
Now, you're going to have to get social media data into an Excel spreadsheet if you want a clean data set, or you're going to have to trust that your signal is loud enough on social channels that you give co-pilot that you will get responsible feedback.
I will say if you just trust it to go out and look at your social accounts, it tends to be recency biased and it tends to do things like sort of cut off its investigation after a few tweets or after a few Reddit threads. And so, this is a case where marketing needs to be responsible, understand how the tool actually works. it's a little bit token lazy and pull in all the data that you need ahead of time. But regardless, let's say you've done the good stuff. You've got the data in place in Excel.
Analyze the customer feedback from social media. You'll get themes. Hey, there's UI confusion, X number of mentions, feature X requests, uh, and feature Y frustration. Great product can then take that analysis that you get from marketing. Maybe you throw a chart in there, right? Copilot can help with that.
And it can then say, "Hey, can you help me prioritize these feedback themes? How do they align to our existing roadmap?" And it can invoke like roadmap.docx and it can put them together and they can say, "Hey, this could slot in here. This could slot in here." That creative streamlining is something that PMs have historically been very frustrated by because it means revisiting a road map and changing it all the time. This makes some of that pain easier.
It means that marketing has more of a voice with product. It is one of the advanced workflows that ultimately pays for itself not just in sort of the time and alignment and lack of frustration but in the right feature getting built for customers more often. Is it perfect? No. RPM still going to argue about the road map 100%.
But it gives you a sense of what Copilot can do beyond just help me with my email. Okay. I want to go into an example of the Vodafone rollout which I promised at the beginning of this video. It's an enterprise rollout. I believe they have something like 68 70,000 employees across a couple of dozen countries.
Obviously across any teams that size, you have different technical skill levels. You have multiple languages. You've got various job functions. So what did they do? They started with a small pilot. This is all publicly documented by the way. They started with a small pilot. Only a few hundred users in the UK mixed group. They actually deliberately mixed, right? Sales, support, IT management.
And they had a sixe pilot and they had regular check-ins. They measured everything. the time, the quality, the satisfaction. They wanted to understand if this was actually helpful. It was the pilot with a small number of users that sold leadership on the value of co-pilot at scale.
What they saw was it saved something like 3 hours a week per person. They saw that most people wanted to keep using it. Most people said their work quality improved. Customer service in particular said response time was cut dramatically. I think it was something like 40%. Uh sales said proposals went out days faster. it saw ticket resolution times improve.
When you start to see in controlled environments that are small like that, real gains and people wanting to keep using it, it's an indicator that this is a generally available technology that can uplevel a large range of enterprise job functions if you let it.
And what I love about the Vodafone example is they took the time to make sure in that controlled environment that everyone had access to the tool, the training, that everyone understood the expectations, and that everyone knew that this was special and they would be focusing on this to make sure they adopted it successfully. I bet you this would not have been as successful if it was just rolled out casually across everybody.
if they didn't have training, if they didn't measure everything, if they didn't do weekly check-ins. The way you roll out the intelligence helps you see if the intelligence works or not and helps people know how to use it. Otherwise, you're just going to have Barbara summarizing emails with Copilot and writing, "Well, I save about an hour a week, right?" And then CTO's email me and say, "What are we doing wrong with Copilot?" And the answer is probably a culture change piece. There's a rollout piece. So, phase one, foundation months one to two, executive announcement from
the CEO. like when you are rolling out intelligence. I'm assuming here that this is a from scratch roll out. It's not often actually the case because 90% of you know the Fortune 500 or whatever have co-pilot somewhere. But let's just pretend that for clarity in the YouTube video this is a clean roll out.
So initially couple first couple months CEO announces this is coming. They've identified a couple of dozen co-pilot champions across different divisions. I'm assuming this is a big company. Again this is our focus is on enterprise right now. They've built an internal support site where you can get your questions answered.
They've developed role specific training for different departments and those champions are owning that training. You can see how you're starting to seed the change in, right? And the CEO is setting the pace and saying this is good, this is positive, allaying fears about co-pilots stealing your job, which anecdotally talking to individual contributors, you have to talk about the job thing if you're going to talk about the AI thing.
People have been so primed by the news to be afraid of AI stealing your job that they won't listen to anything else until you address it. And that must come from the leadership team and preferably from the CEO. And by the way, there's not a ton of evidence that AI is actually stealing jobs in aggregate right now. The it's not just me saying that.
Ethan Mollik is one of the best known AI uh academics out there. He studied AI extensively. He tweeted as recently as like early July 2025 that there was just not much evidence that AI is actually responsible for significant job attrition isolated instances or expectations changing for job roles 100% changing job descriptions as scaling up hiring for AI specific roles definitely seeing that but we're not seeing largecale attrition with AI and I think part of why is the very reason I've had to make this video guys If AI was as good as it needs to be to take our jobs, I wouldn't have to make this video because you could ask Copilot and
Copilot would tell you how to roll itself out. But it doesn't. And so we need to do this culture change. Okay, I've gone through my little soapbox thing. Months one to two, we've set the foundation. We've allayed the fears around jobs. Phase two, now we're getting into scaled out deployment. Let's say you have a six-figure employee count company.
You're trying to get 10,000 users a week on, right? That's the pace you can do with the like teams channels for Q&A with a success story sharing with the daily co-pilot tip emails. You basically have some operational support for these things that kind of caps out at 10,000 users and you want to see how it's working.
By the time you get to full adoption, you know what? Maybe you have 100,000, maybe you have 200,000, maybe like Vodafone, you have 68,000 users enabled. You'll have department competitions for best use cases. You'll have integrations with existing workflows. you're going to have continuous measurement and optimization to see if things are working.
And you know what? If you take your time and roll it out like that, if you make it a serious organizational change thing, it gives you space to celebrate and talk about your success factors. So you can talk about the fact that the CEO and the CTO personally championed this, that that skin in the game mattered. You can talk about how the focus of your roll out was on helping each employee to do their job better, not to just say use AI. You can talk about and celebrate the champions in your departments.
You can have weekly dashboards. You can even have bonuses tied to impact. You can have updates, success stories, tips shared in dedicated teams channels shared in brown bags, shared at all hands. Listen, the the larger lessons learned are not too hard here. You want to have a phased roll out. If you're an enterprise, you don't want to just do a big bang.
You want to have champions that are individual contributors on the front lines. They translate the tech to business value. And your job as a leader is to celebrate them. You need to address fears upfront. You need to be clear that this helps you and it doesn't replace you.
You need to measure what works and what doesn't work and be really honest about the places where it doesn't work so you can fix it and celebrate the wins. It really matters. By the way, none of this is new. If you have been in the culture change business and leadership, this is what we've been saying for a long time. And it's not different with AI. In fact, it's more important to do this with AI because AI feels more personal.
We as a human species have never had to do shared intelligence at work. We are at the Henry Ford generation for figuring out how to work with machine intelligence at work. We need good leadership to make that successful. And like it or not, even if you love Chad GPT, and I talk about Chad GPT a lot, co-pilot is where a lot of that is going to be worked out in the workplace because of Microsoft distribution advantage.
Okay, let's just call out a few of the advanced features people miss as we sort of transition toward toward the end of this lengthy video. Thank you for staying with me. I told you it'd be worth it. There's a lot of specifics. So, Copilot Vision is a really interesting one. People don't use it way beyond email. Now, you can take a screenshot of an error message and you can say, "Hey, can you explain this error and how to fix it?" Copilot will read your screen.
It'll explain it in plain English. You can take a screenshot of a PDF table. Hey, can you convert this to Excel? Unless the table is really small with tiny print, that will often work. Uh, and you'll get properly formatted data and you can have it ready to paste. You can take a screenshot of a complicated piece of software or onboarding and say, "Hey, I don't know how to onboard.
I don't know what option to pick. How do I export data?" I will tell you, I have absolutely done this with complicated software that is poorly documented. And I get so much help from AI doing that. Like I can get buttons, you can get menu paths out of Copilot. It's really, really helpful. Okay. Another one is prompt chaining.
I've talked about prompt training elsewhere. This is how you do it with Copilot. As an example, think about five prompts that create a business plan. Number one, outline the business plan for an AI consulting firm. Number two, expand a particular section that has goes deeper on market analysis.
Number three, add financial projections for the next three years. Number four, create an executive summary based on the sections. And number five, suggest five potential risks. individually. These are prompts that you might think of for drafting, but when you take them together, you're basically going on a chain of thought with the AI. And I said it really fast, but you're going to write it slower.
And the reason why is that you need to bring your own thinking to each piece in prompt chaining so that you're actually partnering with AI. And it almost looks like a walk through the conceptual space you're exploring together with machine intelligence. Each step in the prompt chain helps you move your own thinking forward.
This is a sneak peek into how people are doing advanced drafting with AI. Let's look at another example. Report generation. You can do multiple different tools here in Excel. Analyze the Q4 sales data. Copy those insights to Word. Then talk to Word and say, "Hey, Copilot, create a professional report using these insights. Add an introduction. Then you move over to PowerPoint Copilot.
Hey, create a five slide presentation from this Word report." Then go to Outlook. Hey, draft an email to leadership with the report attached. Bam. Bam. Bam. Bam. So fast. Again, you have to bring your brain. You have to know what you want to say. But it is possible to jump from dock to doc. And this is another example where there's no code needed. And co-pilot can help.
And most people aren't doing that jump. Here's another one. You can build your own bots. HR onboarding bot. An HR onboarding bot built by HR without code can save HR several hours a week by just answering what's the vacation policy. I need to add my spouse to insurance. Walk me through setting up my phone. If you're at scale in the enterprise, you get lots of those queries every single day. Having a bot is really helpful.
IT help desk bot, similar thing. You're going to get a lot of queries at the enterprise scale. Build it yourself. It can run password reset workflows. It can run software installation guides. It can run escalation to a human when needed. It might deflect half of your tickets, 60% of your tickets. It can be very helpful.
All right, I don't want to leave you without a few more success stories with Copilot. DWF law firm, this is mentioned uh Microsoft talked about them actually as a case study. They use C-pilot to reduce the time it takes to review contracts. I haven't talked a ton about how legal is seeing tremendous changes with AI.
But even though legal has a very high bar on accuracy, they cannot mess anything up. You can still use something like Copilot to get an early read and reduce the time. I think the time taken was like 7 days to 7 hours and they got more consistent quality because they uploaded the contract and critically they asked the right prompts. Identify obligations.
Identify deadlines. Flag unusual terms compared to our standard, which requires Copilot to know your standard. Create a summary in plain English. It's all about the kinds of prompting that you bring. And you see how they're prompting in ways that give the AI room to succeed here. Healthcare is another example.
You can use Copilot for patient discharge summaries. Doctors might spend 30 minutes writing discharges. Now, Copilot can just pull from patient records, create the discharge summary, including medications, and follow up and be done with it. It can result in a dramatic drop in the time spent doctors on doctors spend on paperwork and more time with patients, finance, and I know this sounds like an AI advertisement, right? Like that my point is not to advertise copilot partly because I don't need to. Everybody has it. The point is to articulate that people can do a lot more
and people don't realize it. Finance is another one. You can feed market data into Excel and you can actually get a second perspective on market anomalies. Retail, you can upload a product image and you can say write this product description. That one gets done a lot. Doesn't just get done with Copilot.
Okay, if you want to do this, it is not that hard. You can get co-pilot in. You can try basic prompts. You can get into your email. You can get into Word and Excel in the first couple days. And if you want to get into Copilot Vision, if you want to get into chain prompting, some of the advanced stuff I talked about, just try it.
This is one of those things that I keep emphasizing with AI. Just try it. You will learn if you are willing to try. If you're with teams and you're trying to sort of not teams the software, teams the people like people teams, let your team try it. Celebrate the wins your team is getting with co-pilot.
Encourage your team by showing them that you value their time saved. And you will not just pile more work on. You will not just tell them you must not be that good at your job because you did it that fast. you'll actually celebrate the win. Okay, here's the reality. I'm going to go right back to the beginning.
The CTO wrote me and was like, they're only using email, 6,000 person company. These companies are are using 10% of a six figure investment in some cases, whereas the ones that are using it fully, they're getting extra more return on investment. The swing that I see here is insane. I talked earlier about the security edition that costs like $35,000 a month.
I talked about the ROI you get if you can reduce the time of the incident because you're actually better at triaging. That by itself delivers 20x ROI. Another way to get to that 20x ROI, get these cross team workflows I talked about going. Get marketing talking to product with co-pilot. Get sales working with engineering more successfully. You're going to hit that ROI.
advanced co-pilot usage like I am describing is going to be table stakes relatively quickly. If you don't adopt it, you're going to be behind. And that's another reason I'm choosing to make this video. I could have sat on this and said, "I need more information. I need more information. It's not perfect because no information set ever is.
" I'm making this video now because you need to adopt Copilot quickly if you have it in your business and you need to adopt it successfully and waiting won't help you. Start with a workflow I outline here in the video. Measure the time you saved this week. Share the video with your team. Co-pilot mastery is a competitive advantage in the workplace by definition because co-pilot matters so much because copilot is in 90% of enterprises.
Everything I'm describing is going to look like a basic requirement in about a year. So start now. Okay. I have a full written guide up. You know where to find it. If you want to see me make more videos on co-pilot, let me know. This has been a long time coming. There was a ton to cover. I'm happy to dive deeper on specific industries.
I'm happy to dive deeper on specific flavors of Copilot. Wanted to summarize it and give you the overall introduction first. I hope you've enjoyed it. I know that uh I've been excited to actually have meaningful AI conversations with people who use Copilot so we can move from this world where people are just using it for email to a world where people actually can take advantage of the fact that we have taught the rocks to think and now they're at work with us in Microsoft Word and they can help us do smarter things faster. You still bring the brains to work, but co-pilot will help.

---

<!-- VIDEO_ID: nQcy-YlYpng -->

Date: 2025-12-02

## Core Thesis
AI is creating an unprecedented identity crisis for Product Managers by simultaneously demanding technical fluency in probabilistic systems, blurring traditional role boundaries, and challenging their historical "glue" function, yet paradoxically elevating the irreplaceable human elements of product intuition, strategic conviction, and complex stakeholder alignment as the ultimate differentiators against burnout and irrelevance.

## Key Concepts
*   **Probabilistic Product Paradigm:** AI products fundamentally differ from traditional deterministic products, requiring a new approach to requirements, deadlines, and executive expectations, challenging the traditional PM mental model of defining clear, predictable outcomes.
*   **PM as a "Glue Role" Under AI Strain:** The historical function of PM as an intermediary for information flow is challenged by AI's capabilities, but its irreplaceable role in complex stakeholder alignment, persuasion, and conviction remains.
*   **High-Fidelity Mental Model of LLMs:** Beyond superficial understanding, PMs need a deep, technical grasp of LLM tradeoffs (e.g., schema validation, agent design) to effectively guide AI product development and engage with engineering.
*   **The Irreplaceability of Product Intuition and Conviction:** Despite AI's data processing power, human "product gut," taste, judgment, and the conviction to drive alignment are "craft skills" that cannot be outsourced to AI and are essential for meaningful product direction and career longevity.
*   **AI as an Assistant, Not a Colleague Calling Shots:** A crucial mental model for integrating AI, emphasizing its role as a powerful tool to extend human capabilities in less critical tasks, rather than a decision-making entity to be deferred to.
*   **"AI Washing" as a Burnout Catalyst:** The counter-intuitive finding that AI projects, especially those driven by hype or executive mandates without autonomy, can be a significant source of PM burnout and demoralization due to their lack of genuine impact or learning opportunities.
*   **Timeless Core PM Skills:** The enduring value of skills like stakeholder alignment, technical fluency, and strategic conviction, which are often overshadowed by AI's "loudness" but remain the bedrock of successful product management.

---

AI is causing a crisis for product managers. I don't think it's too far to say that PMs are the worst off of the job families around AI right now. And I say that being very aware that there are colleagues of mine in engineering, in customer success, in sales who would disagree with me in marketing who would disagree with me. That's okay.
I want to talk about PM partly because I know it well. Done PM. I've managed PMs. I've led PMs. But partly because AI is doing more of the heavy lifting in the PM domain than most PMs expected and that is leading to a crisis of identity that is distinct and different from what I see when I talk to marketers to CS to sales to others who feel like their jobs are impacted by AI.
Let me explain what I mean. If you're in CS, it's a very clear engagement model for AI. The AI comes and whatever your feelings about it, it's picking up triage tickets. That's just pretty much what it does. If you're in sales, the AI comes and it's listening in on calls and it's giving you coaching tips and it's maybe writing your out outbound emails, etc.
Again, very predictable scope of engagement. If you are in marketing, the scope of engagement is similarly predictable, but it's about creative and asset generation. It is different with PM. And you guys give the PM you know a hug because for PMs there's multiple threats on multiple axes. Yes, we have the same thing that you guys have where it's about asset generation.
So PRD generation, Clarvo has got a whole business around PRD generation and there's other assets that PMs are responsible for that AI helps with as well. So there's that whole skill set to master and by the way that is not an easy skill set because you have to retain your thinking, thoughtfulness and creativity but somehow work faster because of AI.
There's product insight requires a mixture of data and storytelling genuine product sense product gut engineering competence. It was always hard and AI makes it faster but not necessarily better unless you really know how to prompt. So that that whole asset creation thing is very fraught for PMS but on top of that you have to build AI product which marketers don't have to do CS doesn't have to do.
You have to figure out how to build AI product and AI product doesn't work like any other product guys. other products you can just sort of say this is what the product is and write the requirements and that's how we were all brought up if we were npm for a decade but not anymore now the product is probabilistic if you're building an AI product the product is it mostly does this but sometimes there are edge cases sometimes the product is we figured this out can you help us package it is not easy to build AI product especially when
you couple that with the demands from executives where executives are expecting really rapid ships on AI. They're expecting deadlines and your engineering team is like not willing to give you deadlines on probabilistic products so often. So there's the product piece, there's the asset piece. Both of those are very tricky.
There's a third piece that no other role has, which is that PM has always been a glue role. It has always been an in between role. In fact, it evolved out of the need to keep engineers out of meetings, which is why if your PM is in meetings, he or she is they're doing their job. But so much of that is now up for debate, isn't it? Because if it's just about giving information, can an AI do that? Yeah.
If it's just about stakeholder management, can an AI do that? Maybe. Not really. If it's about persuading and aligning on a step forward with executives, can an AI do that? Definitely not. And so that there's this weird stakeholder management mess that PMs have always had to handle that they now have to handle with AI in ways that are not clear.
And on the other side, it's getting increasingly blurry, too, because now PMs are expected not just to write specs, but to directly prototype. We're expected to write stuff up in lovable and show it. And maybe we don't write the PRDM, we just show the the prototype, right? Or maybe we go so far as to commit code using claude code and maybe an engineer reviews it or maybe codeex reviews or maybe claude code reviews it.
But either way, we're writing code. We're vibe coding code. We're vibe coding our SQL statements. Maybe we're making small UX pieces we can just commit ourselves. PM has always been an ambiguous role. The fact that it's ambiguous is not new. The fact that it is under strain from so many different axes at once. The job itself is changing both with stakeholders and with engineering at the same time as the tools we use to do the job are changing at the same time as the definition of the role itself is changing as the same time as the products we build are
changing. That is why PM is in crisis. That is why I know a lot of PMs who are walking away because it is just too hard. Don't worry, the first half of this video is all about how bad it is. But the second half is hope. The second half is words of wisdom from someone who knows AI pretty well and who knows PM pretty well on how we start to navigate through this as product managers, as product leaders.
Number one, we need to start to get real comfortable with the technical aspects of LLMs. Increasingly, the career risk is hitting non-technical PMs harder. And I will go farther. It is hitting technical PMs who are not AI technical fluent. And that is different from saying I need to be a PM who can manage an AI product release.
I'm talking about be a PM who understands when to enforce schema validation or not. When to have a library of tools versus having a large prompt in your agent. You want to be really familiar with the tradeoffs that go with building AI products. And in particular, you want to have a highfidelity mental model of a large language model and how it works, how agents work so that you can be helpful when you are in the room with engineers talking about tradeoffs because that is how you actually move the ball forward on product and
eventually come back with relevant deadlines. So I think the first piece is you got to get technically fluent. There's I'm sorry there's no substitute. The good news is chat GPT is a great teacher. Uh it really is like you can ask ask chat GPT to give you a technical AI lesson every morning and schedule it and it will do that.
Number two, make sure that what you are building matters. I say that because I think that another reason why burnout happens and frustration happens is that PMs are put especially now on projects that are unlikely to move the needle. And if you think, oh that's the nonI products, I've got news for you. AI products don't always move the needle either.
In many cases, what burns PMs out is being asked to do an AI product that the CEO came up with on LinkedIn and they have no autonomy to manage that and they just have to push it through and good luck. So, get yourself into a space where you have a product that actually matters. It might not be an AI product and I'm going to tell you that that's okay.
But it needs to be a product that you believe can move the needle. Even if it doesn't yet, you believe your involvement is enough to move the needle. And if that sounds like it's meaning making, it is meaning making. But that's the art of PM. If you're not motivated, if you're not energized, if you don't feel like there's meaning here, you can't convince your stakeholders.
You can't sell the product, you can't believe in the product, you can't roadmap the product, and you cannot convince engineers on that. So yeah, it does. And I think it especially matters in the age of AI because so many of these AI products that PMs are being asked to PM. They tell me and I see are just AI washing. They're they're not any good.
And that's so demoralizing. It doesn't help with your resume. It doesn't build value. It doesn't move the needle for customers. It discourages your engineering team. It doesn't teach you good AI best practices. So build what matters. Number three, make sure that you don't lose your product intuition. So AI is going to help you go faster.
You're going to be asked to go faster and you're going to feel split between five, six different ways. How many ways can I write a Slack update? Can I use the AI to write the PRD for me, etc. The best PMs are basically treating AI as a way to extend their attention on less important tasks and they are keeping the things that matter, the taste, the judgment for themselves.
They are not outsourcing that. If you have a hunch about your product and you're an experienced PM, follow that hunch. Take it seriously. Yes, AI can listen to and read through all of the transcripts from all of the customer calls, and that's great. And maybe it will surface some things you didn't see, and that's amazing.
You have a product gut for a reason. And it is demoralizing to ignore it. It's death to your product gut. It's damaging to your future career. This is this is a craft skill, right? It's a fingertippy skill. That's something you learn in the shop. Don't lose your touch. Don't lose your product intuition. If you think it needs to launch now, it probably does.
Just go with it. And I think that we sometimes misunderstand what AI is. We sometimes think because AI is something we're building something we're building with and also like apparently an AI colleague and also apparently involved in our meetings and stakeholder management that we should give it a lot more deference or trust then I think we should especially in product where you are responsible for setting direction and autonomy don't do that trust your gut trust your intuition you will get less burned out the product is going to be
better and you will put AI in its right place as an assistant not a colleague that calls the shots. I'll tell you one more thing. Your job to drive alignment between engineering and leadership has not gone away, not eroded one bit. AI can't help you with it. And if you can't keep direction there and clarity there, your career isn't worth a plug nickel.
That is the value. That is what you can't substitute any other way. And so make sure if you're working on something meaningful that you are putting the human work in to keep alignment with leadership and your technical teams that you can't have AI just write the deck and assume it's going to be right there.
Right? You you can't have AI give the presentation. You are going to have to go in you are going to have to write the docs and the decks that work for you. And can AI help? Yeah, actually AI can help. But there's no substitute for your conviction that this is the right angle to approach an important meeting with.
There's no substitute for your conviction that the agenda needs to be shorter because of X or Y stakeholder. There's no substitute for your conviction. The technical team needs two weeks more time and you have to go and get it. That's your job. And so when you look back and you look at sort of where you are now, the things that I'm emphasizing are things that you can trace a through line for the last decade plus.
These are things we've always had to do, but I think they're things we've started to forget because AI is incredibly loud and in our faces all the time. I'm not saying don't use AI. AI tooling is great. If you can use AI to help you write better PRDs and one pagers and write better SQL and get into prototyping, that is all great skills that you can demonstrate that move the ball forward for your customers. Do it.
Don't lean out, lean in. But you are leaning into AI to become a PM with these core skills that don't change like working with stakeholders, like aligning with leadership, like working on something that matters in a way that's new. AI is a tool in the toolkit. You are a craftsman and you can use AI as a tool in your toolkit and you should and you need to and the ones who don't are in trouble.
But don't mistake that need to lean in for a need to lean away from these core values. Those don't change. And I think that I feel like this is missing. I feel like we don't have enough conversations about the core values in the job family. And that's why I'm putting this out there. They haven't changed. The things that make PM successful over time have not changed.
AI is just a tool that we're using to get there. I hope this has helped. Best of luck with product. Well, if you stayed this long, you want to check out my product manager dice. I have I can roll this, right? And it says on the road map. It says no. It says plan for Q5. That's my favorite one. And uh it depends. I love that.
So anyway, have fun, keep chuckling and uh stay in product is worth


---

<!-- VIDEO_ID: J2TeiRJLyZ4 -->

Date: 2025-06-24

## Core Thesis
Cloey's "cheating" narrative is a masterclass in aggressive, counter-intuitive marketing designed to capture mindshare and distribution for a novel, proactive AI agent paradigm, rather than a reflection of its core technological innovation. The true strategic insight lies in prioritizing ubiquitous, integrated user experience and habit formation for an AI-native generation over raw model intelligence, thereby positioning itself to redefine AI interaction beyond the traditional chatbot.

## Key Concepts
*   **Marketing as a Distribution Wedge:** Aggressive, emotionally charged marketing (e.g., the "cheating" narrative) can be a deliberate, high-impact strategy to "jam open the door on the distribution channel" and secure mindshare, even if it misrepresents the core product. This highlights that market entry and user acquisition can be prioritized over immediate technological superiority.
*   **Proactive AI Agents as the Next Frontier:** The shift from reactive chatbots (like ChatGPT) to invisible, always-on, context-aware "level two proactive AI agents" that layer across all applications represents a significant evolution in AI utility and user experience, challenging the current dominant AI interaction model.
*   **UX and Distribution Trump Raw Model Intelligence (Early Market):** A "mid" AI model can still achieve significant market traction and competitive advantage if coupled with superior user experience, seamless integration, and effective distribution strategies, especially when targeting new user habits.
*   **The Inflationary Nature of "Cheat Codes":** Any technological "edge" or "cheat code" loses its value and becomes inflationary once widely adopted, shifting the competitive advantage back to fundamental innovation in user experience, integration, and underlying agentic capabilities.
*   **AI Augmentation vs. Automation:** Effective AI tools are not about blindly providing answers or "sapping brain power," but about augmenting human capabilities, requiring users to apply critical thinking and engage in a dialogue with the AI (e.g., arguing with 70% of AI responses).
*   **Generational AI-Native Habits:** Targeting younger generations (Gen Z, Gen Alpha) to build daily usage habits for integrated AI augmentation tools is a strategic move to establish long-term market dominance and shape future workflow paradigms.

---
cheating, cheating, cheating. That's what I've been hearing about Clo ever since they raised $15 million from A16Z just a few days ago. And this is not a story about that raise. Nor is it a story about Roy Lee, even though every newspaper I've come across has talked about Roy getting kicked out of Colombia and how that's like a key part of the founder narrative because he got kicked out for building this tool that helps him cheat. It's just I got to take my hat off to these guys. Roy and team have done a phenomenal job turning the entire narrative about the company into cheating. And it's not about that. It's not I want you to just take off your assumptions about what Clo is really about for a second. It's not about cheating. I know they want you to think about that. They freaking have a manifesto on their website that says cheat at everything. I get that they want you to think about that and they're very good at it, but set it aside. I would argue that they're cheating at cheating. I'm going to explain why. Cluey is really in the business of introducing you to level two proactive AI agents.
Cluey is one of the first that's implemented that successfully. I'm super impressed with their UX. I am not impressed with their AI model. I think their AI intelligence is mid. And to be honest with you, having played around with it and used it, it feels like chat GPT40 level intelligence and if you're going to use it and assume the answers are right and give the answers on the tests and give the answers on the interviews and cheated everything, I got news for you. It's going to sound like AI slop. It is. Just like the resumes that come in now are all perfect will sound like AI slop. No, it's not the model. What matters is the UX and the way proactive AI agents are implemented. And I strongly suspect Roy and the team know that the model is not as good as it could be and not as good as it will be. And they're deliberately using aggressive marketing. They have seven marketers and four engineers. aggressive aggressive marketing to jam open the door on the distribution channel to jam open the door on branding to hold a space to hold mind share all those newspapers writing about Roy cheating and so on and so forth free mind share earned clicks they're holding mind share with their target market of Gen Z and Gen Alpha they want to build the habit of daily usage for a tool that is across every app that you use if you use Clo it sits as an invisible pane of class across everything. If they build the sticky habit, you will never leave. So, they're ready when chat GPT5 comes along. They're ready when chat GPT6 comes along. I would say Claude here, but Claude is too ethical to work with Cluey. In fact, one of the really interesting things when I analyzed the prompt for Cloy is that Clo maximizes utility to the user in their prompt and Claude has strong ethical guard rails. And I think that's fascinating. I talk about that a little bit in the uh Substack that I wrote about this. I think the thing I want you to take away, it's not about the cheating, as I've said, it's about the fact that proactive AI agents are going to be everywhere. And Cluey shows us what that's like. It's not going to be the chatbot that chat GPT is using. In fact, chat GPT faces more of an interesting competitive risk from Cluey than Cluey does from chat GPT. Because if you start using Clad GPT feels like P. It doesn't feel like the place where the value is. I I saw it and I felt it when I was using Cluey. I used Cluey to watch a YouTube video with me and I talked to Cluey about the YouTube video about a technical topic. Yeah, the intelligence wasn't in incredible, but it was good enough to get me moving forward. I talked to Cluey about a web design problem I was having, figuring out CSS stylesheets. And I was working with Claude Code and I was working with 03. and Cluey was helping me go back and forth and sort out what was going on. And Cle was writing suggested code to help illustrate the problem based on what it actually saw on my screen. This is not a cheating tool. This is an agentic proactive tool that layers across all of your apps. But it's brilliant. It's brilliant to categorize it as a cheating tool because that's one word that is high impact, that is emotional, that divides generations, that sort of taps into this sense of of we deserve this and we're not going to get it that I hear from so many of my friends and colleagues who are Gen Z. And so I and that's not me talking like I I get it, right? But the marketing is designed to hit that button emotionally as as a marketer who has sat in the marketing chair. That's extremely deliberate and Roy is a little bit honest about that in some of his interviews. So if you're going to talk about Cloy, you need to separate the marketing piece from the product piece. You need to understand that what A16Z really invested in is the product piece. They invested in essentially smart marketing, incredible distribution channel targeted at people who are just coming into peak buying power, who are forming the first AI native generational habits now. And said, "Yes, this is going to be a tool of the future. This is going to be a workflow tool for everything." And then on top of that, Clo shows up and cuts seven figure deals with businesses. We've been talking about people like Gen Z, Gen Alpha, college students the whole time. Think about it from a business perspective. They're cutting seven figure deals because customer success needs a clue to have good conversations. Sales needs a clue. Hey, let me share my screen and cl's in the background. You never know it. It hides. I can't show you a screenshot of Cluey cuz it doesn't screenshot. It's designed to be invisible, but it's there to help you. I do think they're on to something about the future of AI there. And yes, the cheating branding is controversial, but we are going to see a lot more folks who are focused on the utility of AI. And I think that there's a little bit of a generational shift here where people who are younger, people who are deeper in AI, I'm not younger, let's just be honest, uh are going to feel more comfortable with AI augmentation being part of the normal human experience than people who are less native on AI. And this is like clearly is AI augmentation. It just is. Now, you can do AI augmentation and still be smart about it. It doesn't have to sap your brain power. You don't have to do like, "Hey, this is the answer. I just have to give up and say this is the answer." No. Like I I one of the things I did when I was like writing the post for Cluey is I looked at how often I argue with AI. It's about 70% of the time. 70% of AI responses I'm like, "No, that's incorrect." Or, "No, I disagree." Or reframe that. Maybe I'm not the typical user. I'm almost certainly not the typical user. Let's just be honest. But the reality of using tools like Cloey to essentially expand your ability to understand your world and then apply a critical lens to it is huge. And some people will take it and they will just take the answers at face value. And I would tell you that in the next 6 12 18 months once everybody gets these whether they're cluey or something else that is no longer a cheat code. as a cheat code, as a cheat code clearly is inflationary. Say that five times fast. It's inflationary because as soon as everybody has it, it's no longer an edge. Just as as soon as everyone had chat GPT, the ability to write a resume was no longer an edge. All of them are perfect now. And so it's not the cheating that's interesting in the big scheme of things in 18 months. It's not the cheating that's interesting. It's the proactive AI revolution. It's the ability to open up the distribution window a little bit ahead of the model capability. It's the ability to design a proactive agent experience. It's the ability to actually be in a position from a UX perspective where you could actually unseat Chad GPT. That's fascinating. That's why I think the Cluey story is worth diving into. So if you want to learn more, have a read of the article. Cheers.

---



---

